{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cc1ebb-e7e9-4ab7-b9f0-660a8d23e2de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_data_dict' from 'grid_utils' (C:\\Users\\Owner\\grid_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 62\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mensemble_training\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     EnsembleModel, DualModelTraining,\n\u001b[0;32m     58\u001b[0m     EnsembleTrainer, EnsembleConfig\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelEvaluator\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrid_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     GridPair, create_data_dict, flatten_and_reshape,\n\u001b[0;32m     64\u001b[0m     get_device, grid_to_image, handle_batch_size,\n\u001b[0;32m     65\u001b[0m     handle_channels, interpolate_tensor\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingGUI\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogger_setup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logger\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'create_data_dict' from 'grid_utils' (C:\\Users\\Owner\\grid_utils.py)"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 1. Import Libraries\n",
    "# ===============================================================\n",
    "\n",
    "# Standard Libraries\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Data Processing and Scientific Computing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Any, Dict, Optional, Set, Tuple\n",
    "\n",
    "\n",
    "# PyTorch and Deep Learning\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import ResNet18_Weights, resnet18\n",
    "\n",
    "# Visualization and GUI\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Tree and Graph Processing\n",
    "from anytree import RenderTree\n",
    "from anytree.exporter import DotExporter\n",
    "\n",
    "# Custom Modules\n",
    "from Augment import GridAugmentor\n",
    "from grid_pair import GridPair\n",
    "from DNN import DeepModelTrainer\n",
    "from cnn_grid_mapper import CNNGridMapper\n",
    "from data_structures import GridPair\n",
    "from grid_pair import GridPair\n",
    "from data_tree import DataTree\n",
    "from datasets import AugmentedARCDataset\n",
    "from embedding import TransformerEmbeddings, GridTransformerEmbeddings, EmbeddingConfig\n",
    "from ensemble_training import (\n",
    "    EnsembleModel, DualModelTraining,\n",
    "    EnsembleTrainer, EnsembleConfig\n",
    ")\n",
    "\n",
    "from evaluation import ModelEvaluator\n",
    "from grid_utils import (\n",
    "    GridPair, create_data_dict, flatten_and_reshape,\n",
    "    get_device, grid_to_image, handle_batch_size,\n",
    "    handle_channels, interpolate_tensor\n",
    ")\n",
    "from gui import TrainingGUI\n",
    "from logger_setup import setup_logger\n",
    "from node import Node\n",
    "from reward_based_model import RewardBasedModel\n",
    "from simple_transformer import SimpleTransformer\n",
    "from train_model_with_gui import train_model_with_gui\n",
    "\n",
    "# Global Configuration\n",
    "logger = setup_logger(__name__)\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561a0ad-e0cb-4583-9886-e186d9b1337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Enhanced transformer model with proper embeddings support.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 5000,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        is_grid: bool = True\n",
    "    ):\n",
    "        \"\"\"Initialize transformer.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            d_model: Model dimension\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of transformer layers\n",
    "            dim_feedforward: Feedforward dimension\n",
    "            dropout: Dropout rate\n",
    "            is_grid: Whether input is grid-structured\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create embedding configuration\n",
    "        embedding_config = EmbeddingConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            use_position_embedding=True,\n",
    "            learn_position=False\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embedding = (\n",
    "            GridTransformerEmbeddings(embedding_config)\n",
    "            if is_grid else\n",
    "            TransformerEmbeddings(embedding_config)\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Output decoder\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor\n",
    "            src_mask: Optional attention mask\n",
    "            src_key_padding_mask: Optional key padding mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Embed input\n",
    "        embedded = self.embedding(src)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        encoded = self.transformer_encoder(\n",
    "            embedded,\n",
    "            mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Decode to vocabulary\n",
    "        output = self.decoder(encoded)\n",
    "        \n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c94f43-d38e-4b08-be92-17bed705721a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ace00-c96f-41de-9acc-bc8aa30b088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, List, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gzip\n",
    "import shutil\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ARCDataConfig:\n",
    "    \"\"\"Configuration for ARC data loading.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir: Directory containing data files\n",
    "        cache_dir: Directory for caching data\n",
    "        use_cache: Whether to use data caching\n",
    "        validate_data: Whether to validate loaded data\n",
    "        parallel_loading: Whether to load files in parallel\n",
    "        compression: Whether to use compression\n",
    "        checksum_verification: Whether to verify file checksums\n",
    "    \"\"\"\n",
    "    data_dir: str = \"data\"\n",
    "    cache_dir: str = \"cache\"\n",
    "    use_cache: bool = True\n",
    "    validate_data: bool = True\n",
    "    parallel_loading: bool = True\n",
    "    compression: bool = True\n",
    "    checksum_verification: bool = True\n",
    "\n",
    "class ARCDataLoader:\n",
    "    \"\"\"Loader for ARC dataset with advanced features.\"\"\"\n",
    "    \n",
    "    FILE_PATHS = {\n",
    "        \"training_challenges\": \"arc-agi_training_challenges.json\",\n",
    "        \"evaluation_challenges\": \"arc-agi_evaluation_challenges.json\",\n",
    "        \"training_solutions\": \"arc-agi_training_solutions.json\",\n",
    "        \"evaluation_solutions\": \"arc-agi_evaluation_solutions.json\"\n",
    "    }\n",
    "    \n",
    "    CHECKSUMS = {\n",
    "        # Add expected SHA-256 checksums for each file\n",
    "        \"arc-agi_training_challenges.json\": \"...\",\n",
    "        \"arc-agi_evaluation_challenges.json\": \"...\",\n",
    "        \"arc-agi_training_solutions.json\": \"...\",\n",
    "        \"arc-agi_evaluation_solutions.json\": \"...\"\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[ARCDataConfig] = None\n",
    "    ):\n",
    "        \"\"\"Initialize data loader.\n",
    "        \n",
    "        Args:\n",
    "            config: Optional configuration\n",
    "        \"\"\"\n",
    "        self.config = config or ARCDataConfig()\n",
    "        self._setup_directories()\n",
    "        self.data_cache: Dict[str, Any] = {}\n",
    "        \n",
    "    def _setup_directories(self) -> None:\n",
    "        \"\"\"Create necessary directories.\"\"\"\n",
    "        Path(self.config.data_dir).mkdir(parents=True, exist_ok=True)\n",
    "        if self.config.use_cache:\n",
    "            Path(self.config.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "    def load_data(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Load ARC dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing dataset components\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.config.use_cache and self._cache_exists():\n",
    "                return self._load_from_cache()\n",
    "                \n",
    "            if self.config.parallel_loading:\n",
    "                data = self._load_parallel()\n",
    "            else:\n",
    "                data = self._load_sequential()\n",
    "                \n",
    "            if self.config.use_cache:\n",
    "                self._save_to_cache(data)\n",
    "                \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading ARC data: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _load_sequential(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Load files sequentially.\n",
    "        \n",
    "        Returns:\n",
    "            Loaded data dictionary\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for key, filename in self.FILE_PATHS.items():\n",
    "            try:\n",
    "                file_path = Path(self.config.data_dir) / filename\n",
    "                data[key] = self._load_single_file(file_path)\n",
    "                logger.info(\n",
    "                    f\"Loaded {key} with {len(data[key])} items \"\n",
    "                    f\"from {filename}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {filename}: {e}\")\n",
    "                data[key] = {}\n",
    "                \n",
    "        return data\n",
    "        \n",
    "    def _load_parallel(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Load files in parallel.\n",
    "        \n",
    "        Returns:\n",
    "            Loaded data dictionary\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future_to_key = {\n",
    "                executor.submit(\n",
    "                    self._load_single_file,\n",
    "                    Path(self.config.data_dir) / filename\n",
    "                ): key\n",
    "                for key, filename in self.FILE_PATHS.items()\n",
    "            }\n",
    "            \n",
    "            for future in future_to_key:\n",
    "                key = future_to_key[future]\n",
    "                try:\n",
    "                    data[key] = future.result()\n",
    "                    logger.info(\n",
    "                        f\"Loaded {key} with {len(data[key])} items\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading {key}: {e}\")\n",
    "                    data[key] = {}\n",
    "                    \n",
    "        return data\n",
    "        \n",
    "    def _load_single_file(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Load and validate single file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to file\n",
    "            \n",
    "        Returns:\n",
    "            Loaded data dictionary\n",
    "        \"\"\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "        if self.config.checksum_verification:\n",
    "            self._verify_checksum(file_path)\n",
    "            \n",
    "        if file_path.suffix == '.gz':\n",
    "            with gzip.open(file_path, 'rt') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "        if self.config.validate_data:\n",
    "            self._validate_data(data, file_path.name)\n",
    "            \n",
    "        return data\n",
    "        \n",
    "    def _verify_checksum(self, file_path: Path) -> None:\n",
    "        \"\"\"Verify file checksum.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to file\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If checksum verification fails\n",
    "        \"\"\"\n",
    "        if file_path.name not in self.CHECKSUMS:\n",
    "            logger.warning(f\"No checksum found for {file_path.name}\")\n",
    "            return\n",
    "            \n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "            \n",
    "        if file_hash != self.CHECKSUMS[file_path.name]:\n",
    "            raise ValueError(\n",
    "                f\"Checksum verification failed for {file_path.name}\"\n",
    "            )\n",
    "            \n",
    "    def _validate_data(\n",
    "        self,\n",
    "        data: Dict,\n",
    "        filename: str\n",
    "    ) -> None:\n",
    "        \"\"\"Validate loaded data.\n",
    "        \n",
    "        Args:\n",
    "            data: Loaded data\n",
    "            filename: Source filename\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"Invalid data format in {filename}\")\n",
    "            \n",
    "        required_keys = {'train', 'test'} if 'challenges' in filename else {'input', 'output'}\n",
    "        \n",
    "        for item in data.values():\n",
    "            if not all(key in item for key in required_keys):\n",
    "                raise ValueError(\n",
    "                    f\"Missing required keys in {filename}\"\n",
    "                )\n",
    "                \n",
    "    def _cache_exists(self) -> bool:\n",
    "        \"\"\"Check if cache exists.\n",
    "        \n",
    "        Returns:\n",
    "            Whether cache exists\n",
    "        \"\"\"\n",
    "        cache_file = Path(self.config.cache_dir) / \"arc_data_cache.json.gz\"\n",
    "        return cache_file.exists()\n",
    "        \n",
    "    def _load_from_cache(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Load data from cache.\n",
    "        \n",
    "        Returns:\n",
    "            Cached data dictionary\n",
    "        \"\"\"\n",
    "        cache_file = Path(self.config.cache_dir) / \"arc_data_cache.json.gz\"\n",
    "        with gzip.open(cache_file, 'rt') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        logger.info(\"Loaded data from cache\")\n",
    "        return data\n",
    "        \n",
    "    def _save_to_cache(self, data: Dict[str, Dict]) -> None:\n",
    "        \"\"\"Save data to cache.\n",
    "        \n",
    "        Args:\n",
    "            data: Data to cache\n",
    "        \"\"\"\n",
    "        cache_file = Path(self.config.cache_dir) / \"arc_data_cache.json.gz\"\n",
    "        with gzip.open(cache_file, 'wt') as f:\n",
    "            json.dump(data, f)\n",
    "            \n",
    "        logger.info(\"Saved data to cache\")\n",
    "        \n",
    "    def get_dataset_stats(\n",
    "        self,\n",
    "        data: Dict[str, Dict]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Get dataset statistics.\n",
    "        \n",
    "        Args:\n",
    "            data: Dataset dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'total_examples': 0,\n",
    "            'challenges': {\n",
    "                'training': len(data.get('training_challenges', {})),\n",
    "                'evaluation': len(data.get('evaluation_challenges', {}))\n",
    "            },\n",
    "            'solutions': {\n",
    "                'training': len(data.get('training_solutions', {})),\n",
    "                'evaluation': len(data.get('evaluation_solutions', {}))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stats['total_examples'] = sum(stats['challenges'].values())\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = ARCDataConfig(\n",
    "    data_dir=\"data\",\n",
    "    cache_dir=\"cache\",\n",
    "    use_cache=True,\n",
    "    validate_data=True,\n",
    "    parallel_loading=True\n",
    ")\n",
    "\n",
    "# Initialize loader\n",
    "loader = ARCDataLoader(config)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    arc_data = loader.load_data()\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = loader.get_dataset_stats(arc_data)\n",
    "    print(f\"Dataset statistics: {stats}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fb652-d318-43b7-af5d-57b476390238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GridDimensions:\n",
    "    \"\"\"Container for grid dimensions and statistics.\n",
    "    \n",
    "    Attributes:\n",
    "        height: Grid height\n",
    "        width: Grid width\n",
    "        min_value: Minimum value in grid\n",
    "        max_value: Maximum value in grid\n",
    "        unique_values: Number of unique values\n",
    "        sparsity: Percentage of zero values\n",
    "    \"\"\"\n",
    "    height: int\n",
    "    width: int\n",
    "    min_value: int\n",
    "    max_value: int\n",
    "    unique_values: int\n",
    "    sparsity: float\n",
    "\n",
    "@dataclass\n",
    "class TaskAnalysis:\n",
    "    \"\"\"Container for task analysis results.\n",
    "    \n",
    "    Attributes:\n",
    "        input_dims: Input grid dimensions\n",
    "        output_dims: Output grid dimensions\n",
    "        transformation_type: Type of grid transformation\n",
    "        complexity_score: Estimated task complexity\n",
    "        patterns: Detected patterns\n",
    "    \"\"\"\n",
    "    input_dims: GridDimensions\n",
    "    output_dims: GridDimensions\n",
    "    transformation_type: str\n",
    "    complexity_score: float\n",
    "    patterns: Dict[str, Any]\n",
    "\n",
    "class GridAnalyzer:\n",
    "    \"\"\"Analyzer for ARC grid data with advanced analytics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize grid analyzer.\"\"\"\n",
    "        self.results = defaultdict(lambda: defaultdict(list))\n",
    "        self.statistics = {}\n",
    "        \n",
    "    def analyze_file(\n",
    "        self,\n",
    "        file_path: Path\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze grids in file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to data file\n",
    "            \n",
    "        Returns:\n",
    "            Analysis results dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and validate data\n",
    "            data = self._load_data(file_path)\n",
    "            \n",
    "            # Analyze each task\n",
    "            for task_id, task_content in data.items():\n",
    "                self._analyze_task(task_id, task_content)\n",
    "                \n",
    "            # Compute global statistics\n",
    "            self._compute_statistics()\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Analyzed {len(data)} tasks from {file_path}\"\n",
    "            )\n",
    "            \n",
    "            return dict(self.results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing file {file_path}: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _load_data(\n",
    "        self,\n",
    "        file_path: Path\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Load and validate data file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to data file\n",
    "            \n",
    "        Returns:\n",
    "            Loaded data dictionary\n",
    "        \"\"\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"Invalid data format\")\n",
    "            \n",
    "        return data\n",
    "        \n",
    "    def _analyze_task(\n",
    "        self,\n",
    "        task_id: str,\n",
    "        task_content: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Analyze single task.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Task identifier\n",
    "            task_content: Task data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Analyze train and test sections\n",
    "            for section_name, sections in task_content.items():\n",
    "                self._analyze_sections(\n",
    "                    task_id,\n",
    "                    section_name,\n",
    "                    sections\n",
    "                )\n",
    "                \n",
    "            # Analyze relationships between input and output\n",
    "            self._analyze_relationships(task_id)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing task {task_id}: {e}\")\n",
    "            \n",
    "    def _analyze_sections(\n",
    "        self,\n",
    "        task_id: str,\n",
    "        section_name: str,\n",
    "        sections: List[Dict[str, Any]]\n",
    "    ) -> None:\n",
    "        \"\"\"Analyze sections of task.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Task identifier\n",
    "            section_name: Section name\n",
    "            sections: Section data\n",
    "        \"\"\"\n",
    "        for section in sections:\n",
    "            input_data = section.get(\"input\", [])\n",
    "            output_data = section.get(\"output\", [])\n",
    "            \n",
    "            # Analyze dimensions\n",
    "            self._analyze_dimensions(\n",
    "                task_id,\n",
    "                section_name,\n",
    "                input_data,\n",
    "                output_data\n",
    "            )\n",
    "            \n",
    "            # Analyze patterns\n",
    "            if \"train\" in section_name:\n",
    "                self._analyze_patterns(\n",
    "                    task_id,\n",
    "                    input_data,\n",
    "                    output_data\n",
    "                )\n",
    "                \n",
    "    def _analyze_dimensions(\n",
    "        self,\n",
    "        task_id: str,\n",
    "        section_name: str,\n",
    "        input_data: List[Any],\n",
    "        output_data: List[Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Analyze grid dimensions.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Task identifier\n",
    "            section_name: Section name\n",
    "            input_data: Input data\n",
    "            output_data: Output data\n",
    "        \"\"\"\n",
    "        # Process input data\n",
    "        if isinstance(input_data, list):\n",
    "            input_dims = self._get_grid_dimensions(input_data)\n",
    "            self.results[task_id][f\"{section_name}_input_dims\"].append(\n",
    "                input_dims\n",
    "            )\n",
    "            \n",
    "        # Process output data\n",
    "        if isinstance(output_data, list):\n",
    "            output_dims = self._get_grid_dimensions(output_data)\n",
    "            self.results[task_id][f\"{section_name}_output_dims\"].append(\n",
    "                output_dims\n",
    "            )\n",
    "            \n",
    "    def _get_grid_dimensions(\n",
    "        self,\n",
    "        grid: List[Any]\n",
    "    ) -> Optional[GridDimensions]:\n",
    "        \"\"\"Calculate grid dimensions and statistics.\n",
    "        \n",
    "        Args:\n",
    "            grid: Grid data\n",
    "            \n",
    "        Returns:\n",
    "            Grid dimensions object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not grid or not isinstance(grid[0], list):\n",
    "                return None\n",
    "                \n",
    "            np_grid = np.array(grid)\n",
    "            \n",
    "            return GridDimensions(\n",
    "                height=np_grid.shape[0],\n",
    "                width=np_grid.shape[1],\n",
    "                min_value=np.min(np_grid),\n",
    "                max_value=np.max(np_grid),\n",
    "                unique_values=len(np.unique(np_grid)),\n",
    "                sparsity=np.mean(np_grid == 0)\n",
    "            )\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "            \n",
    "    def _analyze_patterns(\n",
    "        self,\n",
    "        task_id: str,\n",
    "        input_data: List[Any],\n",
    "        output_data: List[Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Analyze patterns in grids.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Task identifier\n",
    "            input_data: Input grid\n",
    "            output_data: Output grid\n",
    "        \"\"\"\n",
    "        patterns = {\n",
    "            'symmetry': self._check_symmetry(input_data, output_data),\n",
    "            'repetition': self._check_repetition(input_data, output_data),\n",
    "            'transformation': self._identify_transformation(\n",
    "                input_data,\n",
    "                output_data\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.results[task_id]['patterns'].append(patterns)\n",
    "        \n",
    "    def _check_symmetry(\n",
    "        self,\n",
    "        input_grid: List[Any],\n",
    "        output_grid: List[Any]\n",
    "    ) -> Dict[str, bool]:\n",
    "        \"\"\"Check for symmetry in grids.\n",
    "        \n",
    "        Args:\n",
    "            input_grid: Input grid\n",
    "            output_grid: Output grid\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of symmetry properties\n",
    "        \"\"\"\n",
    "        try:\n",
    "            in_grid = np.array(input_grid)\n",
    "            out_grid = np.array(output_grid)\n",
    "            \n",
    "            return {\n",
    "                'horizontal': np.array_equal(\n",
    "                    in_grid,\n",
    "                    np.fliplr(in_grid)\n",
    "                ),\n",
    "                'vertical': np.array_equal(\n",
    "                    in_grid,\n",
    "                    np.flipud(in_grid)\n",
    "                ),\n",
    "                'diagonal': np.array_equal(\n",
    "                    in_grid,\n",
    "                    in_grid.T\n",
    "                )\n",
    "            }\n",
    "            \n",
    "        except Exception:\n",
    "            return {}\n",
    "            \n",
    "    def _check_repetition(\n",
    "        self,\n",
    "        input_grid: List[Any],\n",
    "        output_grid: List[Any]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Check for repetitive patterns.\n",
    "        \n",
    "        Args:\n",
    "            input_grid: Input grid\n",
    "            output_grid: Output grid\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of repetition metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            in_grid = np.array(input_grid)\n",
    "            \n",
    "            unique, counts = np.unique(\n",
    "                in_grid,\n",
    "                return_counts=True\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'unique_ratio': len(unique) / in_grid.size,\n",
    "                'max_repetition': np.max(counts) / in_grid.size\n",
    "            }\n",
    "            \n",
    "        except Exception:\n",
    "            return {}\n",
    "            \n",
    "    def _identify_transformation(\n",
    "        self,\n",
    "        input_grid: List[Any],\n",
    "        output_grid: List[Any]\n",
    "    ) -> str:\n",
    "        \"\"\"Identify transformation type.\n",
    "        \n",
    "        Args:\n",
    "            input_grid: Input grid\n",
    "            output_grid: Output grid\n",
    "            \n",
    "        Returns:\n",
    "            Transformation type string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            in_grid = np.array(input_grid)\n",
    "            out_grid = np.array(output_grid)\n",
    "            \n",
    "            if in_grid.shape == out_grid.shape:\n",
    "                if np.array_equal(out_grid, np.rot90(in_grid)):\n",
    "                    return \"rotation_90\"\n",
    "                elif np.array_equal(out_grid, np.fliplr(in_grid)):\n",
    "                    return \"horizontal_flip\"\n",
    "                elif np.array_equal(out_grid, np.flipud(in_grid)):\n",
    "                    return \"vertical_flip\"\n",
    "                    \n",
    "            return \"unknown\"\n",
    "            \n",
    "        except Exception:\n",
    "            return \"error\"\n",
    "            \n",
    "    def _compute_statistics(self) -> None:\n",
    "        \"\"\"Compute global statistics.\"\"\"\n",
    "        stats = defaultdict(list)\n",
    "        \n",
    "        for task_results in self.results.values():\n",
    "            for key, values in task_results.items():\n",
    "                if 'dims' in key:\n",
    "                    dims = [d for d in values if d is not None]\n",
    "                    if dims:\n",
    "                        stats[f\"{key}_height\"].extend(\n",
    "                            [d.height for d in dims]\n",
    "                        )\n",
    "                        stats[f\"{key}_width\"].extend(\n",
    "                            [d.width for d in dims]\n",
    "                        )\n",
    "                        \n",
    "        self.statistics = {\n",
    "            key: {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values)\n",
    "            }\n",
    "            for key, values in stats.items()\n",
    "        }\n",
    "        \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            Summary DataFrame\n",
    "        \"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for task_id, results in self.results.items():\n",
    "            summary = {\n",
    "                'task_id': task_id,\n",
    "                'input_dims': len(results['train_input_dims']),\n",
    "                'output_dims': len(results['train_output_dims']),\n",
    "                'patterns_found': len(results['patterns']),\n",
    "                'complexity': self._calculate_complexity(results)\n",
    "            }\n",
    "            summary_data.append(summary)\n",
    "            \n",
    "        return pd.DataFrame(summary_data)\n",
    "        \n",
    "    def _calculate_complexity(\n",
    "        self,\n",
    "        task_results: Dict[str, Any]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate task complexity score.\n",
    "        \n",
    "        Args:\n",
    "            task_results: Task results dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Complexity score\n",
    "        \"\"\"\n",
    "        complexity = 0.0\n",
    "        \n",
    "        # Consider grid sizes\n",
    "        for dims in task_results.get('train_input_dims', []):\n",
    "            if dims:\n",
    "                complexity += dims.height * dims.width\n",
    "                \n",
    "        # Consider pattern complexity\n",
    "        for pattern in task_results.get('patterns', []):\n",
    "            if pattern.get('symmetry', {}).get('diagonal', False):\n",
    "                complexity *= 1.5\n",
    "            if pattern.get('repetition', {}).get('unique_ratio', 1.0) < 0.5:\n",
    "                complexity *= 1.2\n",
    "                \n",
    "        return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85d0cc-75f2-4dfd-87ca-b13880cc848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Dict, Any, Optional, List, Union\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ClassStatistics:\n",
    "    \"\"\"Statistics about dataset classes.\n",
    "    \n",
    "    Attributes:\n",
    "        num_classes: Number of unique classes\n",
    "        class_distribution: Distribution of classes\n",
    "        min_class: Minimum class value\n",
    "        max_class: Maximum class value\n",
    "        class_frequencies: Frequency of each class\n",
    "        rare_classes: Classes with low frequency\n",
    "        common_classes: Most common classes\n",
    "    \"\"\"\n",
    "    num_classes: int\n",
    "    class_distribution: Dict[int, float]\n",
    "    min_class: int\n",
    "    max_class: int\n",
    "    class_frequencies: Counter\n",
    "    rare_classes: Set[int]\n",
    "    common_classes: Set[int]\n",
    "\n",
    "class ClassAnalyzer:\n",
    "    \"\"\"Analyzer for determining and analyzing classes in ARC data.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rare_threshold: float = 0.01,\n",
    "        common_threshold: float = 0.1\n",
    "    ):\n",
    "        \"\"\"Initialize class analyzer.\n",
    "        \n",
    "        Args:\n",
    "            rare_threshold: Threshold for rare class determination\n",
    "            common_threshold: Threshold for common class determination\n",
    "        \"\"\"\n",
    "        self.rare_threshold = rare_threshold\n",
    "        self.common_threshold = common_threshold\n",
    "        self.class_cache = {}\n",
    "        \n",
    "    def analyze_classes(\n",
    "        self,\n",
    "        arc_data: Dict[str, Any]\n",
    "    ) -> ClassStatistics:\n",
    "        \"\"\"Analyze classes in ARC data.\n",
    "        \n",
    "        Args:\n",
    "            arc_data: ARC dataset dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Class statistics object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract all classes\n",
    "            classes = self._extract_classes(arc_data)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = self._calculate_statistics(classes)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Found {stats.num_classes} unique classes \"\n",
    "                f\"(min: {stats.min_class}, max: {stats.max_class})\"\n",
    "            )\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing classes: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _extract_classes(\n",
    "        self,\n",
    "        data: Dict[str, Any]\n",
    "    ) -> Set[int]:\n",
    "        \"\"\"Extract unique classes from data.\n",
    "        \n",
    "        Args:\n",
    "            data: Input data dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Set of unique classes\n",
    "        \"\"\"\n",
    "        classes = set()\n",
    "        \n",
    "        for key, content in data.items():\n",
    "            try:\n",
    "                # Extract from dictionary structure\n",
    "                if isinstance(content, dict):\n",
    "                    classes.update(\n",
    "                        self._extract_from_dict(content)\n",
    "                    )\n",
    "                # Extract from list structure\n",
    "                elif isinstance(content, list):\n",
    "                    classes.update(\n",
    "                        self._extract_from_list(content)\n",
    "                    )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(\n",
    "                    f\"Error extracting classes from {key}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "                \n",
    "        return classes\n",
    "        \n",
    "    def _extract_from_dict(\n",
    "        self,\n",
    "        content: Dict[str, Any]\n",
    "    ) -> Set[int]:\n",
    "        \"\"\"Extract classes from dictionary structure.\n",
    "        \n",
    "        Args:\n",
    "            content: Dictionary content\n",
    "            \n",
    "        Returns:\n",
    "            Set of classes\n",
    "        \"\"\"\n",
    "        classes = set()\n",
    "        \n",
    "        for mode in [\"train\", \"test\"]:\n",
    "            entries = content.get(mode, [])\n",
    "            if isinstance(entries, list):\n",
    "                classes.update(\n",
    "                    self._extract_from_entries(entries)\n",
    "                )\n",
    "                \n",
    "        return classes\n",
    "        \n",
    "    def _extract_from_list(\n",
    "        self,\n",
    "        content: List[Any]\n",
    "    ) -> Set[int]:\n",
    "        \"\"\"Extract classes from list structure.\n",
    "        \n",
    "        Args:\n",
    "            content: List content\n",
    "            \n",
    "        Returns:\n",
    "            Set of classes\n",
    "        \"\"\"\n",
    "        return self._extract_from_entries(content)\n",
    "        \n",
    "    def _extract_from_entries(\n",
    "        self,\n",
    "        entries: List[Dict[str, Any]]\n",
    "    ) -> Set[int]:\n",
    "        \"\"\"Extract classes from entries.\n",
    "        \n",
    "        Args:\n",
    "            entries: List of data entries\n",
    "            \n",
    "        Returns:\n",
    "            Set of classes\n",
    "        \"\"\"\n",
    "        classes = set()\n",
    "        \n",
    "        for entry in entries:\n",
    "            output = entry.get(\"output\")\n",
    "            if output is not None:\n",
    "                classes.update(\n",
    "                    self._process_output(output)\n",
    "                )\n",
    "                \n",
    "        return classes\n",
    "        \n",
    "    def _process_output(\n",
    "        self,\n",
    "        output: Union[int, List[Any], np.ndarray]\n",
    "    ) -> Set[int]:\n",
    "        \"\"\"Process output to extract classes.\n",
    "        \n",
    "        Args:\n",
    "            output: Output data\n",
    "            \n",
    "        Returns:\n",
    "            Set of classes\n",
    "        \"\"\"\n",
    "        if isinstance(output, (int, float)):\n",
    "            return {int(output)}\n",
    "        elif isinstance(output, (list, np.ndarray)):\n",
    "            if isinstance(output, np.ndarray):\n",
    "                output = output.flatten().tolist()\n",
    "            return set(map(int, self._flatten_list(output)))\n",
    "        return set()\n",
    "        \n",
    "    def _flatten_list(\n",
    "        self,\n",
    "        lst: List[Any]\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"Flatten nested list.\n",
    "        \n",
    "        Args:\n",
    "            lst: Input list\n",
    "            \n",
    "        Returns:\n",
    "            Flattened list\n",
    "        \"\"\"\n",
    "        flattened = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(self._flatten_list(item))\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "        \n",
    "    def _calculate_statistics(\n",
    "        self,\n",
    "        classes: Set[int]\n",
    "    ) -> ClassStatistics:\n",
    "        \"\"\"Calculate class statistics.\n",
    "        \n",
    "        Args:\n",
    "            classes: Set of classes\n",
    "            \n",
    "        Returns:\n",
    "            Class statistics object\n",
    "        \"\"\"\n",
    "        # Calculate frequencies\n",
    "        frequencies = Counter(classes)\n",
    "        total = sum(frequencies.values())\n",
    "        \n",
    "        # Calculate distribution\n",
    "        distribution = {\n",
    "            cls: count/total\n",
    "            for cls, count in frequencies.items()\n",
    "        }\n",
    "        \n",
    "        # Identify rare and common classes\n",
    "        rare_classes = {\n",
    "            cls for cls, freq in distribution.items()\n",
    "            if freq < self.rare_threshold\n",
    "        }\n",
    "        \n",
    "        common_classes = {\n",
    "            cls for cls, freq in distribution.items()\n",
    "            if freq > self.common_threshold\n",
    "        }\n",
    "        \n",
    "        return ClassStatistics(\n",
    "            num_classes=len(classes),\n",
    "            class_distribution=distribution,\n",
    "            min_class=min(classes),\n",
    "            max_class=max(classes),\n",
    "            class_frequencies=frequencies,\n",
    "            rare_classes=rare_classes,\n",
    "            common_classes=common_classes\n",
    "        )\n",
    "        \n",
    "    def analyze_class_transitions(\n",
    "        self,\n",
    "        arc_data: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze class transitions in sequences.\n",
    "        \n",
    "        Args:\n",
    "            arc_data: ARC dataset dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Transition analysis dictionary\n",
    "        \"\"\"\n",
    "        transitions = defaultdict(Counter)\n",
    "        \n",
    "        for content in arc_data.values():\n",
    "            if isinstance(content, dict):\n",
    "                for mode in [\"train\", \"test\"]:\n",
    "                    entries = content.get(mode, [])\n",
    "                    self._analyze_transitions(entries, transitions)\n",
    "                    \n",
    "        return dict(transitions)\n",
    "        \n",
    "    def _analyze_transitions(\n",
    "        self,\n",
    "        entries: List[Dict[str, Any]],\n",
    "        transitions: Dict[int, Counter]\n",
    "    ) -> None:\n",
    "        \"\"\"Analyze transitions in entries.\n",
    "        \n",
    "        Args:\n",
    "            entries: Data entries\n",
    "            transitions: Transition counter dictionary\n",
    "        \"\"\"\n",
    "        for entry in entries:\n",
    "            output = entry.get(\"output\")\n",
    "            if isinstance(output, list):\n",
    "                for i in range(len(output) - 1):\n",
    "                    current = output[i]\n",
    "                    next_class = output[i + 1]\n",
    "                    transitions[current][next_class] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d909e-6dfc-4fe8-b7d5-97fd89174b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "class GridPattern(Enum):\n",
    "    \"\"\"Types of grid patterns.\"\"\"\n",
    "    RANDOM = \"random\"\n",
    "    DIAGONAL = \"diagonal\"\n",
    "    CHECKERBOARD = \"checkerboard\"\n",
    "    BORDER = \"border\"\n",
    "    GRADIENT = \"gradient\"\n",
    "    SYMMETRIC = \"symmetric\"\n",
    "\n",
    "@dataclass\n",
    "class GridConfig:\n",
    "    \"\"\"Configuration for grid creation.\n",
    "    \n",
    "    Attributes:\n",
    "        num_classes: Number of possible values\n",
    "        min_size: Minimum grid size\n",
    "        max_size: Maximum grid size\n",
    "        patterns: Enabled pattern types\n",
    "        sparsity: Grid sparsity (0-1)\n",
    "        symmetry_probability: Probability of symmetry\n",
    "        noise_level: Level of noise to add\n",
    "    \"\"\"\n",
    "    num_classes: int\n",
    "    min_size: int = 2\n",
    "    max_size: int = 10\n",
    "    patterns: List[GridPattern] = None\n",
    "    sparsity: float = 0.2\n",
    "    symmetry_probability: float = 0.3\n",
    "    noise_level: float = 0.1\n",
    "\n",
    "class GridCreator:\n",
    "    \"\"\"Creator for grid patterns with advanced features.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[GridConfig] = None\n",
    "    ):\n",
    "        \"\"\"Initialize grid creator.\n",
    "        \n",
    "        Args:\n",
    "            config: Optional grid configuration\n",
    "        \"\"\"\n",
    "        self.config = config or GridConfig(num_classes=10)\n",
    "        \n",
    "        if not self.config.patterns:\n",
    "            self.config.patterns = list(GridPattern)\n",
    "            \n",
    "        # Initialize statistics\n",
    "        self.stats = defaultdict(int)\n",
    "        \n",
    "    def create_grids(\n",
    "        self,\n",
    "        task_metadata: Dict[str, Dict[str, Any]]\n",
    "    ) -> Dict[str, Dict[str, List[List[List[int]]]]]:\n",
    "        \"\"\"Create grids based on metadata.\n",
    "        \n",
    "        Args:\n",
    "            task_metadata: Task metadata dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of created grids\n",
    "        \"\"\"\n",
    "        try:\n",
    "            grids_by_task = {}\n",
    "            \n",
    "            for task_id, sections in task_metadata.items():\n",
    "                grids_by_task[task_id] = self._create_task_grids(\n",
    "                    task_id,\n",
    "                    sections\n",
    "                )\n",
    "                \n",
    "            logger.info(\n",
    "                f\"Created grids for {len(grids_by_task)} tasks\"\n",
    "            )\n",
    "            \n",
    "            return grids_by_task\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating grids: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _create_task_grids(\n",
    "        self,\n",
    "        task_id: str,\n",
    "        sections: Dict[str, Any]\n",
    "    ) -> Dict[str, List[List[List[int]]]]:\n",
    "        \"\"\"Create grids for single task.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Task identifier\n",
    "            sections: Section data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of created grids\n",
    "        \"\"\"\n",
    "        task_grids = {}\n",
    "        \n",
    "        for section, data in sections.items():\n",
    "            task_grids[section] = []\n",
    "            \n",
    "            for idx, lengths in data.items():\n",
    "                for length, count in lengths.items():\n",
    "                    task_grids[section].extend(\n",
    "                        self._create_section_grids(length, count)\n",
    "                    )\n",
    "                    \n",
    "        return task_grids\n",
    "        \n",
    "    def _create_section_grids(\n",
    "        self,\n",
    "        size: int,\n",
    "        count: int\n",
    "    ) -> List[List[List[int]]]:\n",
    "        \"\"\"Create grids for section.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            count: Number of grids\n",
    "            \n",
    "        Returns:\n",
    "            List of created grids\n",
    "        \"\"\"\n",
    "        grids = []\n",
    "        \n",
    "        for _ in range(count):\n",
    "            # Select random pattern\n",
    "            pattern = random.choice(self.config.patterns)\n",
    "            \n",
    "            # Create grid with pattern\n",
    "            grid = self._create_pattern_grid(\n",
    "                size,\n",
    "                pattern\n",
    "            )\n",
    "            \n",
    "            # Apply transformations\n",
    "            grid = self._apply_transformations(grid)\n",
    "            \n",
    "            grids.append(grid.tolist())\n",
    "            self.stats[pattern.value] += 1\n",
    "            \n",
    "        return grids\n",
    "        \n",
    "    def _create_pattern_grid(\n",
    "        self,\n",
    "        size: int,\n",
    "        pattern: GridPattern\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create grid with specific pattern.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            pattern: Pattern type\n",
    "            \n",
    "        Returns:\n",
    "            Created grid\n",
    "        \"\"\"\n",
    "        if pattern == GridPattern.RANDOM:\n",
    "            return self._create_random_grid(size)\n",
    "        elif pattern == GridPattern.DIAGONAL:\n",
    "            return self._create_diagonal_grid(size)\n",
    "        elif pattern == GridPattern.CHECKERBOARD:\n",
    "            return self._create_checkerboard_grid(size)\n",
    "        elif pattern == GridPattern.BORDER:\n",
    "            return self._create_border_grid(size)\n",
    "        elif pattern == GridPattern.GRADIENT:\n",
    "            return self._create_gradient_grid(size)\n",
    "        elif pattern == GridPattern.SYMMETRIC:\n",
    "            return self._create_symmetric_grid(size)\n",
    "        else:\n",
    "            return self._create_random_grid(size)\n",
    "            \n",
    "    def _create_random_grid(\n",
    "        self,\n",
    "        size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create random grid.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            \n",
    "        Returns:\n",
    "            Random grid\n",
    "        \"\"\"\n",
    "        grid = np.random.randint(\n",
    "            0,\n",
    "            self.config.num_classes,\n",
    "            size=(size, size)\n",
    "        )\n",
    "        \n",
    "        # Apply sparsity\n",
    "        if self.config.sparsity > 0:\n",
    "            mask = np.random.random(size=(size, size)) < self.config.sparsity\n",
    "            grid[mask] = 0\n",
    "            \n",
    "        return grid\n",
    "        \n",
    "    def _create_diagonal_grid(\n",
    "        self,\n",
    "        size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create diagonal pattern grid.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            \n",
    "        Returns:\n",
    "            Diagonal grid\n",
    "        \"\"\"\n",
    "        grid = np.zeros((size, size), dtype=int)\n",
    "        value = random.randint(1, self.config.num_classes - 1)\n",
    "        \n",
    "        for i in range(size):\n",
    "            grid[i, i] = value\n",
    "            if random.random() < 0.5:\n",
    "                grid[i, size-1-i] = value\n",
    "                \n",
    "        return grid\n",
    "        \n",
    "    def _create_checkerboard_grid(\n",
    "        self,\n",
    "        size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create checkerboard pattern grid.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            \n",
    "        Returns:\n",
    "            Checkerboard grid\n",
    "        \"\"\"\n",
    "        grid = np.zeros((size, size), dtype=int)\n",
    "        values = random.sample(\n",
    "            range(1, self.config.num_classes),\n",
    "            k=2\n",
    "        )\n",
    "        \n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                grid[i, j] = values[(i + j) % 2]\n",
    "                \n",
    "        return grid\n",
    "        \n",
    "    def _create_border_grid(\n",
    "        self,\n",
    "        size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create border pattern grid.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            \n",
    "        Returns:\n",
    "            Border grid\n",
    "        \"\"\"\n",
    "        grid = np.zeros((size, size), dtype=int)\n",
    "        value = random.randint(1, self.config.num_classes - 1)\n",
    "        \n",
    "        grid[0, :] = value\n",
    "        grid[-1, :] = value\n",
    "        grid[:, 0] = value\n",
    "        grid[:, -1] = value\n",
    "        \n",
    "        return grid\n",
    "        \n",
    "    def _create_gradient_grid(\n",
    "        self,\n",
    "        size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create gradient pattern grid.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            \n",
    "        Returns:\n",
    "            Gradient grid\n",
    "        \"\"\"\n",
    "        x = np.linspace(0, 1, size)\n",
    "        y = np.linspace(0, 1, size)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        gradient = (X + Y) / 2\n",
    "        return (gradient * (self.config.num_classes - 1)).astype(int)\n",
    "        \n",
    "    def _create_symmetric_grid(\n",
    "        self,\n",
    "        size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Create symmetric pattern grid.\n",
    "        \n",
    "        Args:\n",
    "            size: Grid size\n",
    "            \n",
    "        Returns:\n",
    "            Symmetric grid\n",
    "        \"\"\"\n",
    "        half_size = (size + 1) // 2\n",
    "        half_grid = self._create_random_grid(half_size)\n",
    "        \n",
    "        # Mirror horizontally\n",
    "        grid = np.concatenate(\n",
    "            [half_grid, np.fliplr(half_grid[:, :size//2])],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return grid[:size, :size]\n",
    "        \n",
    "    def _apply_transformations(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Apply random transformations.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Transformed grid\n",
    "        \"\"\"\n",
    "        # Add noise\n",
    "        if self.config.noise_level > 0:\n",
    "            noise_mask = np.random.random(grid.shape) < self.config.noise_level\n",
    "            noise = np.random.randint(\n",
    "                0,\n",
    "                self.config.num_classes,\n",
    "                size=grid.shape\n",
    "            )\n",
    "            grid[noise_mask] = noise[noise_mask]\n",
    "            \n",
    "        # Apply symmetry\n",
    "        if random.random() < self.config.symmetry_probability:\n",
    "            if random.random() < 0.5:\n",
    "                # Horizontal symmetry\n",
    "                grid = np.concatenate(\n",
    "                    [grid, np.flipud(grid)],\n",
    "                    axis=0\n",
    "                )\n",
    "            else:\n",
    "                # Vertical symmetry\n",
    "                grid = np.concatenate(\n",
    "                    [grid, np.fliplr(grid)],\n",
    "                    axis=1\n",
    "                )\n",
    "                \n",
    "        return grid\n",
    "        \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get creation statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'pattern_counts': dict(self.stats),\n",
    "            'total_grids': sum(self.stats.values()),\n",
    "            'pattern_distribution': {\n",
    "                pattern: count/sum(self.stats.values())\n",
    "                for pattern, count in self.stats.items()\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c7bcf-4d8a-4a3a-8461-37f0c7b480d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional, Tuple\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GridShape:\n",
    "    \"\"\"Container for grid shape information.\n",
    "    \n",
    "    Attributes:\n",
    "        original_shape: Original grid dimensions\n",
    "        flattened_size: Size after flattening\n",
    "        padding: Padding added (if any)\n",
    "        dtype: Data type of grid elements\n",
    "    \"\"\"\n",
    "    original_shape: Tuple[int, ...]\n",
    "    flattened_size: int\n",
    "    padding: Optional[int] = None\n",
    "    dtype: np.dtype = np.int32\n",
    "\n",
    "class GridFlattener:\n",
    "    \"\"\"Utility for flattening and processing grid data.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pad_value: int = 0,\n",
    "        normalize: bool = False,\n",
    "        preserve_shape: bool = True\n",
    "    ):\n",
    "        \"\"\"Initialize grid flattener.\n",
    "        \n",
    "        Args:\n",
    "            pad_value: Value to use for padding\n",
    "            normalize: Whether to normalize values\n",
    "            preserve_shape: Whether to store shape information\n",
    "        \"\"\"\n",
    "        self.pad_value = pad_value\n",
    "        self.normalize = normalize\n",
    "        self.preserve_shape = preserve_shape\n",
    "        self.shape_cache = {}\n",
    "        \n",
    "    def flatten(\n",
    "        self,\n",
    "        grid: Union[List[List[int]], np.ndarray],\n",
    "        grid_id: Optional[str] = None\n",
    "    ) -> Union[List[int], np.ndarray]:\n",
    "        \"\"\"Flatten 2D grid to 1D.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            grid_id: Optional identifier for shape caching\n",
    "            \n",
    "        Returns:\n",
    "            Flattened grid\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If grid is invalid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to numpy array if needed\n",
    "            grid_array = self._to_array(grid)\n",
    "            \n",
    "            # Validate grid\n",
    "            self._validate_grid(grid_array)\n",
    "            \n",
    "            # Store shape information if needed\n",
    "            if self.preserve_shape and grid_id:\n",
    "                self._store_shape(grid_id, grid_array)\n",
    "                \n",
    "            # Flatten grid\n",
    "            flattened = grid_array.flatten()\n",
    "            \n",
    "            # Normalize if requested\n",
    "            if self.normalize:\n",
    "                flattened = self._normalize(flattened)\n",
    "                \n",
    "            return flattened.tolist() if isinstance(grid, list) else flattened\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error flattening grid: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def unflatten(\n",
    "        self,\n",
    "        flat_data: Union[List[int], np.ndarray],\n",
    "        shape: Optional[Tuple[int, ...]] = None,\n",
    "        grid_id: Optional[str] = None\n",
    "    ) -> Union[List[List[int]], np.ndarray]:\n",
    "        \"\"\"Reconstruct 2D grid from flattened data.\n",
    "        \n",
    "        Args:\n",
    "            flat_data: Flattened data\n",
    "            shape: Target shape (optional)\n",
    "            grid_id: Grid identifier for shape lookup\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed grid\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If shape information is missing\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get shape information\n",
    "            if shape is None and grid_id:\n",
    "                shape = self._get_shape(grid_id)\n",
    "            if shape is None:\n",
    "                raise ValueError(\"Shape information required for unflattening\")\n",
    "                \n",
    "            # Convert to array\n",
    "            flat_array = np.array(flat_data)\n",
    "            \n",
    "            # Pad if necessary\n",
    "            required_size = np.prod(shape)\n",
    "            if len(flat_array) < required_size:\n",
    "                flat_array = self._pad_array(\n",
    "                    flat_array,\n",
    "                    required_size\n",
    "                )\n",
    "                \n",
    "            # Reshape array\n",
    "            grid = flat_array.reshape(shape)\n",
    "            \n",
    "            return grid.tolist() if isinstance(flat_data, list) else grid\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error unflattening data: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _to_array(\n",
    "        self,\n",
    "        grid: Union[List[List[int]], np.ndarray]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Convert grid to numpy array.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array\n",
    "        \"\"\"\n",
    "        if isinstance(grid, list):\n",
    "            return np.array(grid, dtype=np.float32 if self.normalize else np.int32)\n",
    "        return grid\n",
    "        \n",
    "    def _validate_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Validate grid data.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If grid is invalid\n",
    "        \"\"\"\n",
    "        if grid.ndim < 2:\n",
    "            raise ValueError(\"Grid must be at least 2-dimensional\")\n",
    "            \n",
    "        if not np.issubdtype(grid.dtype, np.number):\n",
    "            raise ValueError(\"Grid must contain numeric values\")\n",
    "            \n",
    "        if np.any(np.isnan(grid)):\n",
    "            raise ValueError(\"Grid contains NaN values\")\n",
    "            \n",
    "    def _store_shape(\n",
    "        self,\n",
    "        grid_id: str,\n",
    "        grid: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Store grid shape information.\n",
    "        \n",
    "        Args:\n",
    "            grid_id: Grid identifier\n",
    "            grid: Input grid\n",
    "        \"\"\"\n",
    "        self.shape_cache[grid_id] = GridShape(\n",
    "            original_shape=grid.shape,\n",
    "            flattened_size=grid.size,\n",
    "            dtype=grid.dtype\n",
    "        )\n",
    "        \n",
    "    def _get_shape(\n",
    "        self,\n",
    "        grid_id: str\n",
    "    ) -> Optional[Tuple[int, ...]]:\n",
    "        \"\"\"Get stored shape information.\n",
    "        \n",
    "        Args:\n",
    "            grid_id: Grid identifier\n",
    "            \n",
    "        Returns:\n",
    "            Grid shape tuple\n",
    "        \"\"\"\n",
    "        if grid_id in self.shape_cache:\n",
    "            return self.shape_cache[grid_id].original_shape\n",
    "        return None\n",
    "        \n",
    "    def _normalize(\n",
    "        self,\n",
    "        array: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Normalize array values.\n",
    "        \n",
    "        Args:\n",
    "            array: Input array\n",
    "            \n",
    "        Returns:\n",
    "            Normalized array\n",
    "        \"\"\"\n",
    "        if len(array) == 0:\n",
    "            return array\n",
    "            \n",
    "        min_val = np.min(array)\n",
    "        max_val = np.max(array)\n",
    "        \n",
    "        if min_val == max_val:\n",
    "            return np.zeros_like(array, dtype=np.float32)\n",
    "            \n",
    "        return (array - min_val) / (max_val - min_val)\n",
    "        \n",
    "    def _pad_array(\n",
    "        self,\n",
    "        array: np.ndarray,\n",
    "        target_size: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Pad array to target size.\n",
    "        \n",
    "        Args:\n",
    "            array: Input array\n",
    "            target_size: Desired size\n",
    "            \n",
    "        Returns:\n",
    "            Padded array\n",
    "        \"\"\"\n",
    "        if len(array) >= target_size:\n",
    "            return array\n",
    "            \n",
    "        padding_size = target_size - len(array)\n",
    "        return np.pad(\n",
    "            array,\n",
    "            (0, padding_size),\n",
    "            mode='constant',\n",
    "            constant_values=self.pad_value\n",
    "        )\n",
    "        \n",
    "    def get_statistics(\n",
    "        self,\n",
    "        grid: Union[List[List[int]], np.ndarray]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate grid statistics.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            grid_array = self._to_array(grid)\n",
    "            \n",
    "            return {\n",
    "                'min': float(np.min(grid_array)),\n",
    "                'max': float(np.max(grid_array)),\n",
    "                'mean': float(np.mean(grid_array)),\n",
    "                'std': float(np.std(grid_array)),\n",
    "                'sparsity': float(np.mean(grid_array == 0)),\n",
    "                'unique_values': int(len(np.unique(grid_array)))\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating grid statistics: {e}\")\n",
    "            return {}\n",
    "            \n",
    "    def batch_flatten(\n",
    "        self,\n",
    "        grids: List[Union[List[List[int]], np.ndarray]]\n",
    "    ) -> List[Union[List[int], np.ndarray]]:\n",
    "        \"\"\"Flatten multiple grids.\n",
    "        \n",
    "        Args:\n",
    "            grids: List of grids\n",
    "            \n",
    "        Returns:\n",
    "            List of flattened grids\n",
    "        \"\"\"\n",
    "        flattened = []\n",
    "        for i, grid in enumerate(grids):\n",
    "            try:\n",
    "                flat = self.flatten(grid, grid_id=f\"batch_{i}\")\n",
    "                flattened.append(flat)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error flattening grid {i}: {e}\")\n",
    "                continue\n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18abf7d-f750-4b50-ae37-46f1def26f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grid_metadata(arc_data):\n",
    "    \"\"\"\n",
    "    Extract task IDs, list lengths, and organize grids by their sizes.\n",
    "    \"\"\"\n",
    "    task_metadata = {}\n",
    "\n",
    "    for key, tasks in arc_data.items():\n",
    "        for task_id, content in tasks.items():\n",
    "            task_metadata[task_id] = {\"sizes\": {}, \"grids\": {}}\n",
    "\n",
    "            for mode in [\"train\", \"test\"]:\n",
    "                if mode in content:\n",
    "                    for entry in content[mode]:\n",
    "                        input_grid = entry.get(\"input\", [])\n",
    "                        output_grid = entry.get(\"output\", [])\n",
    "\n",
    "                        input_size = (len(input_grid), len(input_grid[0])) if input_grid else (0, 0)\n",
    "                        output_size = (len(output_grid), len(output_grid[0])) if output_grid else (0, 0)\n",
    "\n",
    "                        if input_size not in task_metadata[task_id][\"sizes\"]:\n",
    "                            task_metadata[task_id][\"sizes\"][input_size] = 0\n",
    "                        if output_size not in task_metadata[task_id][\"sizes\"]:\n",
    "                            task_metadata[task_id][\"sizes\"][output_size] = 0\n",
    "\n",
    "                        task_metadata[task_id][\"sizes\"][input_size] += 1\n",
    "                        task_metadata[task_id][\"sizes\"][output_size] += 1\n",
    "\n",
    "                        # Store grids\n",
    "                        if input_size not in task_metadata[task_id][\"grids\"]:\n",
    "                            task_metadata[task_id][\"grids\"][input_size] = {'grids': []}\n",
    "                        task_metadata[task_id][\"grids\"][input_size]['grids'].append(input_grid)\n",
    "\n",
    "                        if output_size not in task_metadata[task_id][\"grids\"]:\n",
    "                            task_metadata[task_id][\"grids\"][output_size] = {'grids': []}\n",
    "                        task_metadata[task_id][\"grids\"][output_size]['grids'].append(output_grid)\n",
    "\n",
    "    return task_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd95034-366f-48a5-83f2-fbc16b8c17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any, Iterator\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GridMetadata:\n",
    "    \"\"\"Metadata for grid data.\n",
    "    \n",
    "    Attributes:\n",
    "        shape: Grid dimensions\n",
    "        dtype: Data type\n",
    "        timestamp: Creation timestamp\n",
    "        source: Data source\n",
    "        transformations: Applied transformations\n",
    "    \"\"\"\n",
    "    shape: tuple\n",
    "    dtype: np.dtype\n",
    "    timestamp: str\n",
    "    source: str\n",
    "    transformations: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class NodeStats:\n",
    "    \"\"\"Statistics for node data.\n",
    "    \n",
    "    Attributes:\n",
    "        total_grids: Total number of grids\n",
    "        total_children: Total number of child nodes\n",
    "        depth: Node depth in tree\n",
    "        leaf_count: Number of leaf nodes\n",
    "        grid_sizes: List of grid sizes\n",
    "    \"\"\"\n",
    "    total_grids: int\n",
    "    total_children: int\n",
    "    depth: int\n",
    "    leaf_count: int\n",
    "    grid_sizes: List[tuple]\n",
    "\n",
    "class DataNode:\n",
    "    \"\"\"Enhanced node class for hierarchical data storage.\n",
    "    \n",
    "    This class represents a node in a tree structure, supporting\n",
    "    grid data storage, metadata tracking, and tree operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        task_id: Optional[str] = None,\n",
    "        parent: Optional['DataNode'] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "        \"\"\"Initialize data node.\n",
    "        \n",
    "        Args:\n",
    "            name: Node name\n",
    "            task_id: Optional task identifier\n",
    "            parent: Optional parent node\n",
    "            metadata: Optional metadata dictionary\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.task_id = task_id\n",
    "        self.parent = parent\n",
    "        self.children: List['DataNode'] = []\n",
    "        self.grids: List[np.ndarray] = []\n",
    "        self.grid_metadata: List[GridMetadata] = []\n",
    "        self.metadata = metadata or {}\n",
    "        self.processed = False\n",
    "        \n",
    "        logger.info(f\"Created node: {self.name} (ID: {self.task_id})\")\n",
    "        \n",
    "    def add_child(\n",
    "        self,\n",
    "        child: 'DataNode'\n",
    "    ) -> None:\n",
    "        \"\"\"Add child node.\n",
    "        \n",
    "        Args:\n",
    "            child: Child node to add\n",
    "        \"\"\"\n",
    "        child.parent = self\n",
    "        self.children.append(child)\n",
    "        logger.debug(f\"Added child {child.name} to {self.name}\")\n",
    "        \n",
    "    def add_grid(\n",
    "        self,\n",
    "        grid: np.ndarray,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add grid with metadata.\n",
    "        \n",
    "        Args:\n",
    "            grid: Grid data to add\n",
    "            metadata: Optional grid metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to numpy array if needed\n",
    "            if not isinstance(grid, np.ndarray):\n",
    "                grid = np.array(grid)\n",
    "                \n",
    "            # Create grid metadata\n",
    "            grid_meta = GridMetadata(\n",
    "                shape=grid.shape,\n",
    "                dtype=grid.dtype,\n",
    "                timestamp=self._get_timestamp(),\n",
    "                source=metadata.get('source', 'unknown') if metadata else 'unknown',\n",
    "                transformations=[]\n",
    "            )\n",
    "            \n",
    "            self.grids.append(grid)\n",
    "            self.grid_metadata.append(grid_meta)\n",
    "            \n",
    "            logger.debug(\n",
    "                f\"Added grid to {self.name} \"\n",
    "                f\"with shape {grid.shape}\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding grid to {self.name}: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def remove_child(\n",
    "        self,\n",
    "        child: 'DataNode'\n",
    "    ) -> None:\n",
    "        \"\"\"Remove child node.\n",
    "        \n",
    "        Args:\n",
    "            child: Child node to remove\n",
    "        \"\"\"\n",
    "        if child in self.children:\n",
    "            child.parent = None\n",
    "            self.children.remove(child)\n",
    "            logger.debug(f\"Removed child {child.name} from {self.name}\")\n",
    "            \n",
    "    def remove_grid(\n",
    "        self,\n",
    "        index: int\n",
    "    ) -> None:\n",
    "        \"\"\"Remove grid by index.\n",
    "        \n",
    "        Args:\n",
    "            index: Index of grid to remove\n",
    "        \"\"\"\n",
    "        if 0 <= index < len(self.grids):\n",
    "            del self.grids[index]\n",
    "            del self.grid_metadata[index]\n",
    "            logger.debug(f\"Removed grid {index} from {self.name}\")\n",
    "            \n",
    "    def get_path(self) -> List[str]:\n",
    "        \"\"\"Get path from root to this node.\n",
    "        \n",
    "        Returns:\n",
    "            List of node names in path\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        current = self\n",
    "        while current:\n",
    "            path.append(current.name)\n",
    "            current = current.parent\n",
    "        return list(reversed(path))\n",
    "        \n",
    "    def find_node(\n",
    "        self,\n",
    "        name: str\n",
    "    ) -> Optional['DataNode']:\n",
    "        \"\"\"Find node by name.\n",
    "        \n",
    "        Args:\n",
    "            name: Name to search for\n",
    "            \n",
    "        Returns:\n",
    "            Matching node or None\n",
    "        \"\"\"\n",
    "        if self.name == name:\n",
    "            return self\n",
    "            \n",
    "        for child in self.children:\n",
    "            result = child.find_node(name)\n",
    "            if result:\n",
    "                return result\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    def get_ancestors(self) -> List['DataNode']:\n",
    "        \"\"\"Get list of ancestor nodes.\n",
    "        \n",
    "        Returns:\n",
    "            List of ancestor nodes\n",
    "        \"\"\"\n",
    "        ancestors = []\n",
    "        current = self.parent\n",
    "        while current:\n",
    "            ancestors.append(current)\n",
    "            current = current.parent\n",
    "        return ancestors\n",
    "        \n",
    "    def get_descendants(self) -> List['DataNode']:\n",
    "        \"\"\"Get list of descendant nodes.\n",
    "        \n",
    "        Returns:\n",
    "            List of descendant nodes\n",
    "        \"\"\"\n",
    "        descendants = []\n",
    "        for child in self.children:\n",
    "            descendants.append(child)\n",
    "            descendants.extend(child.get_descendants())\n",
    "        return descendants\n",
    "        \n",
    "    def get_siblings(self) -> List['DataNode']:\n",
    "        \"\"\"Get list of sibling nodes.\n",
    "        \n",
    "        Returns:\n",
    "            List of sibling nodes\n",
    "        \"\"\"\n",
    "        if not self.parent:\n",
    "            return []\n",
    "        return [\n",
    "            child for child in self.parent.children\n",
    "            if child is not self\n",
    "        ]\n",
    "        \n",
    "    def get_stats(self) -> NodeStats:\n",
    "        \"\"\"Get node statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Node statistics object\n",
    "        \"\"\"\n",
    "        stats = NodeStats(\n",
    "            total_grids=len(self.grids),\n",
    "            total_children=len(self.children),\n",
    "            depth=self._get_depth(),\n",
    "            leaf_count=self._count_leaves(),\n",
    "            grid_sizes=[grid.shape for grid in self.grids]\n",
    "        )\n",
    "        return stats\n",
    "        \n",
    "    def _get_depth(self) -> int:\n",
    "        \"\"\"Calculate node depth.\n",
    "        \n",
    "        Returns:\n",
    "            Node depth\n",
    "        \"\"\"\n",
    "        depth = 0\n",
    "        current = self\n",
    "        while current.parent:\n",
    "            depth += 1\n",
    "            current = current.parent\n",
    "        return depth\n",
    "        \n",
    "    def _count_leaves(self) -> int:\n",
    "        \"\"\"Count leaf nodes in subtree.\n",
    "        \n",
    "        Returns:\n",
    "            Number of leaf nodes\n",
    "        \"\"\"\n",
    "        if not self.children:\n",
    "            return 1\n",
    "        return sum(child._count_leaves() for child in self.children)\n",
    "        \n",
    "    def traverse_breadth_first(self) -> Iterator['DataNode']:\n",
    "        \"\"\"Traverse tree breadth-first.\n",
    "        \n",
    "        Yields:\n",
    "            Nodes in breadth-first order\n",
    "        \"\"\"\n",
    "        queue = deque([self])\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            yield node\n",
    "            queue.extend(node.children)\n",
    "            \n",
    "    def traverse_depth_first(self) -> Iterator['DataNode']:\n",
    "        \"\"\"Traverse tree depth-first.\n",
    "        \n",
    "        Yields:\n",
    "            Nodes in depth-first order\n",
    "        \"\"\"\n",
    "        yield self\n",
    "        for child in self.children:\n",
    "            yield from child.traverse_depth_first()\n",
    "            \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert node to dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary representation\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'task_id': self.task_id,\n",
    "            'metadata': self.metadata,\n",
    "            'grid_count': len(self.grids),\n",
    "            'children': [\n",
    "                child.to_dict() for child in self.children\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def save_to_file(\n",
    "        self,\n",
    "        filepath: str\n",
    "    ) -> None:\n",
    "        \"\"\"Save node to file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Output file path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = self.to_dict()\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            logger.info(f\"Saved node data to {filepath}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving node data: {e}\")\n",
    "            \n",
    "    @classmethod\n",
    "    def load_from_file(\n",
    "        cls,\n",
    "        filepath: str\n",
    "    ) -> 'DataNode':\n",
    "        \"\"\"Load node from file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Input file path\n",
    "            \n",
    "        Returns:\n",
    "            Loaded node\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            return cls._from_dict(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading node data: {e}\")\n",
    "            raise\n",
    "            \n",
    "    @classmethod\n",
    "    def _from_dict(\n",
    "        cls,\n",
    "        data: Dict[str, Any]\n",
    "    ) -> 'DataNode':\n",
    "        \"\"\"Create node from dictionary.\n",
    "        \n",
    "        Args:\n",
    "            data: Input dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Created node\n",
    "        \"\"\"\n",
    "        node = cls(\n",
    "            name=data['name'],\n",
    "            task_id=data['task_id'],\n",
    "            metadata=data.get('metadata')\n",
    "        )\n",
    "        \n",
    "        for child_data in data.get('children', []):\n",
    "            child = cls._from_dict(child_data)\n",
    "            node.add_child(child)\n",
    "            \n",
    "        return node\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of node.\"\"\"\n",
    "        return (\n",
    "            f\"{self.name} \"\n",
    "            f\"(ID: {self.task_id}, \"\n",
    "            f\"Grids: {len(self.grids)}, \"\n",
    "            f\"Children: {len(self.children)})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c19d21-eb50-4cf2-ad48-fca63a56eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TreeConfig:\n",
    "    \"\"\"Configuration for data tree building.\n",
    "    \n",
    "    Attributes:\n",
    "        max_depth: Maximum tree depth\n",
    "        parallel_processing: Whether to use parallel processing\n",
    "        validate_data: Whether to validate data\n",
    "        store_metadata: Whether to store metadata\n",
    "        cache_enabled: Whether to enable caching\n",
    "    \"\"\"\n",
    "    max_depth: int = 5\n",
    "    parallel_processing: bool = True\n",
    "    validate_data: bool = True\n",
    "    store_metadata: bool = True\n",
    "    cache_enabled: bool = True\n",
    "\n",
    "class DataTreeBuilder:\n",
    "    \"\"\"Builder for hierarchical data trees.\"\"\"\n",
    "    \n",
    "    GRID_CATEGORIES = [\n",
    "        \"train_input_grids\",\n",
    "        \"train_output_grids\",\n",
    "        \"test_input_grids\",\n",
    "        \"test_output_grids\"\n",
    "    ]\n",
    "    \n",
    "    LIST_CATEGORIES = [\n",
    "        \"train_input_lists\",\n",
    "        \"train_output_lists\",\n",
    "        \"test_input_lists\",\n",
    "        \"test_output_lists\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[TreeConfig] = None\n",
    "    ):\n",
    "        \"\"\"Initialize tree builder.\n",
    "        \n",
    "        Args:\n",
    "            config: Optional builder configuration\n",
    "        \"\"\"\n",
    "        self.config = config or TreeConfig()\n",
    "        self.cache = {} if self.config.cache_enabled else None\n",
    "        self.stats = defaultdict(int)\n",
    "        \n",
    "    def build_tree(\n",
    "        self,\n",
    "        task_metadata: Dict[str, Dict[str, Any]]\n",
    "    ) -> DataNode:\n",
    "        \"\"\"Build data tree from task metadata.\n",
    "        \n",
    "        Args:\n",
    "            task_metadata: Task metadata dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Root node of built tree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting tree construction\")\n",
    "            \n",
    "            # Create root node\n",
    "            root = DataNode(\n",
    "                \"ARC Data\",\n",
    "                metadata={'total_tasks': len(task_metadata)}\n",
    "            )\n",
    "            \n",
    "            # Build tree\n",
    "            if self.config.parallel_processing:\n",
    "                self._build_parallel(root, task_metadata)\n",
    "            else:\n",
    "                self._build_sequential(root, task_metadata)\n",
    "                \n",
    "            # Log statistics\n",
    "            self._log_tree_stats(root)\n",
    "            \n",
    "            return root\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building tree: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _build_sequential(\n",
    "        self,\n",
    "        root: DataNode,\n",
    "        task_metadata: Dict[str, Dict[str, Any]]\n",
    "    ) -> None:\n",
    "        \"\"\"Build tree sequentially.\n",
    "        \n",
    "        Args:\n",
    "            root: Root node\n",
    "            task_metadata: Task metadata\n",
    "        \"\"\"\n",
    "        for task_id, metadata in task_metadata.items():\n",
    "            try:\n",
    "                self._process_task(root, task_id, metadata)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing task {task_id}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    def _build_parallel(\n",
    "        self,\n",
    "        root: DataNode,\n",
    "        task_metadata: Dict[str, Dict[str, Any]]\n",
    "    ) -> None:\n",
    "        \"\"\"Build tree using parallel processing.\n",
    "        \n",
    "        Args:\n",
    "            root: Root node\n",
    "            task_metadata: Task metadata\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for task_id, metadata in task_metadata.items():\n",
    "                future = executor.submit(\n",
    "                    self._process_task,\n",
    "                    root,\n",
    "                    task_id,\n",
    "                    metadata\n",
    "                )\n",
    "                futures.append((task_id, future))\n",
    "                \n",
    "            # Process results\n",
    "            for task_id, future in futures:\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    logger.error(\n",
    "                        f\"Error processing task {task_id}: {e}\"\n",
    "                    )\n",
    "                    \n",
    "    def _process_task(\n",
    "        self,\n",
    "        root: DataNode,\n",
    "        task_id: str,\n",
    "        metadata: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Process single task.\n",
    "        \n",
    "        Args:\n",
    "            root: Root node\n",
    "            task_id: Task identifier\n",
    "            metadata: Task metadata\n",
    "        \"\"\"\n",
    "        # Create task node\n",
    "        task_node = DataNode(\n",
    "            \"Task\",\n",
    "            task_id,\n",
    "            metadata=self._extract_task_metadata(metadata)\n",
    "        )\n",
    "        root.add_child(task_node)\n",
    "        \n",
    "        # Process grid categories\n",
    "        self._process_categories(\n",
    "            task_node,\n",
    "            metadata,\n",
    "            self.GRID_CATEGORIES,\n",
    "            is_grid=True\n",
    "        )\n",
    "        \n",
    "        # Process list categories\n",
    "        self._process_categories(\n",
    "            task_node,\n",
    "            metadata,\n",
    "            self.LIST_CATEGORIES,\n",
    "            is_grid=False\n",
    "        )\n",
    "        \n",
    "    def _process_categories(\n",
    "        self,\n",
    "        task_node: DataNode,\n",
    "        metadata: Dict[str, Any],\n",
    "        categories: List[str],\n",
    "        is_grid: bool\n",
    "    ) -> None:\n",
    "        \"\"\"Process data categories.\n",
    "        \n",
    "        Args:\n",
    "            task_node: Task node\n",
    "            metadata: Task metadata\n",
    "            categories: Category list\n",
    "            is_grid: Whether processing grids\n",
    "        \"\"\"\n",
    "        for category in categories:\n",
    "            try:\n",
    "                data = metadata.get(category, [])\n",
    "                if data:\n",
    "                    category_node = DataNode(\n",
    "                        category,\n",
    "                        metadata={'type': 'grid' if is_grid else 'list'}\n",
    "                    )\n",
    "                    task_node.add_child(category_node)\n",
    "                    \n",
    "                    self._add_data_to_node(\n",
    "                        category_node,\n",
    "                        data,\n",
    "                        is_grid\n",
    "                    )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Error processing category {category}: {e}\"\n",
    "                )\n",
    "                \n",
    "    def _add_data_to_node(\n",
    "        self,\n",
    "        node: DataNode,\n",
    "        data: List[Any],\n",
    "        is_grid: bool\n",
    "    ) -> None:\n",
    "        \"\"\"Add data to node.\n",
    "        \n",
    "        Args:\n",
    "            node: Target node\n",
    "            data: Data to add\n",
    "            is_grid: Whether data is grid\n",
    "        \"\"\"\n",
    "        for item in data:\n",
    "            try:\n",
    "                processed_item = (\n",
    "                    self._process_grid(item)\n",
    "                    if is_grid else\n",
    "                    self._process_list(item)\n",
    "                )\n",
    "                \n",
    "                if self.config.validate_data:\n",
    "                    self._validate_data(processed_item)\n",
    "                    \n",
    "                node.add_grid(\n",
    "                    processed_item,\n",
    "                    metadata={'is_grid': is_grid}\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error adding data: {e}\")\n",
    "                \n",
    "    def _process_grid(\n",
    "        self,\n",
    "        grid: List[List[Any]]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Process grid data.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Processed grid array\n",
    "        \"\"\"\n",
    "        if self.config.cache_enabled:\n",
    "            grid_hash = hash(str(grid))\n",
    "            if grid_hash in self.cache:\n",
    "                return self.cache[grid_hash]\n",
    "                \n",
    "        processed = np.array(grid)\n",
    "        \n",
    "        if self.config.cache_enabled:\n",
    "            self.cache[grid_hash] = processed\n",
    "            \n",
    "        return processed\n",
    "        \n",
    "    def _process_list(\n",
    "        self,\n",
    "        lst: List[Any]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Process list data.\n",
    "        \n",
    "        Args:\n",
    "            lst: Input list\n",
    "            \n",
    "        Returns:\n",
    "            Processed array\n",
    "        \"\"\"\n",
    "        return np.array(lst)\n",
    "        \n",
    "    def _validate_data(\n",
    "        self,\n",
    "        data: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Validate data array.\n",
    "        \n",
    "        Args:\n",
    "            data: Data array to validate\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If validation fails\n",
    "        \"\"\"\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            raise ValueError(\"Data must be numpy array\")\n",
    "            \n",
    "        if data.size == 0:\n",
    "            raise ValueError(\"Empty data array\")\n",
    "            \n",
    "        if np.any(np.isnan(data)):\n",
    "            raise ValueError(\"Data contains NaN values\")\n",
    "            \n",
    "    def _extract_task_metadata(\n",
    "        self,\n",
    "        metadata: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Extract task metadata.\n",
    "        \n",
    "        Args:\n",
    "            metadata: Raw metadata\n",
    "            \n",
    "        Returns:\n",
    "            Processed metadata dictionary\n",
    "        \"\"\"\n",
    "        if not self.config.store_metadata:\n",
    "            return {}\n",
    "            \n",
    "        return {\n",
    "            'grid_counts': {\n",
    "                category: len(metadata.get(category, []))\n",
    "                for category in self.GRID_CATEGORIES\n",
    "            },\n",
    "            'list_counts': {\n",
    "                category: len(metadata.get(category, []))\n",
    "                for category in self.LIST_CATEGORIES\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def _log_tree_stats(\n",
    "        self,\n",
    "        root: DataNode\n",
    "    ) -> None:\n",
    "        \"\"\"Log tree statistics.\n",
    "        \n",
    "        Args:\n",
    "            root: Root node\n",
    "        \"\"\"\n",
    "        stats = root.get_stats()\n",
    "        logger.info(\n",
    "            f\"Tree built with {stats.total_grids} grids, \"\n",
    "            f\"{stats.total_children} nodes, \"\n",
    "            f\"depth {stats.depth}\"\n",
    "        )\n",
    "        \n",
    "    def get_tree_summary(\n",
    "        self,\n",
    "        root: DataNode\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Get tree summary.\n",
    "        \n",
    "        Args:\n",
    "            root: Root node\n",
    "            \n",
    "        Returns:\n",
    "            Summary dictionary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'total_tasks': len(root.children),\n",
    "            'total_grids': sum(\n",
    "                len(node.grids)\n",
    "                for node in root.traverse_depth_first()\n",
    "            ),\n",
    "            'max_depth': max(\n",
    "                node._get_depth()\n",
    "                for node in root.traverse_depth_first()\n",
    "            ),\n",
    "            'leaf_nodes': sum(\n",
    "                1 for node in root.traverse_depth_first()\n",
    "                if not node.children\n",
    "            ),\n",
    "            'grid_distribution': self._get_grid_distribution(root)\n",
    "        }\n",
    "        \n",
    "    def _get_grid_distribution(\n",
    "        self,\n",
    "        root: DataNode\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"Get grid distribution by category.\n",
    "        \n",
    "        Args:\n",
    "            root: Root node\n",
    "            \n",
    "        Returns:\n",
    "            Distribution dictionary\n",
    "        \"\"\"\n",
    "        distribution = defaultdict(int)\n",
    "        \n",
    "        for node in root.traverse_depth_first():\n",
    "            if node.name in self.GRID_CATEGORIES:\n",
    "                distribution[node.name] = len(node.grids)\n",
    "                \n",
    "        return dict(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f420a121-798c-4d08-b1c2-83e46ae229b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for dynamic grid dataset.\n",
    "    \n",
    "    Attributes:\n",
    "        augmentation_enabled: Whether to use augmentation\n",
    "        cache_enabled: Whether to enable caching\n",
    "        normalize_grids: Whether to normalize grid values\n",
    "        min_grid_size: Minimum grid size to include\n",
    "        max_grid_size: Maximum grid size to include\n",
    "        include_metadata: Whether to include metadata\n",
    "    \"\"\"\n",
    "    augmentation_enabled: bool = False\n",
    "    cache_enabled: bool = True\n",
    "    normalize_grids: bool = True\n",
    "    min_grid_size: Optional[Tuple[int, int]] = None\n",
    "    max_grid_size: Optional[Tuple[int, int]] = None\n",
    "    include_metadata: bool = True\n",
    "\n",
    "class DynamicGridDataset(Dataset):\n",
    "    \"\"\"Dynamic dataset for grid data with enhanced features.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_tree: Any,\n",
    "        config: Optional[DatasetConfig] = None,\n",
    "        transform: Optional[callable] = None\n",
    "    ):\n",
    "        \"\"\"Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_tree: Input data tree\n",
    "            config: Optional dataset configuration\n",
    "            transform: Optional transform function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config or DatasetConfig()\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Initialize containers\n",
    "        self.data: List[Tuple[np.ndarray, str]] = []\n",
    "        self.metadata: List[Dict[str, Any]] = []\n",
    "        self.cache: Dict[int, torch.Tensor] = {}\n",
    "        self.class_mapping: Dict[str, int] = {}\n",
    "        \n",
    "        # Process data tree\n",
    "        self._process_tree(data_tree)\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Initialized dataset with {len(self.data)} samples \"\n",
    "            f\"and {self.num_classes} classes\"\n",
    "        )\n",
    "        \n",
    "    def _process_tree(\n",
    "        self,\n",
    "        data_tree: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Process data tree to build dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_tree: Input data tree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Determine classes\n",
    "            self.num_classes = self._determine_classes(data_tree)\n",
    "            if self.num_classes <= 0:\n",
    "                raise ValueError(\"No classes found in dataset\")\n",
    "                \n",
    "            # Build dataset\n",
    "            for task_node in data_tree.children.values():\n",
    "                self._process_task_node(task_node)\n",
    "                \n",
    "            # Calculate statistics\n",
    "            self._calculate_statistics()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing data tree: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _determine_classes(\n",
    "        self,\n",
    "        data_tree: Any\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of classes.\n",
    "        \n",
    "        Args:\n",
    "            data_tree: Input data tree\n",
    "            \n",
    "        Returns:\n",
    "            Number of classes\n",
    "        \"\"\"\n",
    "        classes = set()\n",
    "        \n",
    "        for task_node in data_tree.children.values():\n",
    "            for mode_node in task_node.children.values():\n",
    "                for grid_node in mode_node.children.values():\n",
    "                    for grid in grid_node.children.values():\n",
    "                        if hasattr(grid, 'grid') and grid.grid is not None:\n",
    "                            classes.update(np.unique(grid.grid))\n",
    "                            \n",
    "        # Create class mapping\n",
    "        self.class_mapping = {\n",
    "            str(cls): idx\n",
    "            for idx, cls in enumerate(sorted(classes))\n",
    "        }\n",
    "        \n",
    "        return len(classes)\n",
    "        \n",
    "    def _process_task_node(\n",
    "        self,\n",
    "        task_node: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Process task node.\n",
    "        \n",
    "        Args:\n",
    "            task_node: Task node to process\n",
    "        \"\"\"\n",
    "        for mode_node in task_node.children.values():\n",
    "            for grid_node in mode_node.children.values():\n",
    "                self._process_grid_node(\n",
    "                    grid_node,\n",
    "                    task_node.name\n",
    "                )\n",
    "                \n",
    "    def _process_grid_node(\n",
    "        self,\n",
    "        grid_node: Any,\n",
    "        task_id: str\n",
    "    ) -> None:\n",
    "        \"\"\"Process grid node.\n",
    "        \n",
    "        Args:\n",
    "            grid_node: Grid node to process\n",
    "            task_id: Task identifier\n",
    "        \"\"\"\n",
    "        for grid in grid_node.children.values():\n",
    "            if hasattr(grid, 'grid') and grid.grid is not None:\n",
    "                if self._validate_grid(grid.grid):\n",
    "                    self._add_grid(grid.grid, task_id, grid)\n",
    "                    \n",
    "    def _validate_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> bool:\n",
    "        \"\"\"Validate grid dimensions.\n",
    "        \n",
    "        Args:\n",
    "            grid: Grid to validate\n",
    "            \n",
    "        Returns:\n",
    "            Whether grid is valid\n",
    "        \"\"\"\n",
    "        if not isinstance(grid, np.ndarray):\n",
    "            return False\n",
    "            \n",
    "        if grid.size == 0:\n",
    "            return False\n",
    "            \n",
    "        if self.config.min_grid_size:\n",
    "            if any(s < m for s, m in zip(grid.shape, self.config.min_grid_size)):\n",
    "                return False\n",
    "                \n",
    "        if self.config.max_grid_size:\n",
    "            if any(s > m for s, m in zip(grid.shape, self.config.max_grid_size)):\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "        \n",
    "    def _add_grid(\n",
    "        self,\n",
    "        grid: np.ndarray,\n",
    "        task_id: str,\n",
    "        node: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Add grid to dataset.\n",
    "        \n",
    "        Args:\n",
    "            grid: Grid data\n",
    "            task_id: Task identifier\n",
    "            node: Grid node\n",
    "        \"\"\"\n",
    "        # Process grid\n",
    "        if self.config.normalize_grids:\n",
    "            grid = self._normalize_grid(grid)\n",
    "            \n",
    "        # Store data\n",
    "        self.data.append((grid, task_id))\n",
    "        \n",
    "        # Store metadata if enabled\n",
    "        if self.config.include_metadata:\n",
    "            self.metadata.append({\n",
    "                'task_id': task_id,\n",
    "                'shape': grid.shape,\n",
    "                'unique_values': len(np.unique(grid))\n",
    "            })\n",
    "            \n",
    "    def _normalize_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Normalize grid values.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Normalized grid\n",
    "        \"\"\"\n",
    "        if grid.size == 0:\n",
    "            return grid\n",
    "            \n",
    "        grid_min = np.min(grid)\n",
    "        grid_max = np.max(grid)\n",
    "        \n",
    "        if grid_min == grid_max:\n",
    "            return np.zeros_like(grid, dtype=np.float32)\n",
    "            \n",
    "        return ((grid - grid_min) / (grid_max - grid_min)).astype(np.float32)\n",
    "        \n",
    "    def _calculate_statistics(self) -> None:\n",
    "        \"\"\"Calculate dataset statistics.\"\"\"\n",
    "        self.statistics = {\n",
    "            'total_samples': len(self.data),\n",
    "            'grid_shapes': defaultdict(int),\n",
    "            'class_distribution': defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for grid, task_id in self.data:\n",
    "            self.statistics['grid_shapes'][grid.shape] += 1\n",
    "            self.statistics['class_distribution'][task_id] += 1\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get dataset length.\"\"\"\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Get dataset item.\n",
    "        \n",
    "        Args:\n",
    "            index: Item index\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (grid tensor, task ID)\n",
    "        \"\"\"\n",
    "        if self.config.cache_enabled and index in self.cache:\n",
    "            return self.cache[index]\n",
    "            \n",
    "        grid, task_id = self.data[index]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        grid_tensor = torch.tensor(\n",
    "            grid,\n",
    "            dtype=torch.float32\n",
    "        ).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        # Apply transform if provided\n",
    "        if self.transform is not None:\n",
    "            grid_tensor = self.transform(grid_tensor)\n",
    "            \n",
    "        # Cache if enabled\n",
    "        if self.config.cache_enabled:\n",
    "            self.cache[index] = (grid_tensor, task_id)\n",
    "            \n",
    "        return grid_tensor, task_id\n",
    "        \n",
    "    def get_grid_shapes(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Get list of unique grid shapes.\n",
    "        \n",
    "        Returns:\n",
    "            List of grid shapes\n",
    "        \"\"\"\n",
    "        return list(self.statistics['grid_shapes'].keys())\n",
    "        \n",
    "    def get_class_distribution(self) -> Dict[str, int]:\n",
    "        \"\"\"Get class distribution.\n",
    "        \n",
    "        Returns:\n",
    "            Class distribution dictionary\n",
    "        \"\"\"\n",
    "        return dict(self.statistics['class_distribution'])\n",
    "        \n",
    "    def get_metadata(\n",
    "        self,\n",
    "        index: int\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get metadata for index.\n",
    "        \n",
    "        Args:\n",
    "            index: Item index\n",
    "            \n",
    "        Returns:\n",
    "            Metadata dictionary or None\n",
    "        \"\"\"\n",
    "        if 0 <= index < len(self.metadata):\n",
    "            return self.metadata[index]\n",
    "        return None\n",
    "        \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get dataset statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        return self.statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6cba9d1-fc9f-48a2-b901-1c4616b69f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any, Optional, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from enum import Enum\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "class AugmentationType(Enum):\n",
    "    \"\"\"Types of grid augmentation.\"\"\"\n",
    "    NOISE = \"noise\"\n",
    "    ROTATION = \"rotation\"\n",
    "    FLIP = \"flip\"\n",
    "    SHIFT = \"shift\"\n",
    "    MASK = \"mask\"\n",
    "    PERMUTE = \"permute\"\n",
    "\n",
    "@dataclass\n",
    "class AugmentationConfig:\n",
    "    \"\"\"Configuration for data augmentation.\n",
    "    \n",
    "    Attributes:\n",
    "        augmentation_factor: Number of augmentations per sample\n",
    "        enabled_augmentations: List of enabled augmentation types\n",
    "        noise_range: Range for noise values\n",
    "        rotation_angles: List of rotation angles\n",
    "        shift_range: Range for shifting\n",
    "        mask_probability: Probability of masking\n",
    "        preserve_class_balance: Whether to preserve class balance\n",
    "    \"\"\"\n",
    "    augmentation_factor: int = 2\n",
    "    enabled_augmentations: List[AugmentationType] = None\n",
    "    noise_range: Tuple[int, int] = (-1, 1)\n",
    "    rotation_angles: List[int] = None\n",
    "    shift_range: Tuple[int, int] = (-1, 1)\n",
    "    mask_probability: float = 0.1\n",
    "    preserve_class_balance: bool = True\n",
    "\n",
    "class AugmentedDynamicGridDataset(DynamicGridDataset):\n",
    "    \"\"\"Enhanced dataset with advanced augmentation capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_tree: Any,\n",
    "        num_classes: int,\n",
    "        config: Optional[AugmentationConfig] = None,\n",
    "        transform: Optional[callable] = None\n",
    "    ):\n",
    "        \"\"\"Initialize augmented dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_tree: Input data tree\n",
    "            num_classes: Number of classes\n",
    "            config: Optional augmentation configuration\n",
    "            transform: Optional transform function\n",
    "        \"\"\"\n",
    "        super().__init__(data_tree, transform=transform)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.config = config or AugmentationConfig()\n",
    "        \n",
    "        # Set default augmentations if none specified\n",
    "        if not self.config.enabled_augmentations:\n",
    "            self.config.enabled_augmentations = list(AugmentationType)\n",
    "            \n",
    "        if not self.config.rotation_angles:\n",
    "            self.config.rotation_angles = [0, 90, 180, 270]\n",
    "            \n",
    "        # Augment data\n",
    "        self.data = self._augment_data()\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Created augmented dataset with \"\n",
    "            f\"{len(self.data)} samples\"\n",
    "        )\n",
    "        \n",
    "    def _augment_data(self) -> List[Tuple[np.ndarray, str]]:\n",
    "        \"\"\"Augment dataset.\n",
    "        \n",
    "        Returns:\n",
    "            List of augmented samples\n",
    "        \"\"\"\n",
    "        augmented_data = []\n",
    "        class_counts = defaultdict(int)\n",
    "        \n",
    "        for grid, task_id in self.data:\n",
    "            # Original sample\n",
    "            augmented_data.append((grid, task_id))\n",
    "            class_counts[task_id] += 1\n",
    "            \n",
    "            # Generate augmentations\n",
    "            for _ in range(self.config.augmentation_factor):\n",
    "                try:\n",
    "                    augmented_grid = self._apply_augmentations(grid)\n",
    "                    \n",
    "                    if self.config.preserve_class_balance:\n",
    "                        if class_counts[task_id] >= max(class_counts.values()):\n",
    "                            continue\n",
    "                            \n",
    "                    augmented_data.append((augmented_grid, task_id))\n",
    "                    class_counts[task_id] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(\n",
    "                        f\"Failed to augment grid for task {task_id}: {e}\"\n",
    "                    )\n",
    "                    continue\n",
    "                    \n",
    "        return augmented_data\n",
    "        \n",
    "    def _apply_augmentations(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Apply multiple augmentations.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Augmented grid\n",
    "        \"\"\"\n",
    "        augmented = grid.copy()\n",
    "        \n",
    "        # Randomly select and apply augmentations\n",
    "        for aug_type in random.sample(\n",
    "            self.config.enabled_augmentations,\n",
    "            k=random.randint(1, len(self.config.enabled_augmentations))\n",
    "        ):\n",
    "            augmented = self._apply_single_augmentation(\n",
    "                augmented,\n",
    "                aug_type\n",
    "            )\n",
    "            \n",
    "        return augmented\n",
    "        \n",
    "    def _apply_single_augmentation(\n",
    "        self,\n",
    "        grid: np.ndarray,\n",
    "        aug_type: AugmentationType\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Apply single augmentation.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            aug_type: Augmentation type\n",
    "            \n",
    "        Returns:\n",
    "            Augmented grid\n",
    "        \"\"\"\n",
    "        if aug_type == AugmentationType.NOISE:\n",
    "            return self._add_noise(grid)\n",
    "        elif aug_type == AugmentationType.ROTATION:\n",
    "            return self._rotate_grid(grid)\n",
    "        elif aug_type == AugmentationType.FLIP:\n",
    "            return self._flip_grid(grid)\n",
    "        elif aug_type == AugmentationType.SHIFT:\n",
    "            return self._shift_grid(grid)\n",
    "        elif aug_type == AugmentationType.MASK:\n",
    "            return self._mask_grid(grid)\n",
    "        elif aug_type == AugmentationType.PERMUTE:\n",
    "            return self._permute_values(grid)\n",
    "        else:\n",
    "            return grid\n",
    "            \n",
    "    def _add_noise(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Add random noise to grid.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Noisy grid\n",
    "        \"\"\"\n",
    "        noise = np.random.randint(\n",
    "            self.config.noise_range[0],\n",
    "            self.config.noise_range[1] + 1,\n",
    "            size=grid.shape\n",
    "        )\n",
    "        noisy = grid + noise\n",
    "        return np.mod(noisy, self.num_classes)\n",
    "        \n",
    "    def _rotate_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Rotate grid.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Rotated grid\n",
    "        \"\"\"\n",
    "        k = np.random.choice(self.config.rotation_angles) // 90\n",
    "        return np.rot90(grid, k=k)\n",
    "        \n",
    "    def _flip_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Flip grid.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Flipped grid\n",
    "        \"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            return np.fliplr(grid)\n",
    "        return np.flipud(grid)\n",
    "        \n",
    "    def _shift_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Shift grid values.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Shifted grid\n",
    "        \"\"\"\n",
    "        shift = np.random.randint(\n",
    "            self.config.shift_range[0],\n",
    "            self.config.shift_range[1] + 1\n",
    "        )\n",
    "        shifted = np.roll(grid, shift, axis=random.randint(0, 1))\n",
    "        return shifted\n",
    "        \n",
    "    def _mask_grid(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Randomly mask grid values.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Masked grid\n",
    "        \"\"\"\n",
    "        mask = np.random.random(grid.shape) < self.config.mask_probability\n",
    "        masked = grid.copy()\n",
    "        masked[mask] = random.randint(0, self.num_classes - 1)\n",
    "        return masked\n",
    "        \n",
    "    def _permute_values(\n",
    "        self,\n",
    "        grid: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Permute grid values.\n",
    "        \n",
    "        Args:\n",
    "            grid: Input grid\n",
    "            \n",
    "        Returns:\n",
    "            Permuted grid\n",
    "        \"\"\"\n",
    "        unique_values = np.unique(grid)\n",
    "        permutation = np.random.permutation(unique_values)\n",
    "        value_map = dict(zip(unique_values, permutation))\n",
    "        return np.vectorize(value_map.get)(grid)\n",
    "        \n",
    "    def get_augmentation_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get augmentation statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'total_samples': len(self.data),\n",
    "            'augmentation_factor': self.config.augmentation_factor,\n",
    "            'enabled_augmentations': [\n",
    "                aug.value for aug in self.config.enabled_augmentations\n",
    "            ],\n",
    "            'class_distribution': defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for _, task_id in self.data:\n",
    "            stats['class_distribution'][task_id] += 1\n",
    "            \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f86efd0-dd99-48af-b6de-b62914739d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data-dir DATA_DIR] [--model-dir MODEL_DIR]\n",
      "                             [--batch-size BATCH_SIZE] [--debug]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Owner\\AppData\\Roaming\\jupyter\\runtime\\kernel-e4b0464c-e810-4653-99eb-be10565d3cd6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from logger_setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    \"\"\"Application configuration.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir: Directory containing data files\n",
    "        model_dir: Directory for model checkpoints\n",
    "        log_dir: Directory for logs\n",
    "        batch_size: Training batch size\n",
    "        num_workers: Number of data loading workers\n",
    "        use_gpu: Whether to use GPU\n",
    "        debug_mode: Whether to enable debug mode\n",
    "    \"\"\"\n",
    "    data_dir: str = \"data\"\n",
    "    model_dir: str = \"models\"\n",
    "    log_dir: str = \"logs\"\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "    use_gpu: bool = True\n",
    "    debug_mode: bool = False\n",
    "\n",
    "class ARCTrainingApp:\n",
    "    \"\"\"Main application for ARC training and visualization.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: AppConfig\n",
    "    ):\n",
    "        \"\"\"Initialize application.\n",
    "        \n",
    "        Args:\n",
    "            config: Application configuration\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.setup_directories()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.device = None\n",
    "        self.data_tree = None\n",
    "        self.gui = None\n",
    "        \n",
    "        logger.info(\"Initialized ARC Training Application\")\n",
    "        \n",
    "    def setup_directories(self) -> None:\n",
    "        \"\"\"Create necessary directories.\"\"\"\n",
    "        for directory in [\n",
    "            self.config.data_dir,\n",
    "            self.config.model_dir,\n",
    "            self.config.log_dir\n",
    "        ]:\n",
    "            Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "    def setup_multiprocessing(self) -> None:\n",
    "        \"\"\"Configure multiprocessing.\"\"\"\n",
    "        if os.name == 'nt':  # Windows\n",
    "            mp.set_start_method('spawn', force=True)\n",
    "        else:\n",
    "            mp.set_start_method('fork', force=True)\n",
    "            \n",
    "    def setup_device(self) -> None:\n",
    "        \"\"\"Configure computation device.\"\"\"\n",
    "        if self.config.use_gpu and torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            # Enable cuDNN autotuner\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and process ARC data.\"\"\"\n",
    "        try:\n",
    "            # Load raw data\n",
    "            self.arc_data = load_arc_data()\n",
    "            \n",
    "            # Analyze dimensions\n",
    "            self.task_metadata = analyze_grid_dimensions(\n",
    "                \"arc-agi_training_challenges.json\"\n",
    "            )\n",
    "            \n",
    "            # Build data tree\n",
    "            self.data_tree = build_data_tree(self.task_metadata)\n",
    "            \n",
    "            logger.info(\"Data loading completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(\"Data loading failed\")\n",
    "            raise RuntimeError(f\"Failed to load data: {e}\")\n",
    "            \n",
    "    def setup_gui(self) -> None:\n",
    "        \"\"\"Initialize GUI components.\"\"\"\n",
    "        try:\n",
    "            self.root = tk.Tk()\n",
    "            self.root.title(\"ARC Training and Visualization GUI\")\n",
    "            \n",
    "            # Configure window\n",
    "            self.root.geometry(\"1200x800\")\n",
    "            self.root.protocol(\n",
    "                \"WM_DELETE_WINDOW\",\n",
    "                self.on_closing\n",
    "            )\n",
    "            \n",
    "            # Create GUI instance\n",
    "            self.gui = TrainingGUI(\n",
    "                root=self.root,\n",
    "                model=self.model,\n",
    "                train_loader=self.train_loader,\n",
    "                val_loader=self.val_loader,\n",
    "                device=self.device,\n",
    "                data_tree=self.data_tree\n",
    "            )\n",
    "            \n",
    "            logger.info(\"GUI setup completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(\"GUI setup failed\")\n",
    "            raise RuntimeError(f\"Failed to setup GUI: {e}\")\n",
    "            \n",
    "    def setup_model(self) -> None:\n",
    "        \"\"\"Initialize model and training components.\"\"\"\n",
    "        try:\n",
    "            # Create dataset\n",
    "            self.dataset = AugmentedDynamicGridDataset(\n",
    "                self.data_tree,\n",
    "                config=DatasetConfig(\n",
    "                    augmentation_enabled=True,\n",
    "                    cache_enabled=True\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Create data loaders\n",
    "            self.train_loader, self.val_loader = self.create_data_loaders()\n",
    "            \n",
    "            # Initialize model\n",
    "            self.model = SimpleTransformer(\n",
    "                config=TransformerConfig(\n",
    "                    vocab_size=self.dataset.num_classes,\n",
    "                    d_model=512\n",
    "                )\n",
    "            ).to(self.device)\n",
    "            \n",
    "            logger.info(\"Model setup completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(\"Model setup failed\")\n",
    "            raise RuntimeError(f\"Failed to setup model: {e}\")\n",
    "            \n",
    "    def create_data_loaders(self) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Create train and validation data loaders.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_loader, val_loader)\n",
    "        \"\"\"\n",
    "        # Split dataset\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        val_size = len(self.dataset) - train_size\n",
    "        \n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            self.dataset,\n",
    "            [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # Create loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "        \n",
    "    def save_session(self) -> None:\n",
    "        \"\"\"Save current session state.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        session_dir = Path(self.config.model_dir) / f\"session_{timestamp}\"\n",
    "        session_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(\n",
    "            self.model.state_dict(),\n",
    "            session_dir / \"model.pth\"\n",
    "        )\n",
    "        \n",
    "        # Save configuration\n",
    "        with open(session_dir / \"config.json\", 'w') as f:\n",
    "            json.dump(vars(self.config), f, indent=2)\n",
    "            \n",
    "        logger.info(f\"Session saved to {session_dir}\")\n",
    "        \n",
    "    def on_closing(self) -> None:\n",
    "        \"\"\"Handle application closing.\"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to save before quitting?\"):\n",
    "            self.save_session()\n",
    "        self.root.destroy()\n",
    "        \n",
    "    def run(self) -> None:\n",
    "        \"\"\"Run the application.\"\"\"\n",
    "        try:\n",
    "            # Setup components\n",
    "            self.setup_multiprocessing()\n",
    "            self.setup_device()\n",
    "            self.load_data()\n",
    "            self.setup_model()\n",
    "            self.setup_gui()\n",
    "            \n",
    "            # Start GUI\n",
    "            self.root.mainloop()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(\"Application failed\")\n",
    "            messagebox.showerror(\n",
    "                \"Error\",\n",
    "                f\"Application failed to start: {e}\"\n",
    "            )\n",
    "            sys.exit(1)\n",
    "            \n",
    "def parse_args() -> argparse.Namespace:\n",
    "    \"\"\"Parse command line arguments.\n",
    "    \n",
    "    Returns:\n",
    "        Parsed arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"ARC Training Application\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        default=\"data\",\n",
    "        help=\"Data directory\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--model-dir\",\n",
    "        default=\"models\",\n",
    "        help=\"Model directory\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Training batch size\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--debug\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable debug mode\"\n",
    "    )\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    # Parse arguments\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Create configuration\n",
    "    config = AppConfig(\n",
    "        data_dir=args.data_dir,\n",
    "        model_dir=args.model_dir,\n",
    "        batch_size=args.batch_size,\n",
    "        debug_mode=args.debug\n",
    "    )\n",
    "    \n",
    "    # Create and run application\n",
    "    app = ARCTrainingApp(config)\n",
    "    app.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3e2e4-5885-4d5a-a1ce-9de5e449a5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
