{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912a8f4-098c-4e8f-94d2-a79f48b486e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configure logging for console output\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "# Function to check and install missing packages\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Successfully installed {package_name}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package_name}\")\n",
    "\n",
    "# Function to upgrade pip\n",
    "def upgrade_pip():\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "        print(\"Pip upgraded successfully.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Failed to upgrade pip.\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'torch', \n",
    "    'torchvision', \n",
    "    'matplotlib', \n",
    "    'numpy', \n",
    "    'scikit-learn', \n",
    "    'pandas'\n",
    "]\n",
    "\n",
    "# Upgrade pip first\n",
    "upgrade_pip()\n",
    "\n",
    "# Import required packages with error handling for missing dependencies\n",
    "for package_name in required_packages:\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"Successfully imported {package_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"Package {package_name} not found. Attempting to install...\")\n",
    "        install_package(package_name)\n",
    "\n",
    "# Define DatabaseManager with better connection handling\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_path='training_pipeline.db'):\n",
    "        self.db_path = db_path\n",
    "        logger.info(f\"Connecting to database at {self.db_path}\")\n",
    "        self.conn = sqlite3.connect(db_path, timeout=5.0, check_same_thread=False)\n",
    "        self.initialize_database()\n",
    "\n",
    "    def initialize_database(self):\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS model_config (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    input_dim INTEGER,\n",
    "                    hidden_dim INTEGER,\n",
    "                    output_dim INTEGER,\n",
    "                    dropout_prob REAL,\n",
    "                    created_at TEXT NOT NULL\n",
    "                )\n",
    "            ''')\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS tensor_activations (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    epoch INTEGER,\n",
    "                    layer TEXT,\n",
    "                    activations TEXT,\n",
    "                    created_at TEXT NOT NULL\n",
    "                )\n",
    "            ''')\n",
    "            self.conn.commit()\n",
    "            logger.info(\"Database schema initialized.\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error initializing database: {e}\")\n",
    "            print(f\"Error initializing database: {e}\")\n",
    "\n",
    "    def store_model_config(self, input_dim, hidden_dim, output_dim, dropout_prob):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO model_config (input_dim, hidden_dim, output_dim, dropout_prob, created_at) \n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (input_dim, hidden_dim, output_dim, dropout_prob, datetime.now().isoformat()))\n",
    "            logger.info(\"Model configuration stored.\")\n",
    "        except sqlite3.IntegrityError as e:\n",
    "            logger.error(f\"IntegrityError: {e}\")\n",
    "            print(f\"IntegrityError: {e}\")\n",
    "\n",
    "    def store_tensor_activations(self, epoch, layer, activations):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO tensor_activations (epoch, layer, activations, created_at) \n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                ''', (epoch, layer, json.dumps(activations), datetime.now().isoformat()))\n",
    "            logger.info(f\"Stored activations for epoch {epoch} and layer {layer}.\")\n",
    "        except sqlite3.OperationalError as e:\n",
    "            logger.error(f\"Error storing tensor activations: {e}\")\n",
    "            print(f\"Error storing tensor activations: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self.conn.close()\n",
    "            logger.info(f\"Closed database connection to {self.db_path}.\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error closing database connection: {e}\")\n",
    "            print(f\"Error closing database connection: {e}\")\n",
    "\n",
    "# Generate a synthetic dataset for binary classification\n",
    "def generate_synthetic_data(samples=1000, features=20, classes=2):\n",
    "    try:\n",
    "        print(f\"Generating synthetic dataset with {samples} samples, {features} features, and {classes} classes...\")\n",
    "        X, y = make_classification(n_samples=samples, n_features=features, n_classes=classes, random_state=42)\n",
    "        print(\"Synthetic dataset generated successfully.\")\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synthetic dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Split and scale the dataset\n",
    "def preprocess_data(X, y, test_size=0.2):\n",
    "    try:\n",
    "        print(f\"Splitting dataset into training and testing sets with test size = {test_size}...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        print(\"Fitting the scaler to the training data...\")\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        print(\"Transforming the test data...\")\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"Data preprocessing completed successfully.\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "def convert_to_tensors(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        print(\"Converting training and testing data to PyTorch tensors...\")\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Debug statements to display tensor shapes\n",
    "        print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "        print(f\"X_test_tensor shape: {X_test_tensor.shape}\")\n",
    "        print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "        print(f\"y_test_tensor shape: {y_test_tensor.shape}\")\n",
    "        \n",
    "        print(\"Conversion to PyTorch tensors completed successfully.\")\n",
    "        \n",
    "        return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting data to tensors: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to create DataLoader objects\n",
    "def create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, batch_size=32):\n",
    "    try:\n",
    "        print(\"Creating DataLoader objects...\")\n",
    "        \n",
    "        # Create TensorDataset objects\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        # Create DataLoader objects\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Debug statements to display the number of batches\n",
    "        print(f\"Number of batches in training DataLoader: {len(train_loader)}\")\n",
    "        print(f\"Number of batches in testing DataLoader: {len(test_loader)}\")\n",
    "        \n",
    "        print(\"DataLoader objects created successfully.\")\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataLoader objects: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to create DataLoader objects\n",
    "X, y = generate_synthetic_data(samples=1000, features=20, classes=2)\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y, test_size=0.2)\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_tensors(X_train, X_test, y_train, y_test)\n",
    "train_loader, test_loader = create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Initialize or load the database\n",
    "def initialize_or_load_db(db_path='training_pipeline.db'):\n",
    "    try:\n",
    "        print(f\"Attempting to connect to the database at {db_path}...\")\n",
    "        db_manager = DatabaseManager(db_path=db_path)\n",
    "        print(\"Database connection established.\")\n",
    "        return db_manager\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing or loading the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to initialize or load the database\n",
    "db_manager = initialize_or_load_db()\n",
    "\n",
    "# Model Definition\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.3, db_manager=None):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Store model configuration in the database\n",
    "        if db_manager:\n",
    "            db_manager.store_model_config(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "\n",
    "        # Visualize initial weights\n",
    "        self.visualize_weights()\n",
    "\n",
    "    def visualize_weights(self):\n",
    "        \"\"\"Visualizes the weights of each layer.\"\"\"\n",
    "        print(\"Visualizing initial weights...\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        layers = [self.layer1, self.layer2, self.layer3, self.layer4, self.output_layer]\n",
    "        for i, layer in enumerate(layers):\n",
    "            weights = layer.weight.data.cpu().numpy()\n",
    "            plt.subplot(3, 2, i + 1)\n",
    "            plt.hist(weights.flatten(), bins=30, alpha=0.7, color='blue')\n",
    "            plt.title(f'Layer {i + 1} Weights Distribution')\n",
    "            plt.xlabel('Weight Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Initial weights visualized for all layers.\")\n",
    "\n",
    "    def forward(self, x, epoch=None, db_manager=None):\n",
    "        print(f\"Forward pass input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After Layer 1: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After Layer 2: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After Layer 3: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        print(f\"After Layer 4: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        print(f\"After Output Layer: {x.shape}\")\n",
    "\n",
    "        # Store activations in the database\n",
    "        if db_manager and epoch is not None:\n",
    "            db_manager.store_tensor_activations(epoch, \"Output Layer\", x.detach().cpu().numpy().tolist())\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model instantiation and training parameters\n",
    "input_dim = X_train.shape[1]  # Set input dimension based on the training data\n",
    "hidden_dim = 128  # Define hidden layer dimension\n",
    "output_dim = 2  # Set output dimension for binary classification\n",
    "dropout_prob = 0.3  # Define dropout probability\n",
    "\n",
    "# Initialize the DNN model\n",
    "print(\"Initializing the DNN model...\")\n",
    "model = DNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_prob=dropout_prob, db_manager=db_manager)\n",
    "\n",
    "# Set the optimizer\n",
    "print(\"Setting up the optimizer...\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the loss function\n",
    "print(\"Setting up the loss function (CrossEntropyLoss)...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to plot the training results\n",
    "def plot_training_results(train_losses, train_accuracies, epochs):\n",
    "    \"\"\"\n",
    "    Plots training loss and accuracy over epochs.\n",
    "    \n",
    "    Args:\n",
    "    - train_losses (list): List of training losses.\n",
    "    - train_accuracies (list): List of training accuracies.\n",
    "    - epochs (int): Number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Plotting training results...\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy', color='green')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Training results plotted successfully.\")\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10, device='cpu', db_manager=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, epoch=epoch, db_manager=db_manager)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    plot_training_results(train_losses, train_accuracies, epochs)  # Plot results after training\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, optimizer, criterion, epochs=10, db_manager=db_manager)\n",
    "\n",
    "# Close the database connection\n",
    "db_manager.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785296a-fcb3-4e88-92a0-77f6d6490457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging for console output\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_path='training_pipeline.db'):\n",
    "        self.db_path = db_path\n",
    "        logger.info(f\"Connecting to database at {self.db_path}\")\n",
    "        self.conn = sqlite3.connect(db_path, timeout=5.0, check_same_thread=False)\n",
    "        self.initialize_database()\n",
    "\n",
    "    def initialize_database(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS arc_data (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                file_name TEXT NOT NULL,\n",
    "                content TEXT,\n",
    "                created_at TEXT NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS model_config (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                input_dim INTEGER,\n",
    "                hidden_dim INTEGER,\n",
    "                output_dim INTEGER,\n",
    "                dropout_prob REAL,\n",
    "                created_at TEXT NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS tensor_activations (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                epoch INTEGER,\n",
    "                layer TEXT,\n",
    "                activations TEXT,\n",
    "                created_at TEXT NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "        logger.info(\"Database schema initialized.\")\n",
    "\n",
    "    def store_arc_data(self, file_name, content):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO arc_data (file_name, content, created_at) \n",
    "                    VALUES (?, ?, ?)\n",
    "                ''', (file_name, content, datetime.now().isoformat()))\n",
    "            logger.info(f\"Stored data from {file_name}.\")\n",
    "        except sqlite3.IntegrityError as e:\n",
    "            logger.error(f\"IntegrityError: {e}\")\n",
    "\n",
    "    def store_model_config(self, input_dim, hidden_dim, output_dim, dropout_prob):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO model_config (input_dim, hidden_dim, output_dim, dropout_prob, created_at) \n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (input_dim, hidden_dim, output_dim, dropout_prob, datetime.now().isoformat()))\n",
    "            logger.info(\"Model configuration stored.\")\n",
    "        except sqlite3.IntegrityError as e:\n",
    "            logger.error(f\"IntegrityError: {e}\")\n",
    "\n",
    "    def store_tensor_activations(self, epoch, layer, activations):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO tensor_activations (epoch, layer, activations, created_at) \n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                ''', (epoch, layer, json.dumps(activations), datetime.now().isoformat()))\n",
    "            logger.info(f\"Stored activations for epoch {epoch} and layer {layer}.\")\n",
    "        except sqlite3.OperationalError as e:\n",
    "            logger.error(f\"Error storing tensor activations: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self.conn.close()\n",
    "            logger.info(f\"Closed database connection to {self.db_path}.\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error closing database connection: {e}\")\n",
    "\n",
    "def search_and_document_arc_data(folder_path):\n",
    "    db_manager = DatabaseManager()\n",
    "\n",
    "    try:\n",
    "        print(f\"Searching for JSON files in {folder_path}...\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    db_manager.store_arc_data(filename, content)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during searching or processing: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        db_manager.close()\n",
    "\n",
    "# Generate a synthetic dataset for binary classification\n",
    "def generate_synthetic_data(samples=1000, features=20, classes=2):\n",
    "    try:\n",
    "        print(f\"Generating synthetic dataset with {samples} samples, {features} features, and {classes} classes...\")\n",
    "        X, y = make_classification(n_samples=samples, n_features=features, n_classes=classes, random_state=42)\n",
    "        print(\"Synthetic dataset generated successfully.\")\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synthetic dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Split and scale the dataset\n",
    "def preprocess_data(X, y, test_size=0.2):\n",
    "    try:\n",
    "        print(f\"Splitting dataset into training and testing sets with test size = {test_size}...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        print(\"Fitting the scaler to the training data...\")\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        print(\"Transforming the test data...\")\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"Data preprocessing completed successfully.\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "def convert_to_tensors(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        print(\"Converting training and testing data to PyTorch tensors...\")\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "        print(f\"X_test_tensor shape: {X_test_tensor.shape}\")\n",
    "        print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "        print(f\"y_test_tensor shape: {y_test_tensor.shape}\")\n",
    "        \n",
    "        print(\"Conversion to PyTorch tensors completed successfully.\")\n",
    "        \n",
    "        return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting data to tensors: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to create DataLoader objects\n",
    "def create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, batch_size=32):\n",
    "    try:\n",
    "        print(\"Creating DataLoader objects...\")\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(f\"Number of batches in training DataLoader: {len(train_loader)}\")\n",
    "        print(f\"Number of batches in testing DataLoader: {len(test_loader)}\")\n",
    "        \n",
    "        print(\"DataLoader objects created successfully.\")\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataLoader objects: {e}\")\n",
    "        raise\n",
    "\n",
    "# Model Definition\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.3, db_manager=None):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        if db_manager:\n",
    "            db_manager.store_model_config(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "\n",
    "        self.visualize_weights()\n",
    "\n",
    "    def visualize_weights(self):\n",
    "        print(\"Visualizing initial weights...\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        layers = [self.layer1, self.layer2, self.layer3, self.layer4, self.output_layer]\n",
    "        for i, layer in enumerate(layers):\n",
    "            weights = layer.weight.data.cpu().numpy()\n",
    "            plt.subplot(3, 2, i + 1)\n",
    "            plt.hist(weights.flatten(), bins=30, alpha=0.7, color='blue')\n",
    "            plt.title(f'Layer {i + 1} Weights Distribution')\n",
    "            plt.xlabel('Weight Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Initial weights visualized for all layers.\")\n",
    "\n",
    "    def forward(self, x, epoch=None, db_manager=None):\n",
    "        print(f\"Forward pass input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After Layer 1: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After Layer 2: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After Layer 3: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        print(f\"After Layer 4: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        print(f\"After Output Layer: {x.shape}\")\n",
    "\n",
    "        if db_manager and epoch is not None:\n",
    "            db_manager.store_tensor_activations(epoch, \"Output Layer\", x.detach().cpu().numpy().tolist())\n",
    "\n",
    "        return x\n",
    "\n",
    "# Function to plot training results\n",
    "def plot_training_results(train_losses, train_accuracies, epochs):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy', color='green')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Training results plotted successfully.\")\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10, device='cpu', db_manager=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "        logger.info(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, epoch=epoch, db_manager=db_manager)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        logger.info(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    plot_training_results(train_losses, train_accuracies, epochs)\n",
    "\n",
    "# Main execution\n",
    "folder_path = r\"C:\\Users\\Owner\\Downloads\\arc-prize-2024\"\n",
    "search_and_document_arc_data(folder_path)\n",
    "\n",
    "# Generate synthetic dataset for binary classification\n",
    "X, y = generate_synthetic_data(samples=1000, features=20, classes=2)\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y, test_size=0.2)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_tensors(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader, test_loader = create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Initialize or load the database\n",
    "db_manager = initialize_or_load_db()\n",
    "\n",
    "# Model instantiation and training parameters\n",
    "input_dim = X_train.shape[1]  # Set input dimension based on the training data\n",
    "hidden_dim = 128  # Define hidden layer dimension\n",
    "output_dim = 2  # Set output dimension for binary classification\n",
    "dropout_prob = 0.3  # Define dropout probability\n",
    "\n",
    "# Initialize the DNN model\n",
    "print(\"Initializing the DNN model...\")\n",
    "model = DNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_prob=dropout_prob, db_manager=db_manager)\n",
    "\n",
    "# Set the optimizer\n",
    "print(f\"Setting up the optimizer with learning rate = 0.001...\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Optimizer set up successfully.\")\n",
    "\n",
    "# Set the loss function\n",
    "print(\"Setting up the loss function (CrossEntropyLoss)...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Loss function set up successfully.\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, optimizer, criterion, epochs=10, db_manager=db_manager)\n",
    "\n",
    "# Close the database connection\n",
    "db_manager.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f29e4c-ec2f-4594-85c3-1e5a1b29ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configure logging for console output\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "# Function to check and install missing packages\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Successfully installed {package_name}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package_name}\")\n",
    "\n",
    "# Function to upgrade pip\n",
    "def upgrade_pip():\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "        print(\"Pip upgraded successfully.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Failed to upgrade pip.\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'torch', \n",
    "    'torchvision', \n",
    "    'matplotlib', \n",
    "    'numpy', \n",
    "    'scikit-learn', \n",
    "    'pandas'\n",
    "]\n",
    "\n",
    "# Upgrade pip first\n",
    "upgrade_pip()\n",
    "\n",
    "# Import required packages with error handling for missing dependencies\n",
    "for package_name in required_packages:\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"Successfully imported {package_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"Package {package_name} not found. Attempting to install...\")\n",
    "        install_package(package_name)\n",
    "\n",
    "# Define DatabaseManager with better connection handling\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_path='training_pipeline.db'):\n",
    "        self.db_path = db_path\n",
    "        logger.info(f\"Connecting to database at {self.db_path}\")\n",
    "        self.conn = sqlite3.connect(db_path, timeout=5.0, check_same_thread=False)\n",
    "        self.initialize_database()\n",
    "\n",
    "    def initialize_database(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS model_config (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                input_dim INTEGER,\n",
    "                hidden_dim INTEGER,\n",
    "                output_dim INTEGER,\n",
    "                dropout_prob REAL,\n",
    "                created_at TEXT NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS tensor_activations (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                epoch INTEGER,\n",
    "                layer TEXT,\n",
    "                activations TEXT,\n",
    "                created_at TEXT NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "        logger.info(\"Database schema initialized.\")\n",
    "\n",
    "    def store_model_config(self, input_dim, hidden_dim, output_dim, dropout_prob):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO model_config (input_dim, hidden_dim, output_dim, dropout_prob, created_at) \n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (input_dim, hidden_dim, output_dim, dropout_prob, datetime.now().isoformat()))\n",
    "            logger.info(\"Model configuration stored.\")\n",
    "        except sqlite3.IntegrityError as e:\n",
    "            logger.error(f\"IntegrityError: {e}\")\n",
    "\n",
    "    def store_tensor_activations(self, epoch, layer, activations):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO tensor_activations (epoch, layer, activations, created_at) \n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                ''', (epoch, layer, json.dumps(activations), datetime.now().isoformat()))\n",
    "            logger.info(f\"Stored activations for epoch {epoch} and layer {layer}.\")\n",
    "        except sqlite3.OperationalError as e:\n",
    "            logger.error(f\"Error storing tensor activations: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self.conn.close()\n",
    "            logger.info(f\"Closed database connection to {self.db_path}.\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error closing database connection: {e}\")\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "def generate_synthetic_data(samples=1000, features=20, classes=5):\n",
    "    try:\n",
    "        print(f\"Generating synthetic dataset with {samples} samples, {features} features, and {classes} classes...\")\n",
    "        # Adjusting the number of informative features to avoid ValueError\n",
    "        n_informative = min(features, classes - 1)  # Ensure we do not exceed limits\n",
    "        X, y = make_classification(n_samples=samples, n_features=features, n_classes=classes, \n",
    "                                   n_informative=n_informative, n_clusters_per_class=1, random_state=42)\n",
    "        print(\"Synthetic dataset generated successfully.\")\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synthetic dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Split and scale the dataset\n",
    "def preprocess_data(X, y, test_size=0.2):\n",
    "    try:\n",
    "        print(f\"Splitting dataset into training and testing sets with test size = {test_size}...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        print(\"Fitting the scaler to the training data...\")\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        print(\"Transforming the test data...\")\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"Data preprocessing completed successfully.\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "def convert_to_tensors(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        print(\"Converting training and testing data to PyTorch tensors...\")\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Debug statements to display tensor shapes\n",
    "        print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "        print(f\"X_test_tensor shape: {X_test_tensor.shape}\")\n",
    "        print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "        print(f\"y_test_tensor shape: {y_test_tensor.shape}\")\n",
    "        \n",
    "        print(\"Conversion to PyTorch tensors completed successfully.\")\n",
    "        \n",
    "        return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting data to tensors: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to create DataLoader objects\n",
    "def create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, batch_size=32):\n",
    "    try:\n",
    "        print(\"Creating DataLoader objects...\")\n",
    "        \n",
    "        # Create TensorDataset objects\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        # Create DataLoader objects\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Debug statements to display the number of batches\n",
    "        print(f\"Number of batches in training DataLoader: {len(train_loader)}\")\n",
    "        print(f\"Number of batches in testing DataLoader: {len(test_loader)}\")\n",
    "        \n",
    "        print(\"DataLoader objects created successfully.\")\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataLoader objects: {e}\")\n",
    "        raise\n",
    "\n",
    "# Model Definition\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.3, db_manager=None):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Store model configuration in the database\n",
    "        if db_manager:\n",
    "            db_manager.store_model_config(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "\n",
    "        # Visualize initial weights\n",
    "        self.visualize_weights()\n",
    "\n",
    "    def visualize_weights(self):\n",
    "        \"\"\"Visualizes the weights of each layer.\"\"\"\n",
    "        print(\"Visualizing initial weights...\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        layers = [self.layer1, self.layer2, self.layer3, self.layer4, self.output_layer]\n",
    "        for i, layer in enumerate(layers):\n",
    "            weights = layer.weight.data.cpu().numpy()\n",
    "            plt.subplot(3, 2, i + 1)\n",
    "            plt.hist(weights.flatten(), bins=30, alpha=0.7, color='blue')\n",
    "            plt.title(f'Layer {i + 1} Weights Distribution')\n",
    "            plt.xlabel('Weight Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Initial weights visualized for all layers.\")\n",
    "\n",
    "    def forward(self, x, epoch=None, db_manager=None):\n",
    "        print(f\"Forward pass input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After Layer 1: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After Layer 2: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After Layer 3: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        print(f\"After Layer 4: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        print(f\"After Output Layer: {x.shape}\")\n",
    "\n",
    "        # Store activations in the database\n",
    "        if db_manager and epoch is not None:\n",
    "            db_manager.store_tensor_activations(epoch, \"Output Layer\", x.detach().cpu().numpy().tolist())\n",
    "\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10, device='cpu', db_manager=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "        logger.info(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, epoch=epoch, db_manager=db_manager)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        logger.info(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies  # Return the losses and accuracies for plotting\n",
    "\n",
    "# 3D Plotting function\n",
    "def plot_3d_training_results(train_losses, train_accuracies, epochs):\n",
    "    print(\"Plotting 3D training results...\")\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = np.arange(1, epochs + 1)\n",
    "\n",
    "    for i in range(len(train_losses)):\n",
    "        ax.plot(x, train_losses[i], train_accuracies[i], label=f'Model {i + 1}', marker='o')\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_zlabel('Accuracy (%)')\n",
    "    ax.set_title('Training Loss and Accuracy over Epochs')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "input_dim = 20  # Example input dimension\n",
    "hidden_dim = 128  # Example hidden layer dimension\n",
    "output_dim = 5  # Example output dimension for 5 classes\n",
    "dropout_prob = 0.3  # Define dropout probability\n",
    "epochs = 100  # Define number of epochs\n",
    "\n",
    "# Prepare to store results for all models\n",
    "all_train_losses = []\n",
    "all_train_accuracies = []\n",
    "\n",
    "# Train multiple models\n",
    "num_models = 10\n",
    "for model_index in range(num_models):\n",
    "    # Generate synthetic dataset for classification\n",
    "    X, y = generate_synthetic_data(samples=1000, features=input_dim, classes=5)\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y, test_size=0.2)\n",
    "    X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_tensors(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader, test_loader = create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Initialize database\n",
    "    db_manager = DatabaseManager()\n",
    "\n",
    "    # Initialize the DNN model\n",
    "    print(f\"Initializing model {model_index + 1}...\")\n",
    "    model = DNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_prob=dropout_prob, db_manager=db_manager)\n",
    "\n",
    "    # Set the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Set the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model and store results\n",
    "    train_losses, train_accuracies = train_model(model, train_loader, optimizer, criterion, epochs=epochs, db_manager=db_manager)\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "\n",
    "    # Close the database connection\n",
    "    db_manager.close()\n",
    "\n",
    "# Plotting the results for all models\n",
    "plot_3d_training_results(all_train_losses, all_train_accuracies, epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e8457-3c33-4e79-b2f4-7885bf003f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to check and install missing packages\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Successfully installed {package_name}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package_name}\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'torch',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'tqdm',\n",
    "]\n",
    "\n",
    "# Import required packages with error handling for missing dependencies\n",
    "for package_name in required_packages:\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"Successfully imported {package_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"Package {package_name} not found. Attempting to install...\")\n",
    "        install_package(package_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b25d0-c502-41eb-8caa-0e5d110c9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for PyTorch\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting device: {e}\")\n",
    "\n",
    "# Set up plotting parameters\n",
    "try:\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "    print(\"Plotting parameters set successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting plotting parameters: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d60ee1-10e8-4f2d-b2b7-748d3a988f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Padding function for input grids\n",
    "def pad_to_30x30(grid, pad_value=10):\n",
    "    \"\"\"Pads the grid to a size of 30x30 with the specified pad_value.\"\"\"\n",
    "    try:\n",
    "        # Create a padded grid filled with the pad_value\n",
    "        padded_grid = torch.full((30, 30), pad_value, dtype=torch.int8)\n",
    "        \n",
    "        # Determine the height and width of the input grid\n",
    "        height, width = len(grid), len(grid[0])\n",
    "        \n",
    "        # Fill the padded grid with the input grid values\n",
    "        padded_grid[:height, :width] = torch.tensor(grid, dtype=torch.int8)\n",
    "        \n",
    "        print(f\"Padded grid created successfully with shape: {padded_grid.shape}\")\n",
    "        return padded_grid\n",
    "    except Exception as e:\n",
    "        print(f\"Error in padding the grid: {e}\")\n",
    "\n",
    "def visualize_padded_grid(grid):\n",
    "    \"\"\"Visualizes the padded grid.\"\"\"\n",
    "    try:\n",
    "        padded_grid = pad_to_30x30(grid)  # Get the padded grid\n",
    "        plt.imshow(padded_grid, cmap='gray', interpolation='nearest')  # Plot the grid\n",
    "        plt.colorbar()  # Add a colorbar for reference\n",
    "        plt.title(\"Padded Grid Visualization\")  # Title for the plot\n",
    "        plt.xlabel(\"Width (Columns)\")  # X-axis label\n",
    "        plt.ylabel(\"Height (Rows)\")  # Y-axis label\n",
    "        plt.show()  # Display the plot\n",
    "        print(\"Padded grid visualized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing padded grid: {e}\")\n",
    "\n",
    "# Example usage of the padding and visualization\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a sample grid (2D list)\n",
    "    sample_grid = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "    print(\"Original Grid:\")\n",
    "    print(torch.tensor(sample_grid))  # Print the original grid\n",
    "\n",
    "    # Visualize the padded grid\n",
    "    visualize_padded_grid(sample_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9686341-c646-4967-b7f5-a577b24f2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(dataset):\n",
    "    \"\"\"Preprocesses the dataset by converting images to one-hot encoding.\"\"\"\n",
    "    try:\n",
    "        one_hot_images = F.one_hot(dataset, num_classes=11)  # Assuming 11 classes\n",
    "        one_hot_images = one_hot_images.permute(0, 3, 1, 2)  # Rearrange dimensions for PyTorch\n",
    "        print(f\"Preprocessed images to one-hot encoding with shape: {one_hot_images.shape}\")\n",
    "        return one_hot_images.float()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing images: {e}\")\n",
    "\n",
    "def visualize_one_hot_images(one_hot_images):\n",
    "    \"\"\"Visualizes one-hot encoded images.\"\"\"\n",
    "    try:\n",
    "        # Assuming the one-hot images are of shape (N, C, H, W)\n",
    "        num_images = one_hot_images.shape[0]\n",
    "        fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            axs[i].imshow(one_hot_images[i].permute(1, 2, 0).numpy())  # Convert back to HWC for plotting\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(f'Image {i + 1}')\n",
    "        \n",
    "        plt.suptitle(\"One-Hot Encoded Images Visualization\")\n",
    "        plt.show()\n",
    "        print(\"One-hot encoded images visualized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing one-hot images: {e}\")\n",
    "\n",
    "def preprocess_text(text_data):\n",
    "    \"\"\"Preprocesses text data by tokenization and lowercasing.\"\"\"\n",
    "    try:\n",
    "        preprocessed_text = [text.lower() for text in text_data]  # Convert to lowercase\n",
    "        print(f\"Preprocessed text: {preprocessed_text}\")\n",
    "        return preprocessed_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing text: {e}\")\n",
    "\n",
    "def preprocess_audio(audio_data):\n",
    "    \"\"\"Preprocesses audio data (placeholder function).\"\"\"\n",
    "    try:\n",
    "        # Implement audio preprocessing logic, e.g., normalization, filtering, etc.\n",
    "        print(f\"Audio data processed: {audio_data}\")  # Placeholder\n",
    "        return audio_data  # Return processed audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing audio: {e}\")\n",
    "\n",
    "# Example usage of preprocessing functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a sample dataset (tensor of image classes)\n",
    "    sample_images = torch.randint(0, 11, (5, 30, 30))  # 5 images with random class labels\n",
    "    print(\"Original Image Data:\")\n",
    "    print(sample_images)\n",
    "\n",
    "    # Preprocess and visualize images\n",
    "    one_hot_encoded_images = preprocess_images(sample_images)\n",
    "    visualize_one_hot_images(one_hot_encoded_images)\n",
    "\n",
    "    # Define sample text data\n",
    "    sample_text = [\"Hello World!\", \"Preprocessing is fun.\"]\n",
    "    print(\"Original Text Data:\")\n",
    "    print(sample_text)\n",
    "    \n",
    "    # Preprocess text\n",
    "    preprocessed_text = preprocess_text(sample_text)\n",
    "\n",
    "    # Define sample audio data (placeholder)\n",
    "    sample_audio = [0.5, -0.1, 0.7]  # Placeholder audio data\n",
    "    print(\"Original Audio Data:\")\n",
    "    print(sample_audio)\n",
    "\n",
    "    # Preprocess audio\n",
    "    preprocessed_audio = preprocess_audio(sample_audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8aaad-7934-4d4a-8b80-20fa66715136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Counts the number of trainable parameters in a model.\"\"\"\n",
    "    try:\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total trainable parameters: {num_params}\")\n",
    "        return num_params\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting parameters: {e}\")\n",
    "\n",
    "# Set up color map for visualizations\n",
    "def setup_colormap():\n",
    "    \"\"\"Sets up the colormap for visualizations.\"\"\"\n",
    "    try:\n",
    "        _cmap = colors.ListedColormap(\n",
    "            ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "             '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25', \"#FFFFFF\"]\n",
    "        )\n",
    "        _norm = colors.Normalize(vmin=0, vmax=10)\n",
    "        print(\"Colormap and normalization set up successfully.\")\n",
    "        return _cmap, _norm\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up colormap: {e}\")\n",
    "        return None, None  # Return None in case of error\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a simple model for demonstration\n",
    "    class SimpleModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleModel, self).__init__()\n",
    "            self.fc1 = nn.Linear(10, 5)\n",
    "            self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = SimpleModel()\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    # Count parameters\n",
    "    count_parameters(model)\n",
    "\n",
    "    # Set up colormap\n",
    "    colormap, normalization = setup_colormap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8287b6-53c1-4170-b46a-8ca38b684cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Setup colormap and normalization (assuming this is defined somewhere in your code)\n",
    "_cmap = colors.ListedColormap(\n",
    "    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25', \"#FFFFFF\"]\n",
    ")\n",
    "_norm = colors.Normalize(vmin=0, vmax=10)\n",
    "\n",
    "def plot_img(input_matrix, ax, title=\"\"):\n",
    "    \"\"\"Plots an input matrix as an image.\"\"\"\n",
    "    try:\n",
    "        ax.imshow(input_matrix, cmap=_cmap, norm=_norm)\n",
    "        ax.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        ax.set_yticks([x - 0.5 for x in range(1 + len(input_matrix))])\n",
    "        ax.set_xticks([x - 0.5 for x in range(1 + len(input_matrix[0]))]) \n",
    "        \n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        print(f\"Successfully plotted image with title: '{title}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting image: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    # Generate a sample input matrix (e.g., 10x10 grid with random values)\n",
    "    sample_matrix = np.random.randint(0, 10, size=(10, 10))\n",
    "\n",
    "    # Plot the sample matrix\n",
    "    plot_img(sample_matrix, ax, title=\"Sample Image\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5263979-5c88-4ec2-bc37-b8f84c5d2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "def search_file(filename, search_path):\n",
    "    \"\"\"Search for a file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            print(f\"Found file: {os.path.join(root, filename)}\")\n",
    "            return os.path.join(root, filename)\n",
    "    print(f\"File not found: {filename}\")\n",
    "    return None\n",
    "\n",
    "# Define paths for datasets\n",
    "train_challenges_filename = \"arc-agi_training_challenges.json\"\n",
    "train_solutions_filename = \"arc-agi_training_solutions.json\"\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading the training challenges dataset...\")\n",
    "train_challenges_path = search_file(train_challenges_filename, \".\")\n",
    "train_solutions_path = search_file(train_solutions_filename, \".\")\n",
    "\n",
    "if train_challenges_path is not None and train_solutions_path is not None:\n",
    "    try:\n",
    "        with open(train_challenges_path, 'r') as f:\n",
    "            train_challenges = json.load(f)\n",
    "            print(\"Training challenges dataset loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading training challenges dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        with open(train_solutions_path, 'r') as f:\n",
    "            train_solutions = json.load(f)\n",
    "            print(\"Training solutions dataset loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading training solutions dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    train_ids = list(train_challenges.keys())\n",
    "else:\n",
    "    print(\"Failed to load datasets. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555d651-7b33-4fc6-82ac-cc7605eccf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Neural Network Components\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bnorm1 = nn.BatchNorm2d(C)\n",
    "        self.bnorm2 = nn.BatchNorm2d(C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        print(f\"Initialized ResBlock with {C} channels and {dropout_prob} dropout probability.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            r = self.conv1(self.relu(self.bnorm1(x)))\n",
    "            r = self.dropout(r)\n",
    "            r = self.conv2(self.relu(self.bnorm2(r)))\n",
    "            print(\"Forward pass successful in ResBlock.\")\n",
    "            return r + x\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in ResBlock: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c47a6-85b1-4212-a303-b591ecff0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, mode: str, C_in: int, C_out: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(C_out)\n",
    "        \n",
    "        # Initialize the convolution layer based on the mode\n",
    "        if mode == \"down\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "            print(f\"Initialized ConvBlock in 'down' mode with {C_in} input channels and {C_out} output channels.\")\n",
    "        elif mode == \"up\":\n",
    "            self.conv = nn.ConvTranspose2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "            print(f\"Initialized ConvBlock in 'up' mode with {C_in} input channels and {C_out} output channels.\")\n",
    "        elif mode == \"same\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1)\n",
    "            print(f\"Initialized ConvBlock in 'same' mode with {C_in} input channels and {C_out} output channels.\")\n",
    "        else:\n",
    "            raise ValueError(\"Wrong ConvBlock mode.\")\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, z):\n",
    "        try:\n",
    "            x = self.conv(z)\n",
    "            x = self.bnorm(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            print(\"Forward pass successful in ConvBlock.\")\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in ConvBlock: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586d863-b9ac-40d2-b6a2-583b08ef1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], latent_dim=512, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Initialize convolutional and residual blocks\n",
    "        try:\n",
    "            self.conv1 = ConvBlock(\"down\", 11, channels[0], dropout)\n",
    "            self.res12 = ResBlock(channels[0], dropout)\n",
    "            self.conv2 = ConvBlock(\"down\", channels[0], channels[1], dropout)\n",
    "            self.res23 = ResBlock(channels[1], dropout)\n",
    "            self.conv3 = ConvBlock(\"down\", channels[1], channels[2], dropout)\n",
    "            self.fc = nn.Linear(channels[2] * 2 * 2, latent_dim)\n",
    "            print(f\"Encoder initialized with channels: {channels}, latent_dim: {latent_dim}, dropout: {dropout}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Encoder initialization: {e}\")\n",
    "\n",
    "    def forward(self, z):\n",
    "        residuals = [0] * 3\n",
    "        try:\n",
    "            x = preprocess_images(z)  # Preprocess the input\n",
    "            x = self.conv1(x)  # Apply the first convolution\n",
    "            x = self.res12(x)  # Apply the first residual block\n",
    "            residuals[0] = x  # Store the residual\n",
    "            x = self.conv2(x)  # Apply the second convolution\n",
    "            x = self.res23(x)  # Apply the second residual block\n",
    "            residuals[1] = x  # Store the residual\n",
    "            x = self.conv3(x)  # Apply the third convolution\n",
    "            residuals[2] = x  # Store the residual\n",
    "            \n",
    "            x = x.reshape(x.size(0), -1)  # Flatten the output\n",
    "            encoded = self.fc(x)  # Linear transformation\n",
    "            print(\"Forward pass successful in Encoder.\")\n",
    "            return encoded, residuals\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in Encoder: {e}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b243937-9fa3-4206-88c8-da775f87cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], latent_dim=512, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.channels = channels\n",
    "        \n",
    "        try:\n",
    "            self.fc = nn.Linear(latent_dim, channels[-1] * 2 * 2)\n",
    "            self.conv3 = ConvBlock(\"up\", channels[-1] * 2, channels[-2], dropout)\n",
    "            self.res32 = ResBlock(channels[-2], dropout)\n",
    "            self.conv2 = ConvBlock(\"up\", channels[-2] * 2, channels[-3], dropout)\n",
    "            self.res21 = ResBlock(channels[-3], dropout)\n",
    "            self.conv1 = ConvBlock(\"up\", channels[-3] * 2, channels[-3], dropout)\n",
    "            self.conv0 = nn.Conv2d(channels[-3], 11, kernel_size=3, padding=1)\n",
    "            print(f\"Decoder initialized with channels: {channels}, latent_dim: {latent_dim}, dropout: {dropout}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Decoder initialization: {e}\")\n",
    "\n",
    "    def forward(self, z, residuals):\n",
    "        try:\n",
    "            x = self.fc(z)  # Apply the fully connected layer\n",
    "            x = x.reshape(x.size(0), self.channels[-1], 2, 2)  # Reshape for convolutional layers\n",
    "            \n",
    "            # Concatenate and apply the decoding layers\n",
    "            x = torch.cat((x, residuals[2]), dim=1)\n",
    "            x = self.conv3(x)\n",
    "            x = self.res32(x)\n",
    "            x = torch.cat((x, residuals[1]), dim=1)\n",
    "            x = self.conv2(x)\n",
    "            x = self.res21(x)\n",
    "            x = torch.cat((x, residuals[0]), dim=1)\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv0(x)  # Final convolution\n",
    "            \n",
    "            print(\"Forward pass successful in Decoder.\")\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in Decoder: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c0640-654f-46c2-bd8b-917904315259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        try:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "            print(f\"MLP initialized with input_size: {input_size}, hidden_size: {hidden_size}, output_size: {output_size}, dropout: {dropout}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during MLP initialization: {e}\")\n",
    "\n",
    "    def forward(self, z):\n",
    "        try:\n",
    "            x = self.relu(self.bn1(self.fc1(z)))  # First layer with batch normalization and ReLU\n",
    "            x = self.dropout(x)  # Apply dropout\n",
    "            x = self.relu(self.bn2(self.fc2(x)))  # Second layer with batch normalization and ReLU\n",
    "            x = self.dropout(x)  # Apply dropout\n",
    "            output = self.fc3(x)  # Final output layer\n",
    "            \n",
    "            print(\"Forward pass successful in MLP.\")\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in MLP: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69081878-d180-40cc-8e39-be5ffe0e5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Magician(nn.Module):\n",
    "    def __init__(self, channels, latent_dim, hidden_size, dropout=0.1):\n",
    "        super(Magician, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        try:\n",
    "            self.encoder = Encoder(channels, latent_dim, dropout)\n",
    "            self.decoder = Decoder(channels, latent_dim, dropout)\n",
    "            self.mlp_key = MLP(latent_dim * 6, hidden_size, latent_dim)\n",
    "            self.mlp_map = MLP(latent_dim * 2, hidden_size, latent_dim)\n",
    "            print(f\"Magician initialized with channels: {channels}, latent_dim: {latent_dim}, hidden_size: {hidden_size}, dropout: {dropout}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Magician initialization: {e}\")\n",
    "\n",
    "    def forward(self, input, edu_pairs):\n",
    "        try:\n",
    "            edu_pairs_encoded = edu_pairs.flatten(end_dim=-3)\n",
    "            edu_pairs_encoded, _ = self.encoder(edu_pairs_encoded)\n",
    "            edu_pairs_encoded = edu_pairs_encoded.reshape(2, -1, 3, self.latent_dim).permute(1, 0, 2, 3).flatten(start_dim=1)\n",
    "            key = self.mlp_key(edu_pairs_encoded)\n",
    "\n",
    "            input_encoded, residuals = self.encoder(input)\n",
    "            output_encoded = self.mlp_map(torch.cat((input_encoded, key), dim=1))\n",
    "            output_decoded = self.decoder(output_encoded, residuals)\n",
    "            \n",
    "            print(\"Forward pass successful in Magician.\")\n",
    "            return output_decoded\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass in Magician: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527d5ce-1f64-4100-815b-acdd9cbc0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pred(id_val, model):\n",
    "    try:\n",
    "        # Load educational examples for the specified task ID\n",
    "        edu_examples = train_challenges[id_val][\"train\"]\n",
    "        edu_examples = edu_examples[:3]\n",
    "\n",
    "        # Ensure there are exactly 3 examples\n",
    "        while len(edu_examples) < 3:\n",
    "            edu_examples += [edu_examples[-1]]\n",
    "\n",
    "        # Pad the input grid to 30x30\n",
    "        input = pad_to_30x30(train_challenges[id_val][\"test\"][0][\"input\"]).unsqueeze(0)\n",
    "        edu_pairs = torch.zeros((2, 1, 3, 30, 30), dtype=torch.int64)\n",
    "\n",
    "        for j in range(3):\n",
    "            edu_pairs[0, 0, j] = pad_to_30x30(edu_examples[j][\"input\"])\n",
    "            edu_pairs[1, 0, j] = pad_to_30x30(edu_examples[j][\"output\"])\n",
    "\n",
    "        # Convert to long type\n",
    "        input = input.long()\n",
    "        edu_pairs = edu_pairs.long()\n",
    "\n",
    "        # Make predictions using the model\n",
    "        output_pred_padded = torch.argmax(model(input, edu_pairs)[0], dim=0)\n",
    "\n",
    "        # Calculate the limits for horizontal and vertical dimensions\n",
    "        lim_hor = (output_pred_padded[0] < 10).sum()\n",
    "        lim_ver = (output_pred_padded[:, 0] < 10).sum()\n",
    "\n",
    "        # Slice the output prediction based on calculated limits\n",
    "        output_pred = output_pred_padded[:lim_ver, :lim_hor]\n",
    "\n",
    "        print(f\"Predictions generated successfully for task ID: {id_val}.\")\n",
    "        return output_pred\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction for task ID {id_val}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb240e-9a82-43ff-a482-f181a56d6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_task_prediction(id_val, model):\n",
    "    try:\n",
    "        # Load the true output for the specified task ID\n",
    "        output_true = np.array(train_solutions[id_val][0])\n",
    "        \n",
    "        # Generate predictions using the model\n",
    "        output_pred = model_pred(id_val, model).cpu().numpy()\n",
    "        \n",
    "        # Check if the shapes of predicted and true outputs match\n",
    "        if output_pred is None:\n",
    "            print(f\"Prediction failed for task ID {id_val}. Skipping display.\")\n",
    "            return\n",
    "        \n",
    "        if output_pred.shape == output_true.shape:\n",
    "            mes = f\"#err = {(output_pred != output_true).sum()}\"\n",
    "        else:\n",
    "            mes = f\"wrong shape: predicted shape {output_pred.shape}, expected shape {output_true.shape}\"\n",
    "\n",
    "        # Load educational examples for visualization\n",
    "        imgs = train_challenges[id_val][\"train\"]\n",
    "        edu_examples_num = min(len(imgs), 3)\n",
    "\n",
    "        # Create subplots for visualization\n",
    "        fig, axs = plt.subplots(2, edu_examples_num + 2, dpi=150, figsize=((edu_examples_num + 2) * 3, 6))\n",
    "\n",
    "        # Plot educational inputs and outputs\n",
    "        for j in range(edu_examples_num):\n",
    "            plot_img(imgs[j][\"input\"], axs[0, j], \"edu input\")\n",
    "            plot_img(imgs[j][\"output\"], axs[1, j], \"edu output\")\n",
    "        \n",
    "        # Plot the test input, true output, and predicted output\n",
    "        plot_img(train_challenges[id_val][\"test\"][0][\"input\"], axs[0, edu_examples_num], \"input\")\n",
    "        plot_img(output_true, axs[1, edu_examples_num + 1], \"output true\")\n",
    "        plot_img(output_pred, axs[1, edu_examples_num], \"output pred\")\n",
    "\n",
    "        # Remove the unused subplot\n",
    "        fig.delaxes(axs[0, edu_examples_num + 1])\n",
    "        \n",
    "        # Set the title for the figure\n",
    "        plt.suptitle(f\"{id_val}: {mes}\")\n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Displayed predictions for task ID: {id_val}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying prediction for task ID {id_val}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e9226-bf9b-42c5-927f-3c50732f9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset for binary classification\n",
    "def generate_synthetic_data(samples=1000, features=20, classes=2):\n",
    "    try:\n",
    "        print(f\"Generating synthetic dataset with {samples} samples, {features} features, and {classes} classes...\")\n",
    "        X, y = make_classification(n_samples=samples, n_features=features, n_classes=classes, random_state=42)\n",
    "        print(\"Synthetic dataset generated successfully.\")\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synthetic dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Split and scale the dataset\n",
    "def preprocess_data(X, y, test_size=0.2):\n",
    "    try:\n",
    "        print(f\"Splitting dataset into training and testing sets with test size = {test_size}...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        print(\"Fitting the scaler to the training data...\")\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        print(\"Transforming the test data...\")\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"Data preprocessing completed successfully.\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data preprocessing: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5a3ff-e0fa-4039-844b-f3077953559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "def convert_to_tensors(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        print(\"Converting training and testing data to PyTorch tensors...\")\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Debug statements to display tensor shapes\n",
    "        print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "        print(f\"X_test_tensor shape: {X_test_tensor.shape}\")\n",
    "        print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "        print(f\"y_test_tensor shape: {y_test_tensor.shape}\")\n",
    "        \n",
    "        print(\"Conversion to PyTorch tensors completed successfully.\")\n",
    "        \n",
    "        return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting data to tensors: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f06dc-bc39-452c-a6b6-48a8fcf5eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configure logging for console output\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# Define DatabaseManager with better connection handling\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_path='training_pipeline.db'):\n",
    "        self.db_path = db_path\n",
    "        logger.info(f\"Connecting to database at {self.db_path}\")\n",
    "        self.conn = sqlite3.connect(db_path, timeout=5.0, check_same_thread=False)\n",
    "        self.initialize_database()\n",
    "\n",
    "    def initialize_database(self):\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS model_config (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    input_dim INTEGER,\n",
    "                    hidden_dim INTEGER,\n",
    "                    output_dim INTEGER,\n",
    "                    dropout_prob REAL,\n",
    "                    created_at TEXT NOT NULL\n",
    "                )\n",
    "            ''')\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS tensor_activations (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    epoch INTEGER,\n",
    "                    layer TEXT,\n",
    "                    activations TEXT,\n",
    "                    created_at TEXT NOT NULL\n",
    "                )\n",
    "            ''')\n",
    "            self.conn.commit()\n",
    "            logger.info(\"Database schema initialized.\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error initializing database: {e}\")\n",
    "            print(f\"Error initializing database: {e}\")\n",
    "\n",
    "    def store_model_config(self, input_dim, hidden_dim, output_dim, dropout_prob):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO model_config (input_dim, hidden_dim, output_dim, dropout_prob, created_at) \n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (input_dim, hidden_dim, output_dim, dropout_prob, datetime.now().isoformat()))\n",
    "            logger.info(\"Model configuration stored.\")\n",
    "        except sqlite3.IntegrityError as e:\n",
    "            logger.error(f\"IntegrityError: {e}\")\n",
    "            print(f\"IntegrityError: {e}\")\n",
    "\n",
    "    def store_tensor_activations(self, epoch, layer, activations):\n",
    "        try:\n",
    "            with self.conn:\n",
    "                cursor = self.conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO tensor_activations (epoch, layer, activations, created_at) \n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                ''', (epoch, layer, json.dumps(activations), datetime.now().isoformat()))\n",
    "            logger.info(f\"Stored activations for epoch {epoch} and layer {layer}.\")\n",
    "        except sqlite3.OperationalError as e:\n",
    "            logger.error(f\"Error storing tensor activations: {e}\")\n",
    "            print(f\"Error storing tensor activations: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self.conn.close()\n",
    "            logger.info(f\"Closed database connection to {self.db_path}.\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error closing database connection: {e}\")\n",
    "            print(f\"Error closing database connection: {e}\")\n",
    "\n",
    "\n",
    "# Function to create DataLoader objects\n",
    "def create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, batch_size=32):\n",
    "    try:\n",
    "        print(\"Creating DataLoader objects...\")\n",
    "        \n",
    "        # Create TensorDataset objects\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        # Create DataLoader objects\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Debug statements to display the number of batches\n",
    "        print(f\"Number of batches in training DataLoader: {len(train_loader)}\")\n",
    "        print(f\"Number of batches in testing DataLoader: {len(test_loader)}\")\n",
    "        \n",
    "        print(\"DataLoader objects created successfully.\")\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataLoader objects: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to create DataLoader objects\n",
    "X, y = generate_synthetic_data(samples=1000, features=20, classes=2)\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y, test_size=0.2)\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_tensors(X_train, X_test, y_train, y_test)\n",
    "train_loader, test_loader = create_data_loaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Initialize or load the database\n",
    "def initialize_or_load_db(db_path='training_pipeline.db'):\n",
    "    try:\n",
    "        print(f\"Attempting to connect to the database at {db_path}...\")\n",
    "        db_manager = DatabaseManager(db_path=db_path)\n",
    "        print(\"Database connection established.\")\n",
    "        return db_manager\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing or loading the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to initialize or load the database\n",
    "db_manager = initialize_or_load_db()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268091e-9b7b-4246-91d0-883bb7305b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.3, db_manager=None):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Store model configuration in the database\n",
    "        if db_manager:\n",
    "            db_manager.store_model_config(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "\n",
    "        # Visualize initial weights\n",
    "        self.visualize_weights()\n",
    "\n",
    "    def visualize_weights(self):\n",
    "        \"\"\"Visualizes the weights of each layer.\"\"\"\n",
    "        print(\"Visualizing initial weights...\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        layers = [self.layer1, self.layer2, self.layer3, self.layer4, self.output_layer]\n",
    "        for i, layer in enumerate(layers):\n",
    "            weights = layer.weight.data.cpu().numpy()\n",
    "            plt.subplot(3, 2, i + 1)\n",
    "            plt.hist(weights.flatten(), bins=30, alpha=0.7, color='blue')\n",
    "            plt.title(f'Layer {i + 1} Weights Distribution')\n",
    "            plt.xlabel('Weight Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Initial weights visualized for all layers.\")\n",
    "\n",
    "    def forward(self, x, epoch=None, db_manager=None):\n",
    "        print(f\"Forward pass input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After Layer 1: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After Layer 2: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After Layer 3: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        print(f\"After Layer 4: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        print(f\"After Output Layer: {x.shape}\")\n",
    "\n",
    "        # Store activations in the database\n",
    "        if db_manager and epoch is not None:\n",
    "            db_manager.store_tensor_activations(epoch, \"Output Layer\", x.detach().cpu().numpy().tolist())\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model instantiation and training parameters\n",
    "input_dim = X_train.shape[1]  # Set input dimension based on the training data\n",
    "hidden_dim = 128  # Define hidden layer dimension\n",
    "output_dim = 2  # Set output dimension for binary classification\n",
    "dropout_prob = 0.3  # Define dropout probability\n",
    "\n",
    "# Initialize the DNN model\n",
    "print(\"Initializing the DNN model...\")\n",
    "model = DNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_prob=dropout_prob, db_manager=db_manager)\n",
    "\n",
    "# Set the optimizer\n",
    "print(\"Setting up the optimizer...\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the loss function\n",
    "print(\"Setting up the loss function (CrossEntropyLoss)...\")\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45687580-0314-45ae-ad72-56168e036c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training results\n",
    "def plot_training_results(train_losses, train_accuracies, epochs):\n",
    "    \"\"\"\n",
    "    Plots training loss and accuracy over epochs.\n",
    "    \n",
    "    Args:\n",
    "    - train_losses (list): List of training losses.\n",
    "    - train_accuracies (list): List of training accuracies.\n",
    "    - epochs (int): Number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Plotting training results...\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy', color='green')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Training results plotted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae6ce3-fa28-407c-bd8e-1140cf5dc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10, device='cpu', db_manager=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, epoch=epoch, db_manager=db_manager)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    plot_training_results(train_losses, train_accuracies, epochs)  # Plot results after training\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, optimizer, criterion, epochs=10, db_manager=db_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb69603-2e05-4be4-be3d-de9a10a1478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_training_results(all_train_losses, all_train_accuracies, num_models):\n",
    "    if not all_train_losses or not all_train_accuracies or len(all_train_losses) != num_models or len(all_train_accuracies) != num_models:\n",
    "        print(\"Error during 3D plotting: Data structure is not valid.\")\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for model_id in range(num_models):\n",
    "        ax.plot(range(len(all_train_losses[model_id])), all_train_losses[model_id], zs=model_id, zdir='y', label=f'Model {model_id + 1}', alpha=0.5)\n",
    "        ax.plot(range(len(all_train_accuracies[model_id])), all_train_accuracies[model_id], zs=model_id, zdir='x', label=f'Model {model_id + 1}', alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Model ID')\n",
    "    ax.set_zlabel('Loss/Accuracy')\n",
    "    ax.set_title('3D Training Loss and Accuracy of Models')\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4153156-d5c1-4df5-a45e-0fe950c2c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training results\n",
    "def plot_training_results(train_losses, train_accuracies, epochs):\n",
    "    \"\"\"\n",
    "    Plots training loss and accuracy over epochs.\n",
    "    \n",
    "    Args:\n",
    "    - train_losses (list): List of training losses.\n",
    "    - train_accuracies (list): List of training accuracies.\n",
    "    - epochs (int): Number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Plotting training results...\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy', color='green')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Training results plotted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8d3a3-ec47-43a4-8b76-5e5958a137bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10, device='cpu', db_manager=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, epoch=epoch, db_manager=db_manager)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    plot_training_results(train_losses, train_accuracies, epochs)  # Plot results after training\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, optimizer, criterion, epochs=10, db_manager=db_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b042bf5-8a46-4df5-b0d3-50c42983e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output grid predictions using the trained models on the test inputs\n",
    "try:\n",
    "    print(\"Generating output grid predictions...\")\n",
    "    \n",
    "    # Prepare test inputs\n",
    "    test_inputs = []\n",
    "    for id_val in train_ids:\n",
    "        padded_input = pad_to_30x30(train_challenges[id_val][\"test\"][0][\"input\"])\n",
    "        test_inputs.append(padded_input)\n",
    "        print(f\"Padded input grid for ID {id_val} created with shape: {padded_input.shape}\")\n",
    "\n",
    "    # Initialize a list to store output grids for each model\n",
    "    output_grids = []\n",
    "    models = []\n",
    "    \n",
    "    # Iterate through each model and generate predictions\n",
    "    for i, model in enumerate(models):\n",
    "        try:\n",
    "            # Get the model prediction for each test input\n",
    "            predictions = [model_pred(id_val, model).cpu().numpy() for id_val in train_ids]\n",
    "            output_grids.append(predictions)\n",
    "            print(f\"Model {i + 1} predictions generated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating predictions for model {i + 1}: {e}\")\n",
    "    \n",
    "    print(\"Output grid predictions generation completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during output grid predictions generation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60066391-bd9b-44a0-b380-0b1bac584c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the output grids\n",
    "try:\n",
    "    for idx, model_outputs in enumerate(output_grids):\n",
    "        for output_grid in model_outputs:\n",
    "            try:\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.title(f\"Predicted Output Grid for Test ID {train_ids[idx]}\")\n",
    "                plt.imshow(output_grid, cmap=_cmap, norm=_norm)\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "                print(f\"Output grid for Test ID {train_ids[idx]} visualized successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error visualizing output grid for Test ID {train_ids[idx]}: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during output grid visualization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085fee0-7b1f-4960-80c9-fb5d1d7cb65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e378442-359c-464f-a3e8-678d3e33170e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
