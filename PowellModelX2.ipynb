{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12beca56-5e7b-4b25-b12e-b79cccd83c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import threading\n",
    "import queue\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import copy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3f994a-6f12-4306-af36-8be5415469e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Configure Logging\n",
    "# ==========================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"training.log\", mode='w')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97e7dcd-4c27-4219-bdb8-068899435284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Model Definitions\n",
    "# ==========================\n",
    "\n",
    "class CNNMagician(nn.Module):\n",
    "    def __init__(self, conv_layers=[(32, 5), (64, 5)], dropout_rate=0.5, use_batchnorm=True, num_classes=10):\n",
    "        \"\"\"\n",
    "        Initialize CNNMagician model.\n",
    "\n",
    "        Parameters:\n",
    "        - conv_layers: List of tuples where each tuple defines (number_of_filters, kernel_size)\n",
    "        - dropout_rate: Dropout probability to avoid overfitting\n",
    "        - use_batchnorm: Boolean to decide whether to use batch normalization\n",
    "        - num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        super(CNNMagician, self).__init__()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        # Dynamically create convolutional layers\n",
    "        in_channels = 1  # Input channels for grayscale images\n",
    "        for out_channels, kernel_size in conv_layers:\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2))\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm2d(out_channels))\n",
    "            else:\n",
    "                self.bns.append(nn.Identity())  # Skip batch normalization if not needed\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer to reduce spatial dimensions\n",
    "        self.fc1 = None  # Fully connected layer initialized dynamically\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # Final output layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.initialize_weights()  # Initialize weights for all layers\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Apply Xavier initialization to convolutional and fully connected layers.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def preprocess_input(self, x):\n",
    "        \"\"\"\n",
    "        Ensure the input tensor is correctly shaped and normalized.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(x, torch.Tensor) and len(x.shape) == 3:\n",
    "                x = x.unsqueeze(1)  # Add a channel dimension for grayscale images\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)  # Convert and add channel dimension\n",
    "            x = x / 255.0  # Normalize input\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preprocessing input: {e}\")\n",
    "            raise\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the CNNMagician.\"\"\"\n",
    "        try:\n",
    "            x = self.preprocess_input(x)\n",
    "            logging.info(f\"Input shape after preprocessing: {x.shape}\")\n",
    "            \n",
    "            # Apply convolutional layers with batch normalization and pooling\n",
    "            for conv, bn in zip(self.conv_layers, self.bns):\n",
    "                x = self.pool(F.relu(bn(conv(x))))\n",
    "                logging.info(f\"Shape after conv layer: {x.shape}\")\n",
    "\n",
    "            # Dynamically define fully connected layer based on the input size\n",
    "            if self.fc1 is None:\n",
    "                flattened_size = x.view(x.size(0), -1).size(1)\n",
    "                self.fc1 = nn.Linear(flattened_size, 128).to(x.device)\n",
    "                logging.info(f\"Dynamically created fc1 with input size: {flattened_size}\")\n",
    "\n",
    "            # Flatten the tensor to match the input size of the fully connected layer\n",
    "            x = x.view(x.size(0), -1)\n",
    "            logging.info(f\"Shape after flattening: {x.shape}\")\n",
    "\n",
    "            # Pass through the fully connected layers\n",
    "            x = F.relu(self.fc1(x))\n",
    "            logging.info(f\"Shape after fc1: {x.shape}\")\n",
    "            x = self.dropout(x)  # Apply dropout\n",
    "            x = self.fc2(x)  # Output layer\n",
    "            return x\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass of CNNMagician: {e}\")\n",
    "            raise\n",
    "        \n",
    "\n",
    "class DNNMagician(nn.Module):\n",
    "    def __init__(self, input_size=900, hidden_sizes=[512, 256], dropout_rate=0.5, num_classes=10, \n",
    "                 activation_fn=F.relu, use_layer_norm=False):\n",
    "        \"\"\"\n",
    "        A flexible deep neural network with parameterized activation functions, normalization, \n",
    "        and dropout options.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size (int): The size of the input feature vector.\n",
    "        - hidden_sizes (list): List of sizes for hidden layers.\n",
    "        - dropout_rate (float): Dropout rate applied after each hidden layer.\n",
    "        - num_classes (int): The number of output classes.\n",
    "        - activation_fn (function): Activation function to be used (default: ReLU).\n",
    "        - use_layer_norm (bool): If True, use LayerNorm instead of BatchNorm.\n",
    "        \"\"\"\n",
    "        super(DNNMagician, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], num_classes)\n",
    "        \n",
    "        # Normalization layers\n",
    "        if use_layer_norm:\n",
    "            self.norm1 = nn.LayerNorm(hidden_sizes[0])\n",
    "            self.norm2 = nn.LayerNorm(hidden_sizes[1])\n",
    "        else:\n",
    "            self.norm1 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "            self.norm2 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Weight initialization for the fully connected layers.\"\"\"\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def preprocess_input(self, x):\n",
    "        \"\"\"Preprocess input by ensuring it is flattened and normalized, with error handling for shape issues.\"\"\"\n",
    "        try:\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                if len(x.shape) > 2:  # Flatten input if it's more than 2D\n",
    "                    x = x.view(x.size(0), -1)\n",
    "            else:\n",
    "                raise ValueError(\"Input should be a torch.Tensor\")\n",
    "            x = x / 255.0  # Normalize input (assuming inputs are images in range [0, 255])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in preprocessing input: {e}\")\n",
    "            raise\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        try:\n",
    "            x = self.preprocess_input(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during input preprocessing: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Layer 1 with normalization, activation, and dropout\n",
    "        try:\n",
    "            x = self.fc1(x)\n",
    "            x = self.norm1(x)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at layer 1: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Layer 2 with normalization, activation, and dropout\n",
    "        try:\n",
    "            x = self.fc2(x)\n",
    "            x = self.norm2(x)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at layer 2: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Final output layer (no activation)\n",
    "        try:\n",
    "            output = self.fc3(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at final layer: {e}\")\n",
    "            return None\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResNetMagician(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Neural Network architecture with input preprocessing and error handling for ARC dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, activation_fn=F.relu):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_classes (int): The number of output classes.\n",
    "        - activation_fn (function): Activation function to be used (default: ReLU).\n",
    "        \"\"\"\n",
    "        super(ResNetMagician, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "        # Convolutional and batch normalization layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64 * 7 * 7, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize the weights of the layers.\"\"\"\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def preprocess_input(self, x):\n",
    "        \"\"\"Preprocess input by ensuring it has the correct shape and is normalized, with error handling.\"\"\"\n",
    "        try:\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                if len(x.shape) == 3:\n",
    "                    x = x.unsqueeze(1)  # Add channel dimension if missing (assuming grayscale input)\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)  # Convert and add channel dimension\n",
    "            else:\n",
    "                raise ValueError(\"Input must be a torch.Tensor or a numpy array\")\n",
    "            x = x / 255.0  # Normalize input\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in input preprocessing: {e}\")\n",
    "            raise\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        try:\n",
    "            x = self.preprocess_input(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during input preprocessing: {e}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # First convolution + batch norm + activation\n",
    "            residual = x  # Save input for residual connection\n",
    "            out = self.activation_fn(self.bn1(self.conv1(x)))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at conv1: {e}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Second convolution + batch norm + residual connection\n",
    "            out = self.bn2(self.conv2(out))\n",
    "            out += residual  # Add the residual connection\n",
    "            out = self.activation_fn(out)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at conv2: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Third convolution + pooling\n",
    "            out = self.pool(self.activation_fn(self.bn3(self.conv3(out))))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at conv3 and pooling: {e}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Flatten and fully connected layer\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.fc(out)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in forward pass at fully connected layer: {e}\")\n",
    "            return None\n",
    "\n",
    "        return out\n",
    "\n",
    "class VisionTransformerMagician(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer architecture with input preprocessing and error handling for ARC dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, patch_size=5, embed_dim=64, depth=6, num_heads=8, mlp_dim=128, dropout=0.1):\n",
    "        super(VisionTransformerMagician, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (30 // patch_size) ** 2\n",
    "        self.embedding = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout)\n",
    "        self.transformer = TransformerEncoder(encoder_layers, num_layers=depth)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize the weights of the model layers.\"\"\"\n",
    "        nn.init.kaiming_normal_(self.embedding.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "    def preprocess_input(self, x):\n",
    "        \"\"\"Preprocess input to ensure it has the correct shape and normalization, with error handling.\"\"\"\n",
    "        try:\n",
    "            if isinstance(x, np.ndarray):\n",
    "                x = torch.tensor(x, dtype=torch.float32)\n",
    "            if len(x.shape) == 3:  # If missing batch or channel dimension\n",
    "                x = x.unsqueeze(1)  # Add channel dimension if missing (grayscale assumption)\n",
    "            elif len(x.shape) == 4 and x.shape[1] != 1:  # Handle incorrect channel\n",
    "                x = x[:, :1, :, :]  # Ensure single channel is processed\n",
    "            x = x / 255.0  # Normalize input\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in input preprocessing: {e}\")\n",
    "            raise ValueError(f\"Input preprocessing failed: {e}\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input with error handling\n",
    "        try:\n",
    "            x = self.preprocess_input(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during input preprocessing: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Apply patch embedding\n",
    "            x = self.embedding(x)  # [batch_size, embed_dim, H/patch_size, W/patch_size]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during embedding layer: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            x = x.flatten(2)  # Flatten height and width into a single patch dimension [batch_size, embed_dim, num_patches]\n",
    "            x = x.transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during patch flattening or transpose: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Add positional embedding\n",
    "            x = x + self.pos_embedding[:, :x.size(1), :]  # Match positional embedding size with x\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during positional embedding addition: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Pass through transformer encoder\n",
    "            x = self.transformer(x)  # [batch_size, num_patches, embed_dim]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during transformer encoder: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Global average pooling across patches\n",
    "            x = x.mean(dim=1)  # [batch_size, embed_dim]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during global average pooling: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Ensure correct shape for the classifier layer\n",
    "            if x.size(1) != self.classifier.in_features:\n",
    "                raise ValueError(f\"Expected input size {self.classifier.in_features} but got {x.size(1)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during classifier shape check: {e}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Classification layer\n",
    "            out = self.classifier(x)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during final classifier layer: {e}\")\n",
    "            return None\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d40e996-b12f-47a1-94cf-0260f9b2ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Data Loading and Preprocessing\n",
    "# ==========================\n",
    "\n",
    "def load_arc_data(file_paths: Optional[Dict[str, str]] = None, retries: int = 3, delay: float = 1.0) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Loads ARC dataset JSON files into a dictionary with enhanced error handling and retry logic.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_paths (dict): Optional. Dictionary of file names and paths. Defaults to ARC dataset.\n",
    "    - retries (int): Number of times to retry loading a file if an error occurs.\n",
    "    - delay (float): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "    - arc_data (dict): Dictionary containing the loaded ARC data.\n",
    "    \"\"\"\n",
    "    default_file_paths = {\n",
    "        \"arc-agi_training-challenges\": \"arc-agi_training_challenges.json\",\n",
    "        \"arc-agi_training-solutions\": \"arc-agi_training_solutions.json\",\n",
    "        \"arc-agi_evaluation-challenges\": \"arc-agi_evaluation_challenges.json\",\n",
    "        \"arc-agi_evaluation-solutions\": \"arc-agi_evaluation_solutions.json\"\n",
    "    }\n",
    "\n",
    "    # Use custom file_paths if provided; otherwise, fall back to defaults\n",
    "    file_paths = file_paths or default_file_paths\n",
    "    arc_data = {}\n",
    "\n",
    "    def load_single_file(key, path):\n",
    "        \"\"\"\n",
    "        Loads a single file, with retry logic on failure.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            logging.error(f\"File {path} does not exist. Please check the file path.\")\n",
    "            arc_data[key] = {}\n",
    "            return\n",
    "\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    arc_data[key] = json.load(f)\n",
    "                    logging.info(f\"Successfully loaded {key} from {path} on attempt {attempt}.\")\n",
    "                    return  # Exit on successful load\n",
    "            except FileNotFoundError:\n",
    "                logging.error(f\"File {path} not found. Attempt {attempt} failed.\")\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"File {path} contains invalid JSON. Attempt {attempt} failed.\")\n",
    "            except PermissionError:\n",
    "                logging.error(f\"Permission denied for file {path}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An unexpected error occurred while loading {path}: {e}\")\n",
    "\n",
    "            # Retry logic if load fails\n",
    "            if attempt < retries:\n",
    "                logging.warning(f\"Retrying {path} (attempt {attempt + 1}/{retries})...\")\n",
    "                time.sleep(delay)  # Delay between retries\n",
    "            else:\n",
    "                logging.error(f\"Failed to load {path} after {retries} attempts.\")\n",
    "                arc_data[key] = {}  # Default empty value after max retries\n",
    "\n",
    "    # Start loading process\n",
    "    start_time = time.time()\n",
    "    logging.info(\"Starting to load ARC dataset files...\")\n",
    "\n",
    "    # Use threading for concurrent file loading\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_single_file, key, path) for key, path in file_paths.items()]\n",
    "\n",
    "        # Wait for all files to be loaded\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Finished loading ARC dataset files. Total time: {end_time - start_time:.2f} seconds.\")\n",
    "    return arc_data\n",
    "\n",
    "def pad_to_30x30(data):\n",
    "    \"\"\"\n",
    "    Pads the input data to a 30x30 array.\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "        data = np.array(data)  # Convert list to numpy array if necessary\n",
    "    \n",
    "    if not isinstance(data, (np.ndarray, torch.Tensor)):\n",
    "        raise TypeError(f\"Expected data to be of type np.ndarray or torch.Tensor, but got {type(data)}\")\n",
    "    \n",
    "    padded_data = np.zeros((30, 30), dtype=data.dtype)\n",
    "    padded_data[:data.shape[0], :data.shape[1]] = data\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "def preprocess_arc_data(train_challenges, train_solutions):\n",
    "    \"\"\"\n",
    "    Processes and pads input data to a consistent size (30x30) and prepares labels.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Example normalization\n",
    "    ])\n",
    "    \n",
    "    for key, challenge in train_challenges.items():\n",
    "        if 'train' in challenge:\n",
    "            for item in challenge['train']:\n",
    "                if 'input' in item and 'output' in item:\n",
    "                    input_data = item['input']\n",
    "                    label_data = item['output']\n",
    "                    \n",
    "                    # Ensure input_data and label_data are in expected types\n",
    "                    if isinstance(input_data, (list, np.ndarray)) and isinstance(label_data, (list, np.ndarray)):\n",
    "                        if input_data and label_data:\n",
    "                            try:\n",
    "                                padded_input = pad_to_30x30(input_data)\n",
    "                                # Normalize to uint8\n",
    "                                if padded_input.dtype != np.uint8:\n",
    "                                    padded_input = (255 * (padded_input - padded_input.min()) / \n",
    "                                                    (padded_input.ptp() + 1e-8)).astype(np.uint8)\n",
    "\n",
    "                                # Transform the input\n",
    "                                padded_input = transform(padded_input)\n",
    "\n",
    "                                # Append the numpy array of the padded input\n",
    "                                inputs.append(padded_input.numpy())\n",
    "\n",
    "                                # Simplify label: assuming one-hot encoding, take argmax\n",
    "                                simplified_label = int(np.argmax(np.array(label_data).flatten()) % 10)  # Adjust as needed\n",
    "                                labels.append(simplified_label)\n",
    "\n",
    "                            except ValueError as value_error:\n",
    "                                logging.warning(f\"ValueError processing input data {input_data}: {value_error}\")\n",
    "                            except Exception as e:\n",
    "                                logging.warning(f\"Error processing item with input: {input_data} and label: {label_data}. Error: {e}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"Invalid data type for input or output: input type {type(input_data)}, output type {type(label_data)}\")\n",
    "\n",
    "    # Validate that inputs and labels were populated\n",
    "    if not inputs or not labels:\n",
    "        raise ValueError(\"No valid data found during preprocessing.\")\n",
    "\n",
    "    inputs_array = np.array(inputs, dtype=np.float32).reshape(-1, 1, 30, 30)\n",
    "    labels_array = np.array(labels, dtype=np.int64)\n",
    "    logging.info(f\"Preprocessed data: {inputs_array.shape[0]} samples.\")\n",
    "    \n",
    "    return inputs_array, labels_array\n",
    "\n",
    "\n",
    "\n",
    "def prepare_training_data(arc_data):\n",
    "    \"\"\"\n",
    "    Converts processed data into PyTorch DataLoader for batching.\n",
    "    \n",
    "    Parameters:\n",
    "    - arc_data: Dictionary containing processed ARC dataset.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader object for training data, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_challenges = arc_data['arc-agi_training-challenges']\n",
    "        train_solutions = arc_data['arc-agi_training-solutions']\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Missing key in arc_data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Preprocess the data and handle potential errors\n",
    "    try:\n",
    "        input_data, labels = preprocess_arc_data(train_challenges, train_solutions)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error during preprocessing: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if input data and labels are not empty\n",
    "    if input_data.size == 0 or labels.size == 0:\n",
    "        logging.error(\"Input data or labels are empty after preprocessing.\")\n",
    "        return None\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    try:\n",
    "        dataset = TensorDataset(torch.tensor(input_data, dtype=torch.float32), \n",
    "                                torch.tensor(labels, dtype=torch.int64))\n",
    "        logging.info(\"Created TensorDataset for training.\")\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        return data_loader\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating DataLoader: {e}\")\n",
    "        return None\n",
    "        \n",
    "def prepare_evaluation_data(arc_data):\n",
    "    \"\"\"\n",
    "    Prepares evaluation data similar to training data.\n",
    "\n",
    "    Parameters:\n",
    "    - arc_data: Dictionary containing processed ARC dataset.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader object for evaluation data, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        eval_challenges = arc_data['arc-agi_evaluation-challenges']\n",
    "        eval_solutions = arc_data['arc-agi_evaluation-solutions']\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Missing key in arc_data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Preprocess the evaluation data and handle potential errors\n",
    "    try:\n",
    "        input_data, labels = preprocess_arc_data(eval_challenges, eval_solutions)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error during preprocessing: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if input data and labels are not empty\n",
    "    if input_data.size == 0 or labels.size == 0:\n",
    "        logging.error(\"Input data or labels are empty after preprocessing.\")\n",
    "        return None\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    try:\n",
    "        dataset = TensorDataset(torch.tensor(input_data, dtype=torch.float32), \n",
    "                                torch.tensor(labels, dtype=torch.int64))\n",
    "        logging.info(\"Created TensorDataset for evaluation.\")\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "        return data_loader\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating DataLoader: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "396d5115-4a71-4896-8a2f-e4c4fcdc18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Utility Functions\n",
    "# ==========================\n",
    "\n",
    "def generic_preprocessor(train_loader):\n",
    "    \"\"\"\n",
    "    Generic preprocessor for models that don't require specific preprocessing steps.\n",
    "    Applies normalization and reshaping if needed.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader object containing training data.\n",
    "\n",
    "    Yields:\n",
    "    - Normalized inputs and corresponding labels.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Ensure inputs are in float32 format and normalized to [0, 1]\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                raise TypeError(f\"Expected inputs to be a torch.Tensor, got {type(inputs).__name__}\")\n",
    "\n",
    "            inputs = inputs.float() / 255.0\n",
    "            \n",
    "            # Check for required input shape\n",
    "            if len(inputs.shape) < 2:\n",
    "                raise ValueError(f\"Input tensors must have at least 2 dimensions, but got {inputs.shape}.\")\n",
    "\n",
    "            yield inputs, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {batch_idx} during preprocessing: {e}\")\n",
    "            continue  # Proceed to the next batch even if one fails\n",
    "\n",
    "def vision_transformer_preprocessor(train_loader, patch_size=5):\n",
    "    \"\"\"\n",
    "    Preprocessor specifically for VisionTransformerMagician models.\n",
    "    Ensures that the data is reshaped into patches.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader object containing training data.\n",
    "    - patch_size: Size of the patches to be created from the images.\n",
    "\n",
    "    Yields:\n",
    "    - Reshaped inputs and corresponding labels.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Ensure inputs are in float32 format\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                raise TypeError(f\"Expected inputs to be a torch.Tensor, got {type(inputs).__name__}\")\n",
    "\n",
    "            # Check for correct input dimensions\n",
    "            if inputs.ndim != 4:\n",
    "                raise ValueError(f\"Expected input with 4 dimensions (batch_size, channels, height, width), but got {inputs.shape}.\")\n",
    "\n",
    "            # Reshape the input to match Vision Transformer input, assuming inputs are images\n",
    "            inputs = inputs.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "            inputs = inputs.contiguous().view(inputs.size(0), -1, patch_size * patch_size * inputs.size(1))  # Flatten patches\n",
    "            \n",
    "            # Normalize the inputs to [0, 1]\n",
    "            inputs = inputs.float() / 255.0\n",
    "            \n",
    "            yield inputs, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {batch_idx} during Vision Transformer preprocessing: {e}\")\n",
    "            continue  # Proceed to the next batch even if one fails\n",
    "\n",
    "def vision_transformer_preprocessor(train_loader, patch_size=5):\n",
    "    \"\"\"\n",
    "    Preprocessor specifically for VisionTransformerMagician models.\n",
    "    Ensures that the data is reshaped into patches.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader object containing training data.\n",
    "    - patch_size: Size of the patches to be created from the images.\n",
    "\n",
    "    Yields:\n",
    "    - Reshaped inputs and corresponding labels.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Ensure inputs are a torch.Tensor\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                raise TypeError(f\"Expected inputs to be a torch.Tensor, but got {type(inputs).__name__}.\")\n",
    "\n",
    "            # Check for correct input dimensions\n",
    "            if inputs.ndim != 4:\n",
    "                raise ValueError(f\"Expected input with 4 dimensions (batch_size, channels, height, width), but got {inputs.shape}.\")\n",
    "\n",
    "            # Reshape the input to match Vision Transformer input, assuming inputs are images\n",
    "            inputs = inputs.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "            inputs = inputs.contiguous().view(inputs.size(0), -1, patch_size * patch_size * inputs.size(1))  # Flatten patches\n",
    "            \n",
    "            # Normalize the inputs to [0, 1]\n",
    "            inputs = inputs.float() / 255.0\n",
    "            \n",
    "            yield inputs, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {batch_idx} during Vision Transformer preprocessing: {e}\")\n",
    "            continue  # Proceed to the next batch even if one fails\n",
    "\n",
    "\n",
    "def cnn_preprocessor(train_loader):\n",
    "    \"\"\"\n",
    "    Preprocessor specifically for CNNMagician models.\n",
    "    Ensures that the data is in the correct format with the right dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader object containing training data.\n",
    "\n",
    "    Yields:\n",
    "    - Normalized inputs and corresponding labels.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Ensure inputs are a torch.Tensor\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                raise TypeError(f\"Expected inputs to be a torch.Tensor, but got {type(inputs).__name__}.\")\n",
    "\n",
    "            # Check for correct input dimensions\n",
    "            if inputs.ndim == 3:  # If the channel dimension is missing\n",
    "                inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
    "            elif inputs.ndim != 4:\n",
    "                raise ValueError(f\"Expected input with 4 dimensions (batch_size, channels, height, width), but got {inputs.shape}.\")\n",
    "\n",
    "            # Normalize inputs to [0, 1]\n",
    "            inputs = inputs.float() / 255.0\n",
    "            \n",
    "            yield inputs, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {batch_idx} during CNN preprocessing: {e}\")\n",
    "            continue  # Proceed to the next batch even if one fails\n",
    "\n",
    "def dnn_preprocessor(train_loader):\n",
    "    \"\"\"\n",
    "    Preprocessor specifically for DNNMagician models.\n",
    "    Flattens the inputs for fully connected layers.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader object containing training data.\n",
    "\n",
    "    Yields:\n",
    "    - Normalized and flattened inputs and corresponding labels.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Ensure inputs are a torch.Tensor\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                raise TypeError(f\"Expected inputs to be a torch.Tensor, but got {type(inputs).__name__}.\")\n",
    "\n",
    "            # Check for correct input dimensions\n",
    "            if inputs.ndim < 2:  # If inputs are 1D or less\n",
    "                raise ValueError(f\"Expected input with at least 2 dimensions, but got {inputs.shape}.\")\n",
    "            \n",
    "            # Flatten inputs for DNNs\n",
    "            if inputs.ndim > 2:  # If inputs are not already flattened\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            # Normalize inputs to [0, 1]\n",
    "            inputs = inputs.float() / 255.0\n",
    "\n",
    "            yield inputs, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {batch_idx} during DNN preprocessing: {e}\")\n",
    "            continue  # Proceed to the next batch even if one fails\n",
    "            \n",
    "def resnet_preprocessor(train_loader):\n",
    "    \"\"\"\n",
    "    Preprocessor specifically for ResNetMagician models.\n",
    "    Ensures that the data is 4D (batch_size, channels, height, width).\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader object containing training data.\n",
    "\n",
    "    Yields:\n",
    "    - Normalized inputs and corresponding labels.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Ensure inputs are a torch.Tensor\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                raise TypeError(f\"Expected inputs to be a torch.Tensor, but got {type(inputs).__name__}.\")\n",
    "\n",
    "            # Check for correct input dimensions\n",
    "            if inputs.ndim < 3:  # ResNet expects at least 3 dimensions (batch_size, height, width)\n",
    "                raise ValueError(f\"Expected input with at least 3 dimensions, but got {inputs.shape}.\")\n",
    "            \n",
    "            # Ensure inputs are 4D: [batch_size, channels, height, width]\n",
    "            if inputs.ndim == 3:  # If the channel dimension is missing, add it\n",
    "                inputs = inputs.unsqueeze(1)  # Add the channel dimension\n",
    "            \n",
    "            # Normalize inputs to [0, 1]\n",
    "            inputs = inputs.float() / 255.0\n",
    "\n",
    "            yield inputs, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {batch_idx} during ResNet preprocessing: {e}\")\n",
    "            continue  # Proceed to the next batch even if one fails\n",
    "\n",
    "def vision_transformer_postprocessor(model):\n",
    "    \"\"\"\n",
    "    Postprocessor for VisionTransformerMagician models.\n",
    "    Resets weights or parameters if necessary after training.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained Vision Transformer model instance.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Postprocessing Vision Transformer model {model.__class__.__name__}\")\n",
    "\n",
    "    try:\n",
    "        # Example: Reset positional encodings if needed\n",
    "        if hasattr(model, 'pos_embedding'):\n",
    "            logging.info(\"Resetting positional embedding parameters.\")\n",
    "            model.pos_embedding = nn.Parameter(torch.randn_like(model.pos_embedding))\n",
    "\n",
    "        # Optionally, reset other parameters or weights as needed\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                logging.info(f\"Resetting weights for {name}.\")\n",
    "                nn.init.kaiming_normal_(param)  # Reset weights using Kaiming normal initialization\n",
    "            elif 'bias' in name:\n",
    "                logging.info(f\"Resetting biases for {name}.\")\n",
    "                nn.init.constant_(param, 0)  # Reset biases to zero\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during postprocessing of Vision Transformer model: {e}\")\n",
    "\n",
    "def cnn_postprocessor(model):\n",
    "    \"\"\"\n",
    "    Postprocessor for CNNMagician models.\n",
    "    Resets or reinitializes parts of the model if needed.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained CNN model instance.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Postprocessing CNN model {model.__class__.__name__}\")\n",
    "\n",
    "    try:\n",
    "        # Example: Reset batch normalization layers if needed\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                logging.info(f\"Resetting running stats for {layer.__class__.__name__}.\")\n",
    "                layer.reset_running_stats()  # Reset running statistics\n",
    "\n",
    "        # Optionally reinitialize weights of the convolutional layers\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                logging.info(f\"Reinitializing weights for {layer.__class__.__name__}.\")\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')  # Kaiming normal initialization\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)  # Reset biases to zero\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during postprocessing of CNN model: {e}\")\n",
    "\n",
    "def dnn_postprocessor(model):\n",
    "    \"\"\"\n",
    "    Postprocessor for DNNMagician models.\n",
    "    Resets weights and biases after training if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained DNN model instance.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Postprocessing DNN model {model.__class__.__name__}\")\n",
    "\n",
    "    try:\n",
    "        # Reset weights and biases in fully connected layers\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                logging.info(f\"Resetting weights for layer {layer.__class__.__name__}.\")\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='linear')  # Kaiming normal initialization\n",
    "                if layer.bias is not None:\n",
    "                    logging.info(f\"Resetting biases for layer {layer.__class__.__name__}.\")\n",
    "                    nn.init.constant_(layer.bias, 0)  # Reset biases to zero\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during postprocessing of DNN model: {e}\")\n",
    "\n",
    "def resnet_postprocessor(model):\n",
    "    \"\"\"\n",
    "    Postprocessor for ResNetMagician models.\n",
    "    Resets layers and statistics after training if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained ResNet model instance.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Postprocessing ResNet model {model.__class__.__name__}\")\n",
    "\n",
    "    try:\n",
    "        # Reset running stats for batch normalization layers\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                logging.info(f\"Resetting running stats for layer {layer.__class__.__name__}.\")\n",
    "                layer.reset_running_stats()\n",
    "\n",
    "        # Example: Reinitialize the final fully connected layer\n",
    "        if hasattr(model, 'fc'):\n",
    "            logging.info(f\"Reinitializing final layer {model.fc.__class__.__name__}.\")\n",
    "            nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')  # Use appropriate initialization\n",
    "            if model.fc.bias is not None:\n",
    "                nn.init.constant_(model.fc.bias, 0)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during postprocessing of ResNet model: {e}\")\n",
    "\n",
    "\n",
    "def ensure_correct_shape(x, expected_features):\n",
    "    \"\"\"\n",
    "    Ensures that the input x has the correct shape to be passed into a layer that expects\n",
    "    'expected_features' number of input features. Reshapes if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - x: The input tensor.\n",
    "    - expected_features: The number of input features expected by the layer.\n",
    "\n",
    "    Returns:\n",
    "    - The reshaped tensor if necessary, or the original tensor if the shape is correct.\n",
    "    \"\"\"\n",
    "    # Check if input is a tensor\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        logging.error(\"Input is not a PyTorch tensor.\")\n",
    "        raise ValueError(\"Input must be a PyTorch tensor.\")\n",
    "\n",
    "    # Log the current shape of the input\n",
    "    logging.info(f\"Current input shape: {x.shape}, expected shape features: {expected_features}\")\n",
    "\n",
    "    # Check if input shape matches expected features\n",
    "    if x.size(1) != expected_features:\n",
    "        # If not, print warning and reshape accordingly\n",
    "        logging.warning(f\"Input shape {x.size(1)} does not match expected {expected_features}. Attempting to reshape.\")\n",
    "        \n",
    "        # Attempt reshaping the input\n",
    "        try:\n",
    "            # If the tensor has more than 2 dimensions, flatten the relevant dimensions\n",
    "            if x.dim() > 2:\n",
    "                x = x.view(x.size(0), -1)  # Flatten all but the batch dimension\n",
    "                logging.info(f\"Flattened input to shape: {x.shape}\")\n",
    "            else:\n",
    "                x = x.view(x.size(0), expected_features)  # Adjust shape directly\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reshaping input tensor: {e}\")\n",
    "            raise RuntimeError(f\"Could not reshape tensor from shape {x.shape} to expected shape with features {expected_features}.\")\n",
    "\n",
    "    return x\n",
    "\n",
    "def ensure_correct_shape_for_model(inputs, model):\n",
    "    \"\"\"\n",
    "    Adjusts the shape of the input tensor to fit the model's expected input.\n",
    "    \"\"\"\n",
    "    if isinstance(model, CNNMagician) or isinstance(model, ResNetMagician):\n",
    "        # CNN and ResNet expect 4D input: (batch_size, channels, height, width)\n",
    "        if len(inputs.shape) == 2:\n",
    "            # Reshape 2D tensor (batch_size, features) to 4D (batch_size, channels, height, width)\n",
    "            # Assuming input image is square, we can infer height and width from sqrt of the feature size\n",
    "            size = int(inputs.size(1) ** 0.5)  # Infer height/width\n",
    "            inputs = inputs.view(inputs.size(0), 1, size, size)  # Reshape to (batch_size, 1, height, width)\n",
    "\n",
    "    elif isinstance(model, DNNMagician):\n",
    "        # DNN expects 2D input: (batch_size, features)\n",
    "        if len(inputs.shape) == 4:\n",
    "            # Flatten 4D tensor (batch_size, channels, height, width) to 2D (batch_size, features)\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "    elif isinstance(model, VisionTransformerMagician):\n",
    "        # Vision Transformer expects 4D input but with specific patches\n",
    "        if len(inputs.shape) == 2:\n",
    "            size = int(inputs.size(1) ** 0.5)\n",
    "            inputs = inputs.view(inputs.size(0), 1, size, size)  # Reshape to (batch_size, 1, height, width)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "def ensure_correct_shape_for_model(inputs, model):\n",
    "    \"\"\"\n",
    "    Adjusts the shape of the input tensor to fit the model's expected input.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: The input tensor.\n",
    "    - model: The model instance (e.g., CNNMagician, DNNMagician).\n",
    "\n",
    "    Returns:\n",
    "    - The reshaped input tensor.\n",
    "    \"\"\"\n",
    "    # Log the current shape of the inputs\n",
    "    logging.info(f\"Current input shape: {inputs.shape} for model: {model.__class__.__name__}\")\n",
    "\n",
    "    try:\n",
    "        if isinstance(model, (CNNMagician, ResNetMagician)):\n",
    "            # CNN and ResNet expect 4D input: (batch_size, channels, height, width)\n",
    "            if len(inputs.shape) == 2:\n",
    "                # Reshape 2D tensor (batch_size, features) to 4D (batch_size, channels, height, width)\n",
    "                size = int(inputs.size(1) ** 0.5)  # Infer height/width\n",
    "                inputs = inputs.view(inputs.size(0), 1, size, size)  # Reshape to (batch_size, 1, height, width)\n",
    "                logging.info(f\"Reshaped inputs to 4D: {inputs.shape}\")\n",
    "\n",
    "        elif isinstance(model, DNNMagician):\n",
    "            # DNN expects 2D input: (batch_size, features)\n",
    "            if len(inputs.shape) == 4:\n",
    "                # Flatten 4D tensor (batch_size, channels, height, width) to 2D (batch_size, features)\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                logging.info(f\"Flattened inputs to 2D: {inputs.shape}\")\n",
    "\n",
    "        elif isinstance(model, VisionTransformerMagician):\n",
    "            # Vision Transformer expects 4D input but with specific patches\n",
    "            if len(inputs.shape) == 2:\n",
    "                size = int(inputs.size(1) ** 0.5)\n",
    "                inputs = inputs.view(inputs.size(0), 1, size, size)  # Reshape to (batch_size, 1, height, width)\n",
    "                logging.info(f\"Reshaped inputs for Vision Transformer to 4D: {inputs.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while reshaping inputs for model {model.__class__.__name__}: {e}\")\n",
    "        raise RuntimeError(\"Input reshaping failed. Please check input dimensions and model expectations.\")\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def ensure_flattened(x, expected_size):\n",
    "    \"\"\"\n",
    "    Ensures that the tensor is properly flattened to match the expected size.\n",
    "    If not, it will attempt to flatten it until it matches the expected size.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor to flatten.\n",
    "        expected_size (int): The expected size of the flattened tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The properly flattened tensor.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the tensor cannot be flattened to the expected size.\n",
    "    \"\"\"\n",
    "    # Log the initial shape of the input tensor\n",
    "    logging.info(f\"Initial shape of input tensor: {x.shape}\")\n",
    "\n",
    "    # Flatten the tensor\n",
    "    flattened_size = x.view(x.size(0), -1).size(1)\n",
    "    \n",
    "    # Check if the current flattened size matches the expected size\n",
    "    attempts = 0\n",
    "    max_attempts = 10  # Safeguard to prevent infinite loops\n",
    "    \n",
    "    while flattened_size != expected_size and attempts < max_attempts:\n",
    "        logging.warning(f\"Flattened size {flattened_size} does not match expected size {expected_size}. Attempting to flatten again.\")\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        flattened_size = x.size(1)  # Check the new flattened size\n",
    "        attempts += 1\n",
    "    \n",
    "    if flattened_size != expected_size:\n",
    "        logging.error(f\"Unable to flatten tensor to expected size {expected_size} after {attempts} attempts.\")\n",
    "        raise ValueError(f\"Tensor shape cannot be flattened to expected size {expected_size}. Current shape: {x.shape}\")\n",
    "\n",
    "    logging.info(f\"Successfully flattened tensor to shape: {x.shape}\")\n",
    "    return x\n",
    "\n",
    "def randomize_params():\n",
    "    \"\"\"\n",
    "    Generates random hyperparameters for model training.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the randomized hyperparameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num_models = random.randint(10, 20)\n",
    "        num_epochs = random.randint(20, 200)\n",
    "        initial_learning_rate = random.uniform(0.001, 0.01)  # Reasonable range for learning rates\n",
    "        accuracy_threshold = random.uniform(0.75, 0.95)\n",
    "\n",
    "        # Log the generated parameters\n",
    "        logging.info(f\"Randomized Parameters - Models: {num_models}, Epochs: {num_epochs}, \"\n",
    "                     f\"LR: {initial_learning_rate:.4f}, Accuracy Threshold: {accuracy_threshold:.2f}\")\n",
    "\n",
    "        return {\n",
    "            \"num_models\": num_models,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"initial_learning_rate\": initial_learning_rate,\n",
    "            \"accuracy_threshold\": accuracy_threshold\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating random parameters: {e}\")\n",
    "        raise\n",
    "        \n",
    "def get_random_model(model_type=None):\n",
    "    \"\"\"\n",
    "    Instantiates a model based on the specified type.\n",
    "    If no type is specified, randomly selects between available models.\n",
    "    \n",
    "    Args:\n",
    "        model_type (str, optional): The type of model to instantiate. \n",
    "                                     If None, a model is chosen randomly.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: An instance of the specified or randomly chosen model.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the model type is unknown.\n",
    "    \"\"\"\n",
    "    available_models = ['CNN', 'DNN', 'ResNet', 'ViT']\n",
    "    \n",
    "    try:\n",
    "        if model_type is None:\n",
    "            model_type = random.choice(available_models)\n",
    "        \n",
    "        logging.info(f\"Selected model type: {model_type}\")\n",
    "\n",
    "        if model_type == 'CNN':\n",
    "            return CNNMagician()\n",
    "        elif model_type == 'DNN':\n",
    "            # Assuming input_size=900 for 30x30 images flattened\n",
    "            return DNNMagician(input_size=900)\n",
    "        elif model_type == 'ResNet':\n",
    "            return ResNetMagician()\n",
    "        elif model_type == 'ViT':\n",
    "            return VisionTransformerMagician()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error instantiating model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e338a9-36ea-49f9-a3c3-2badf952ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Helper Reinforcement Model\n",
    "# ==========================\n",
    "\n",
    "class HyperparameterHelper:\n",
    "    \"\"\"\n",
    "    A helper model to adjust hyperparameters based on training metrics.\n",
    "    Uses a simple rule-based approach for demonstration, but can be expanded \n",
    "    with more sophisticated strategies if needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_lr):\n",
    "        self.lr = initial_lr\n",
    "        self.lr_history = deque(maxlen=10)\n",
    "        self.accuracy_history = deque(maxlen=10)\n",
    "        self.loss_history = deque(maxlen=10)\n",
    "\n",
    "    def update_metrics(self, loss, accuracy):\n",
    "        \"\"\"Update the recorded metrics for loss and accuracy.\"\"\"\n",
    "        self.loss_history.append(loss)\n",
    "        self.accuracy_history.append(accuracy)\n",
    "\n",
    "    def adjust_hyperparameters(self):\n",
    "        \"\"\"\n",
    "        Adjust learning rate based on recent performance metrics.\n",
    "        \"\"\"\n",
    "        if len(self.loss_history) < 10:\n",
    "            return self.lr  # Not enough data to adjust\n",
    "\n",
    "        avg_loss = np.mean(self.loss_history)\n",
    "        avg_acc = np.mean(self.accuracy_history)\n",
    "\n",
    "        # Simple rule-based adjustments\n",
    "        try:\n",
    "            if avg_loss < 0.5 and avg_acc > 0.9:\n",
    "                self.lr *= 0.95  # Slightly decrease learning rate\n",
    "                logging.info(f\"Helper Model: Decreasing LR to {self.lr:.6f}\")\n",
    "            elif avg_loss > 1.0 and avg_acc < 0.5:\n",
    "                self.lr *= 1.05  # Slightly increase learning rate\n",
    "                logging.info(f\"Helper Model: Increasing LR to {self.lr:.6f}\")\n",
    "\n",
    "            # Ensure learning rate remains within bounds\n",
    "            self.lr = max(1e-6, min(self.lr, 1.0))\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adjusting hyperparameters: {e}\")\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "    def get_current_lr(self):\n",
    "        \"\"\"Return the current learning rate.\"\"\"\n",
    "        return self.lr\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the helper model metrics and learning rate.\"\"\"\n",
    "        self.lr_history.clear()\n",
    "        self.accuracy_history.clear()\n",
    "        self.loss_history.clear()\n",
    "        self.lr = 0.001  # Reset to default or initial learning rate\n",
    "        logging.info(\"Helper Model: Metrics reset and learning rate set to default.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b9ba38-7e5d-449f-9b9a-5624ff0009b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# GUI Class\n",
    "# ==========================\n",
    "\n",
    "class TrainingGUI:\n",
    "    \"\"\"\n",
    "    A Tkinter-based GUI that displays real-time training progress, including current model number, epoch, loss, accuracy, and learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, total_models, total_epochs):\n",
    "        self.root = root\n",
    "        self.root.title(\"Model Training Progress Tracker\")\n",
    "        self.queue = queue.Queue()\n",
    "\n",
    "        # Initialize GUI components\n",
    "        self.model_label = tk.Label(root, text=f\"Training Model: 0/{total_models}\", font=(\"Helvetica\", 14))\n",
    "        self.model_label.pack(pady=5)\n",
    "\n",
    "        self.epoch_label = tk.Label(root, text=f\"Epoch: 0/{total_epochs}\", font=(\"Helvetica\", 14))\n",
    "        self.epoch_label.pack(pady=5)\n",
    "\n",
    "        self.loss_label = tk.Label(root, text=\"Loss: 0.0000\", font=(\"Helvetica\", 12))\n",
    "        self.loss_label.pack(pady=2)\n",
    "\n",
    "        self.accuracy_label = tk.Label(root, text=\"Accuracy: 0.0000\", font=(\"Helvetica\", 12))\n",
    "        self.accuracy_label.pack(pady=2)\n",
    "\n",
    "        self.val_loss_label = tk.Label(root, text=\"Validation Loss: 0.0000\", font=(\"Helvetica\", 12))\n",
    "        self.val_loss_label.pack(pady=2)\n",
    "\n",
    "        self.val_accuracy_label = tk.Label(root, text=\"Validation Accuracy: 0.0000\", font=(\"Helvetica\", 12))\n",
    "        self.val_accuracy_label.pack(pady=2)\n",
    "\n",
    "        self.lr_label = tk.Label(root, text=\"Learning Rate: 0.000000\", font=(\"Helvetica\", 12))\n",
    "        self.lr_label.pack(pady=2)\n",
    "\n",
    "        self.progress_bar = ttk.Progressbar(root, orient=\"horizontal\", length=400, mode=\"determinate\")\n",
    "        self.progress_bar.pack(pady=10)\n",
    "\n",
    "        # Real-time plots\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4))\n",
    "        self.line_loss, = self.ax.plot([], [], label='Training Loss', color='blue')\n",
    "        self.line_val_loss, = self.ax.plot([], [], label='Validation Loss', color='orange')\n",
    "        self.line_acc, = self.ax.plot([], [], label='Training Accuracy', color='green')\n",
    "        self.line_val_acc, = self.ax.plot([], [], label='Validation Accuracy', color='red')\n",
    "        self.ax.set_xlabel('Epochs')\n",
    "        self.ax.set_ylabel('Metrics')\n",
    "        self.ax.legend()\n",
    "        self.ax.grid(True)\n",
    "        self.canvas = FigureCanvasTkAgg(self.fig, master=root)\n",
    "        self.canvas.draw()\n",
    "        self.canvas.get_tk_widget().pack()\n",
    "\n",
    "        self.loss_data = []\n",
    "        self.val_loss_data = []\n",
    "        self.acc_data = []\n",
    "        self.val_acc_data = []\n",
    "\n",
    "        # Ensemble Metrics\n",
    "        self.ensemble_label = tk.Label(root, text=\"Ensemble Accuracy: N/A\", font=(\"Helvetica\", 14))\n",
    "        self.ensemble_label.pack(pady=5)\n",
    "\n",
    "        # Start processing the queue\n",
    "        self.root.after(100, self.process_queue)\n",
    "\n",
    "    def process_queue(self):\n",
    "        \"\"\"\n",
    "        Process the queue for thread-safe GUI updates.\n",
    "        \"\"\"\n",
    "        while not self.queue.empty():\n",
    "            message = self.queue.get()\n",
    "            if isinstance(message, dict):\n",
    "                msg_type = message.get('type')\n",
    "                if msg_type == 'epoch':\n",
    "                    self.update_epoch(message)\n",
    "                elif msg_type == 'mini_epoch':\n",
    "                    self.update_mini_epoch(message)\n",
    "                elif msg_type == 'ensemble_accuracy':\n",
    "                    self.update_ensemble_accuracy(message.get('accuracy'))\n",
    "                elif msg_type == 'training_completed':\n",
    "                    self.model_label.config(text=\"Training Completed\")\n",
    "            else:\n",
    "                logging.warning(f\"Unexpected message type: {type(message)}\")\n",
    "        self.root.after(100, self.process_queue)\n",
    "\n",
    "    def update_epoch(self, data):\n",
    "        \"\"\"\n",
    "        Updates the GUI elements with new training epoch information.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model_label.config(text=f\"Training Model: {data['model_num']}/{data['total_models']}\")\n",
    "            self.epoch_label.config(text=f\"Epoch: {data['epoch']}/{data['total_epochs']}\")\n",
    "            self.loss_label.config(text=f\"Loss: {data['loss']:.4f}\")\n",
    "            self.accuracy_label.config(text=f\"Accuracy: {data['accuracy']:.4f}\")\n",
    "            self.val_loss_label.config(text=f\"Validation Loss: {data['val_loss']:.4f}\")\n",
    "            self.val_accuracy_label.config(text=f\"Validation Accuracy: {data['val_accuracy']:.4f}\")\n",
    "            self.lr_label.config(text=f\"Learning Rate: {data['lr']:.6f}\")\n",
    "\n",
    "            # Update progress bar\n",
    "            self.progress_bar[\"value\"] = (data['epoch'] / data['total_epochs']) * 100\n",
    "            self.root.update_idletasks()\n",
    "\n",
    "            # Update plots\n",
    "            self.loss_data.append(data['loss'])\n",
    "            self.val_loss_data.append(data['val_loss'])\n",
    "            self.acc_data.append(data['accuracy'])\n",
    "            self.val_acc_data.append(data['val_accuracy'])\n",
    "\n",
    "            self.line_loss.set_data(range(1, len(self.loss_data) + 1), self.loss_data)\n",
    "            self.line_val_loss.set_data(range(1, len(self.val_loss_data) + 1), self.val_loss_data)\n",
    "            self.line_acc.set_data(range(1, len(self.acc_data) + 1), self.acc_data)\n",
    "            self.line_val_acc.set_data(range(1, len(self.val_acc_data) + 1), self.val_acc_data)\n",
    "\n",
    "            self.ax.relim()\n",
    "            self.ax.autoscale_view()\n",
    "            self.canvas.draw()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating epoch data: {e}\")\n",
    "\n",
    "    def update_mini_epoch(self, data):\n",
    "        \"\"\"\n",
    "        Updates the GUI elements with new injected mini-epoch information.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model_label.config(text=f\"Training Model: {data['model_num']}/{data['total_models']}\")\n",
    "            self.epoch_label.config(text=f\"Injected Mini-Epoch: {data['current_mini_epoch']}/{data['max_mini_epochs']}\")\n",
    "            self.loss_label.config(text=f\"Loss: {data['loss']:.4f}\")\n",
    "            self.accuracy_label.config(text=f\"Accuracy: {data['accuracy']:.4f}\")\n",
    "            self.val_loss_label.config(text=f\"Validation Loss: {data['val_loss']:.4f}\")\n",
    "            self.val_accuracy_label.config(text=f\"Validation Accuracy: {data['val_accuracy']:.4f}\")\n",
    "            self.lr_label.config(text=f\"Learning Rate: {data['lr']:.6f}\")\n",
    "\n",
    "            # Update plots\n",
    "            self.loss_data.append(data['loss'])\n",
    "            self.val_loss_data.append(data['val_loss'])\n",
    "            self.acc_data.append(data['accuracy'])\n",
    "            self.val_acc_data.append(data['val_accuracy'])\n",
    "\n",
    "            self.line_loss.set_data(range(1, len(self.loss_data) + 1), self.loss_data)\n",
    "            self.line_val_loss.set_data(range(1, len(self.val_loss_data) + 1), self.val_loss_data)\n",
    "            self.line_acc.set_data(range(1, len(self.acc_data) + 1), self.acc_data)\n",
    "            self.line_val_acc.set_data(range(1, len(self.val_acc_data) + 1), self.val_acc_data)\n",
    "\n",
    "            self.ax.relim()\n",
    "            self.ax.autoscale_view()\n",
    "            self.canvas.draw()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating mini epoch data: {e}\")\n",
    "\n",
    "    def update_ensemble_accuracy(self, accuracy):\n",
    "        \"\"\"\n",
    "        Updates the ensemble accuracy label.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.ensemble_label.config(text=f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating ensemble accuracy: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2de3231-6372-4310-aa6e-be188b964931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Training Functions\n",
    "# ==========================\n",
    "\n",
    "def ensure_correct_shape(x, expected_shape):\n",
    "    \"\"\"\n",
    "    Ensure the input tensor matches the expected shape by flattening or reshaping if necessary.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor to reshape.\n",
    "        expected_shape (int): The expected number of input features.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The reshaped tensor.\n",
    "    \"\"\"\n",
    "    # Check if the input is a tensor\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(\"Input must be a PyTorch tensor.\")\n",
    "\n",
    "    original_shape = x.shape  # Store the original shape for logging\n",
    "    actual_shape = x.size(1)  # Input feature size (excluding batch dimension)\n",
    "\n",
    "    if actual_shape != expected_shape:\n",
    "        logging.warning(f\"Shape mismatch: Expected {expected_shape}, got {actual_shape}. Adjusting...\")\n",
    "        \n",
    "        # If the tensor has more than 2 dimensions, we need to flatten it\n",
    "        if len(original_shape) > 2:  \n",
    "            x = x.view(x.size(0), -1)  # Flatten while preserving batch size\n",
    "        \n",
    "        # Check again if the flattened shape matches the expected shape\n",
    "        actual_shape = x.size(1)\n",
    "        if actual_shape != expected_shape:\n",
    "            logging.error(f\"Failed to reshape tensor. Original shape: {original_shape}. Current shape: {x.shape}.\")\n",
    "            raise RuntimeError(f\"Unable to reshape tensor to match the expected shape {expected_shape}. Got {actual_shape} instead.\")\n",
    "\n",
    "    logging.info(f\"Input tensor reshaped from {original_shape} to {x.shape}.\")\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, eval_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the evaluation data, ensuring inputs are preprocessed.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        eval_loader (DataLoader): The DataLoader containing evaluation data.\n",
    "        criterion (nn.Module): The loss function to use for evaluation.\n",
    "        device (torch.device): The device to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss and accuracy of the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    if len(eval_loader) == 0:\n",
    "        logging.warning(\"Evaluation loader is empty. No evaluation performed.\")\n",
    "        return float('inf'), 0.0  # Return high loss and zero accuracy\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in eval_loader:\n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Ensure the model preprocesses inputs\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation: {e}\")\n",
    "                continue  # Log the error and skip this batch\n",
    "\n",
    "    avg_loss = total_loss / len(eval_loader) if total_samples > 0 else float('inf')\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    logging.info(f\"Evaluation complete. Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_regular_model(model, train_loader, eval_loader, num_epochs, initial_learning_rate, gui, model_num, total_models, helper, device):\n",
    "    \"\"\"\n",
    "    Trains a single model (CNN, DNN, ResNet, ViT, etc.) with adaptive learning rate and early stopping.\n",
    "    \"\"\"\n",
    "    logging.debug(f\"Starting training for Model {model_num} with architecture: {model.__class__.__name__}\")\n",
    "\n",
    "    model.to(device)  # Move model to the appropriate device\n",
    "    optimizer = Adam(model.parameters(), lr=initial_learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        logging.debug(f\"Epoch {epoch}/{num_epochs} for Model {model_num}\")\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            # Ensure inputs and labels are moved to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            logging.debug(f\"Model {model_num}, Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    logging.info(f\"Completed training for Model {model_num}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inject_mini_epochs(model, train_loader, eval_loader, optimizer, scheduler, criterion,\n",
    "                      gui, model_num, total_models, helper, best_loss, patience=10, \n",
    "                      max_injections=7, epochs_per_injection=3, current_layer=1, max_layers=7):\n",
    "    \"\"\"\n",
    "    Injects additional mini-epochs with different model architectures to overcome stagnation until improvement is found.\n",
    "    Dynamically reshapes data and adapts architecture to ensure compatibility without breaking or skipping any steps.\n",
    "    \"\"\"\n",
    "    def create_mini_model(layer):\n",
    "        \"\"\"Instantiate a mini model based on the current layer.\"\"\"\n",
    "        if layer == 1:\n",
    "            return DNNMagician(input_size=900)\n",
    "        elif layer == 2:\n",
    "            return ResNetMagician()\n",
    "        elif layer == 3:\n",
    "            return VisionTransformerMagician()\n",
    "        elif layer == 4:\n",
    "            return CNNMagician()\n",
    "        elif layer in [5, 6, 7]:\n",
    "            return DNNMagician(input_size=900) if layer % 2 == 0 else ResNetMagician()\n",
    "        return CNNMagician()  # Default fallback\n",
    "\n",
    "    def train_mini_model(mini_model, train_loader):\n",
    "        \"\"\"Train the mini model for a specified number of epochs.\"\"\"\n",
    "        mini_model.train()\n",
    "        total_loss, correct_predictions, total_samples = 0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            try:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                inputs = try_dynamic_reshape(inputs, mini_model)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = mini_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                mini_optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            except (RuntimeError, Exception) as error:\n",
    "                logging.error(f\"Error during training mini model: {error}\")\n",
    "                continue  # Log and proceed to the next batch\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader) if total_samples > 0 else float('inf')\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    injected_epochs = 0\n",
    "    improvement_found = False\n",
    "\n",
    "    while injected_epochs < max_injections and not improvement_found:\n",
    "        injected_epochs += 1\n",
    "        logging.info(f\"Model {model_num}/{total_models}, Injected Mini-Epoch [{injected_epochs}/{max_injections}] at Layer {current_layer}/{max_layers}\")\n",
    "\n",
    "        mini_model = create_mini_model(current_layer)\n",
    "        mini_model.to(device)\n",
    "        mini_optimizer = Adam(mini_model.parameters(), lr=optimizer.param_groups[0]['lr'])\n",
    "        mini_scheduler = lr_scheduler.ReduceLROnPlateau(mini_optimizer, mode='min', patience=5, factor=0.5)\n",
    "        mini_criterion = nn.CrossEntropyLoss()\n",
    "        mini_helper = copy.deepcopy(helper)  # Clone the helper to maintain separate histories\n",
    "\n",
    "        for mini_epoch in range(1, epochs_per_injection + 1):\n",
    "            avg_loss, accuracy = train_mini_model(mini_model, train_loader)\n",
    "\n",
    "            # Validation Phase\n",
    "            try:\n",
    "                injected_val_loss, injected_val_accuracy = evaluate_model(mini_model, eval_loader, mini_criterion, device)\n",
    "                mini_scheduler.step(injected_val_loss)\n",
    "            except Exception as eval_error:\n",
    "                logging.error(f\"Error during validation in mini model: {eval_error}\")\n",
    "                injected_val_loss, injected_val_accuracy = float('inf'), 0.0\n",
    "\n",
    "            logging.info(f\"Model {model_num}/{total_models}, Injected Mini-Epoch [{injected_epochs}/{max_injections}], \"\n",
    "                         f\"Layer: {current_layer}, Epoch [{mini_epoch}/{epochs_per_injection}], \"\n",
    "                         f\"Model Type: {mini_model.__class__.__name__}, \"\n",
    "                         f\"Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}, \"\n",
    "                         f\"Validation Loss: {injected_val_loss:.4f}, Validation Accuracy: {injected_val_accuracy:.4f}\")\n",
    "\n",
    "            # Update GUI\n",
    "            try:\n",
    "                gui.queue.put({\n",
    "                    'type': 'mini_epoch',\n",
    "                    'model_num': model_num,\n",
    "                    'total_models': total_models,\n",
    "                    'current_mini_epoch': injected_epochs,\n",
    "                    'max_mini_epochs': max_injections,\n",
    "                    'loss': avg_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    'val_loss': injected_val_loss,\n",
    "                    'val_accuracy': injected_val_accuracy,\n",
    "                    'lr': mini_optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "            except Exception as gui_error:\n",
    "                logging.error(f\"Error updating GUI with mini-epoch info: {gui_error}\")\n",
    "\n",
    "            # Update helper model and adjust learning rate\n",
    "            try:\n",
    "                mini_helper.update_metrics(injected_val_loss, injected_val_accuracy)\n",
    "                adjusted_mini_lr = mini_helper.adjust_hyperparameters()\n",
    "                for param_group in mini_optimizer.param_groups:\n",
    "                    param_group['lr'] = adjusted_mini_lr\n",
    "            except Exception as helper_error:\n",
    "                logging.error(f\"Error adjusting hyperparameters or updating helper model: {helper_error}\")\n",
    "\n",
    "            # Check for improvement\n",
    "            if injected_val_loss < best_loss:\n",
    "                best_loss = injected_val_loss\n",
    "                improvement_found = True\n",
    "                patience = 10  # Reset patience after improvement\n",
    "                logging.info(f\"Improvement found during injected mini-epochs for Model {model_num}.\")\n",
    "                try:\n",
    "                    torch.save(mini_model.state_dict(), f'best_model_{model_num}_mini_epoch_layer{current_layer}.pth')\n",
    "                except Exception as save_error:\n",
    "                    logging.error(f\"Error saving mini-epoch model at Layer {current_layer}: {save_error}\")\n",
    "\n",
    "        # Proceed to next layer if no improvement found\n",
    "        if not improvement_found and current_layer < max_layers:\n",
    "            current_layer += 1  # Move to the next layer\n",
    "\n",
    "def try_dynamic_reshape(inputs: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dynamically reshapes inputs to match the expected shape of the model.\n",
    "    Tries to flatten, reduce, or modify the input data to ensure it can be processed\n",
    "    by the model without shape errors. Includes error handling and corrective steps.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): The input tensor to reshape.\n",
    "        model (nn.Module): The model to which the inputs will be passed.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The reshaped inputs compatible with the model.\n",
    "    \"\"\"\n",
    "    def reshape_for_dnn(inputs: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Reshape inputs for fully connected models.\"\"\"\n",
    "        inputs = inputs.view(inputs.size(0), -1)  # Flatten inputs\n",
    "        input_size = model.fc1.in_features\n",
    "        if inputs.size(1) != input_size:\n",
    "            logging.warning(f\"Input size mismatch for DNN: {inputs.size(1)} vs. {input_size}. Adjusting size.\")\n",
    "            if inputs.size(1) > input_size:\n",
    "                inputs = inputs[:, :input_size]  # Trim the input\n",
    "            else:\n",
    "                # Pad the input if it's smaller than expected\n",
    "                padding = torch.zeros(inputs.size(0), input_size - inputs.size(1)).to(inputs.device)\n",
    "                inputs = torch.cat((inputs, padding), dim=1)\n",
    "        return inputs\n",
    "\n",
    "    def reshape_for_cnn(inputs: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Reshape inputs for CNN and ResNet models.\"\"\"\n",
    "        expected_channels = model.conv1.in_channels\n",
    "        if inputs.dim() == 2:\n",
    "            height = int(inputs.size(1) ** 0.5)\n",
    "            inputs = inputs.view(inputs.size(0), 1, height, height)  # Assuming square input images\n",
    "        if inputs.size(1) != expected_channels:\n",
    "            logging.warning(f\"Adjusting input channels from {inputs.size(1)} to {expected_channels}.\")\n",
    "            inputs = inputs.unsqueeze(1)  # Add channel dimension if necessary\n",
    "        return inputs\n",
    "\n",
    "    def reshape_for_transformer(inputs: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Reshape inputs for Transformer models.\"\"\"\n",
    "        patch_size = model.patch_size\n",
    "        inputs = inputs.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "        inputs = inputs.flatten(2).transpose(1, 2)  # Transform input into patches\n",
    "        return inputs\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'fc1'):  # DNNMagician\n",
    "            inputs = reshape_for_dnn(inputs, model)\n",
    "        elif hasattr(model, 'conv1'):  # CNNMagician or ResNetMagician\n",
    "            inputs = reshape_for_cnn(inputs, model)\n",
    "        elif hasattr(model, 'embedding'):  # VisionTransformerMagician\n",
    "            inputs = reshape_for_transformer(inputs, model)\n",
    "        else:\n",
    "            logging.error(f\"Model type not recognized for reshaping: {model.__class__.__name__}\")\n",
    "\n",
    "    except RuntimeError as reshape_error:\n",
    "        logging.error(f\"Error reshaping inputs: {reshape_error}\")\n",
    "        try:\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Last resort: fully flatten inputs\n",
    "            logging.info(f\"Falling back to fully flattened input: {inputs.shape}\")\n",
    "        except Exception as fallback_error:\n",
    "            logging.error(f\"Failed to reshape even after fallback: {fallback_error}\")\n",
    "            raise fallback_error\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def combine_models_random_forest(train_loader, regular_models, mini_epoch_models=None, n_estimators=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Combines outputs from multiple models (regular and mini-epoch) using a Random Forest classifier.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader providing training data.\n",
    "        regular_models: List of regular trained models.\n",
    "        mini_epoch_models: Optional list of mini-epoch models.\n",
    "        n_estimators: Number of trees in the Random Forest (default: 100).\n",
    "        random_state: Random state for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "        RandomForestClassifier: The trained Random Forest classifier.\n",
    "    \"\"\"\n",
    "    if mini_epoch_models is None:\n",
    "        mini_epoch_models = []\n",
    "        \n",
    "    all_train_features = []\n",
    "    all_train_labels = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        model_outputs = []\n",
    "        for model in regular_models + mini_epoch_models:\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    model_outputs.append(outputs.detach().cpu().numpy())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during model prediction in batch {batch_idx}: {e}\")\n",
    "                continue  # Skip this model if there's an error\n",
    "\n",
    "        if not model_outputs:\n",
    "            logging.warning(f\"No outputs collected for batch {batch_idx}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        features = np.concatenate(model_outputs, axis=1)  # Concatenate model outputs along the feature axis\n",
    "        all_train_features.append(features)\n",
    "        all_train_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    if not all_train_features or not all_train_labels:\n",
    "        raise ValueError(\"No features or labels collected from models. Cannot proceed with training.\")\n",
    "\n",
    "    try:\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "        rf.fit(np.vstack(all_train_features), np.hstack(all_train_labels))\n",
    "        logging.info(\"Combined models using Random Forest.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fitting Random Forest model: {e}\")\n",
    "        raise\n",
    "\n",
    "    return rf\n",
    "\n",
    "def evaluate_ensemble_model(ensemble_model, eval_loader, regular_models, mini_epoch_models=None):\n",
    "    \"\"\"\n",
    "    Evaluates the ensemble model's performance on evaluation data.\n",
    "\n",
    "    Args:\n",
    "        ensemble_model: The trained ensemble model (e.g., Random Forest).\n",
    "        eval_loader: DataLoader providing evaluation data.\n",
    "        regular_models: List of regular trained models.\n",
    "        mini_epoch_models: Optional list of mini-epoch models.\n",
    "\n",
    "    Returns:\n",
    "        predictions: The predicted labels for the evaluation dataset.\n",
    "        all_eval_labels: The true labels for the evaluation dataset.\n",
    "        accuracy: The accuracy of the ensemble model.\n",
    "    \"\"\"\n",
    "    if mini_epoch_models is None:\n",
    "        mini_epoch_models = []\n",
    "        \n",
    "    all_eval_features, all_eval_labels = [], []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(eval_loader):\n",
    "        model_outputs = []\n",
    "        for model in regular_models + mini_epoch_models:\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    model_outputs.append(outputs.detach().cpu().numpy())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during model prediction in batch {batch_idx}: {e}\")\n",
    "                continue  # Skip this model if there's an error\n",
    "\n",
    "        if not model_outputs:\n",
    "            logging.warning(f\"No outputs collected for batch {batch_idx}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Concatenate model outputs along the feature axis\n",
    "        features = np.concatenate(model_outputs, axis=1)\n",
    "        all_eval_features.append(features)\n",
    "        all_eval_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    if not all_eval_features or not all_eval_labels:\n",
    "        raise ValueError(\"No features or labels collected from models. Cannot proceed with evaluation.\")\n",
    "\n",
    "    try:\n",
    "        predictions = ensemble_model.predict(np.vstack(all_eval_features))\n",
    "        accuracy = (predictions == np.hstack(all_eval_labels)).mean()\n",
    "        logging.info(f\"Ensemble Model Accuracy: {accuracy:.4f}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during ensemble model evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "    return predictions, np.hstack(all_eval_labels), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa935a18-03af-4d17-9c14-6f88f9365071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Training with GUI\n",
    "# ==========================\n",
    "\n",
    "def reshape_data_for_model(inputs, model):\n",
    "    \"\"\"Adjust the input shape to match the model's expected input shape.\"\"\"\n",
    "    input_size = inputs.view(inputs.size(0), -1).size(1)  # Flatten inputs\n",
    "    logging.info(f\"Current input size: {input_size}\")\n",
    "\n",
    "    try:\n",
    "        # Check if the model has a fully connected layer as the first layer\n",
    "        if hasattr(model, 'fc1'):\n",
    "            model_input_size = model.fc1.in_features\n",
    "        else:\n",
    "            # Handle other model types that may not have fc1\n",
    "            if hasattr(model, 'conv1'):\n",
    "                model_input_size = None  # For convolutional models, we'll handle differently\n",
    "            else:\n",
    "                logging.error(\"Model type is not recognized for reshaping.\")\n",
    "                raise ValueError(\"Model does not have identifiable input features.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing model input size: {e}\")\n",
    "        raise\n",
    "\n",
    "    if model_input_size is not None:\n",
    "        if input_size != model_input_size:\n",
    "            logging.info(f\"Reshaping input from {input_size} to {model_input_size}.\")\n",
    "            try:\n",
    "                inputs = inputs.view(inputs.size(0), model_input_size)  # Reshape inputs for fully connected layers\n",
    "            except RuntimeError as reshape_error:\n",
    "                logging.error(f\"Failed to reshape input: {reshape_error}\")\n",
    "                raise\n",
    "    else:\n",
    "        # Handle convolutional model inputs\n",
    "        if len(inputs.shape) == 2:  # If inputs are 2D, reshape to 4D\n",
    "            logging.info(f\"Reshaping 2D input to 4D for convolutional model.\")\n",
    "            height = width = int(input_size ** 0.5)  # Assuming square input for convolutional models\n",
    "            inputs = inputs.view(inputs.size(0), 1, height, width)  # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def run_training_with_gui(train_loader, eval_loader, num_models, num_epochs, initial_learning_rate):\n",
    "    \"\"\"\n",
    "    Initializes the GUI and orchestrates the training of multiple models in a separate thread to keep the GUI responsive.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    gui = TrainingGUI(root, num_models, num_epochs)\n",
    "\n",
    "    regular_models = []\n",
    "    mini_epoch_models = []\n",
    "\n",
    "    # Initialize Helper Reinforcement Model\n",
    "    helper = HyperparameterHelper(initial_lr=initial_learning_rate)\n",
    "\n",
    "    def train_models():\n",
    "        \"\"\"\n",
    "        Orchestrates the training of multiple models, handling errors with dynamic reshaping, preprocessing, and post-processing.\n",
    "        Ensures graceful recovery from errors without breaking the process.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "        # Debug statement to log the data type of num_models\n",
    "        logging.debug(f\"Data type of num_models: {type(num_models)}\")\n",
    "    \n",
    "        for model_num in range(1, num_models + 1):\n",
    "            model = None\n",
    "            model_type = random.choice(['CNN', 'DNN', 'ResNet', 'ViT'])\n",
    "    \n",
    "            try:\n",
    "                model = get_random_model(model_type=model_type)\n",
    "                model.to(device)\n",
    "                logging.info(f\"Training Model {model_num}/{num_models} with architecture: {model.__class__.__name__}\")\n",
    "    \n",
    "                # Train the model\n",
    "                train_regular_model(\n",
    "                    model, train_loader, eval_loader, num_epochs, initial_learning_rate,\n",
    "                    gui, model_num, num_models, helper, device\n",
    "                )\n",
    "    \n",
    "                regular_models.append(model)\n",
    "    \n",
    "                # Post-process the model after training\n",
    "                postprocessor = postprocessor_for_model(model)\n",
    "                postprocessor.apply()\n",
    "    \n",
    "            except TypeError as type_error:\n",
    "                logging.error(f\"TypeError encountered with Model {model_num} ({model_type}): {type_error}\")\n",
    "                logging.error(f\"Check types for model_num: {model_num} and num_models: {num_models}.\")\n",
    "    \n",
    "            except Exception as model_error:\n",
    "                logging.error(f\"Error with Model {model_num} ({model_type}): {model_error}\")\n",
    "\n",
    "    logging.info(\"All regular models have been trained.\")\n",
    "\n",
    "    # Combine models using Random Forest\n",
    "    ensemble_model = combine_models_with_error_handling(regular_models, train_loader)\n",
    "    \n",
    "    # Evaluate the ensemble model\n",
    "    evaluate_ensemble_with_error_handling(ensemble_model, eval_loader, gui)\n",
    "\n",
    "    # Notify GUI of completion\n",
    "    gui.queue.put({'type': 'training_completed'})\n",
    "\n",
    "        \n",
    "    # Start training in a separate thread\n",
    "    threading.Thread(target=train_models).start()\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "def combine_models_with_error_handling(regular_models, train_loader):\n",
    "    \"\"\"Combine models into a Random Forest classifier with error handling.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Combining models using Random Forest.\")\n",
    "        ensemble_model = combine_models_random_forest(train_loader, regular_models)\n",
    "        return ensemble_model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error combining models: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_ensemble_with_error_handling(ensemble_model, eval_loader, gui):\n",
    "    \"\"\"Evaluate the ensemble model with error handling.\"\"\"\n",
    "    try:\n",
    "        _, _, ensemble_accuracy = evaluate_ensemble_model(ensemble_model, eval_loader)\n",
    "        logging.info(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
    "        gui.queue.put({'type': 'ensemble_accuracy', 'accuracy': ensemble_accuracy})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating ensemble model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9952be6-efc0-4900-9329-53edae9f526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-15 18:01:51,472] INFO - Starting to load ARC dataset files...\n",
      "[2024-10-15 18:01:51,495] INFO - Successfully loaded arc-agi_training-solutions from arc-agi_training_solutions.json on attempt 1.\n",
      "[2024-10-15 18:01:51,528] INFO - Successfully loaded arc-agi_training-challenges from arc-agi_training_challenges.json on attempt 1.\n",
      "[2024-10-15 18:01:51,535] INFO - Successfully loaded arc-agi_evaluation-solutions from arc-agi_evaluation_solutions.json on attempt 1.\n",
      "[2024-10-15 18:01:51,574] INFO - Successfully loaded arc-agi_evaluation-challenges from arc-agi_evaluation_challenges.json on attempt 1.\n",
      "[2024-10-15 18:01:51,582] INFO - Finished loading ARC dataset files. Total time: 0.11 seconds.\n",
      "[2024-10-15 18:01:51,854] INFO - Preprocessed data: 1302 samples.\n",
      "[2024-10-15 18:01:51,863] INFO - Created TensorDataset for training.\n",
      "[2024-10-15 18:01:52,116] INFO - Preprocessed data: 1363 samples.\n",
      "[2024-10-15 18:01:52,116] INFO - Created TensorDataset for evaluation.\n",
      "[2024-10-15 18:01:52,121] INFO - Randomized Parameters - Models: 11, Epochs: 120, LR: 0.0098, Accuracy Threshold: 0.85\n",
      "[2024-10-15 18:01:52,862] INFO - All regular models have been trained.\n",
      "[2024-10-15 18:01:52,862] INFO - Combining models using Random Forest.\n",
      "[2024-10-15 18:01:52,878] WARNING - No outputs collected for batch 0. Skipping...\n",
      "[2024-10-15 18:01:52,879] WARNING - No outputs collected for batch 1. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 2. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 3. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 4. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 5. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 6. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 7. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 8. Skipping...\n",
      "[2024-10-15 18:01:52,881] WARNING - No outputs collected for batch 9. Skipping...\n",
      "[2024-10-15 18:01:52,893] WARNING - No outputs collected for batch 10. Skipping...\n",
      "[2024-10-15 18:01:52,894] WARNING - No outputs collected for batch 11. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 12. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 13. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 14. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 15. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 16. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 17. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 18. Skipping...\n",
      "[2024-10-15 18:01:52,896] WARNING - No outputs collected for batch 19. Skipping...\n",
      "[2024-10-15 18:01:52,908] WARNING - No outputs collected for batch 20. Skipping...\n",
      "[2024-10-15 18:01:52,911] WARNING - No outputs collected for batch 21. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 22. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 23. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 24. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 25. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 26. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 27. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 28. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 29. Skipping...\n",
      "[2024-10-15 18:01:52,912] WARNING - No outputs collected for batch 30. Skipping...\n",
      "[2024-10-15 18:01:52,925] WARNING - No outputs collected for batch 31. Skipping...\n",
      "[2024-10-15 18:01:52,926] WARNING - No outputs collected for batch 32. Skipping...\n",
      "[2024-10-15 18:01:52,928] WARNING - No outputs collected for batch 33. Skipping...\n",
      "[2024-10-15 18:01:52,929] WARNING - No outputs collected for batch 34. Skipping...\n",
      "[2024-10-15 18:01:52,930] WARNING - No outputs collected for batch 35. Skipping...\n",
      "[2024-10-15 18:01:52,931] WARNING - No outputs collected for batch 36. Skipping...\n",
      "[2024-10-15 18:01:52,932] WARNING - No outputs collected for batch 37. Skipping...\n",
      "[2024-10-15 18:01:52,933] WARNING - No outputs collected for batch 38. Skipping...\n",
      "[2024-10-15 18:01:52,933] WARNING - No outputs collected for batch 39. Skipping...\n",
      "[2024-10-15 18:01:52,933] WARNING - No outputs collected for batch 40. Skipping...\n",
      "[2024-10-15 18:01:52,933] ERROR - Error combining models: No features or labels collected from models. Cannot proceed with training.\n",
      "[2024-10-15 18:01:52,933] ERROR - Error evaluating ensemble model: evaluate_ensemble_model() missing 1 required positional argument: 'regular_models'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4 (train_models):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_17064\\53583223.py\", line 66, in train_models\n",
      "TypeError: can only concatenate str (not \"int\") to str\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Main Function\n",
    "# ==========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, prepare data loaders, set hyperparameters, initiate training, and evaluate the ensemble model.\n",
    "    \"\"\"\n",
    "    # 1. Load ARC Data\n",
    "    arc_data = load_arc_data()\n",
    "\n",
    "    # 2. Prepare Training and Evaluation Data\n",
    "    train_loader = prepare_training_data(arc_data)\n",
    "    if train_loader is None:\n",
    "        logging.error(\"Failed to prepare training data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    eval_loader = prepare_evaluation_data(arc_data)\n",
    "    if eval_loader is None:\n",
    "        logging.error(\"Failed to prepare evaluation data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 3. Set Hyperparameters (randomized)\n",
    "    num_models, num_epochs, initial_learning_rate, accuracy_threshold = randomize_params()\n",
    "\n",
    "    # 4. Run Training with GUI\n",
    "    run_training_with_gui(train_loader, eval_loader, num_models, num_epochs, initial_learning_rate)\n",
    "\n",
    "    # 5. After training, combine models using Random Forest\n",
    "    # Note: This is now handled automatically within the training thread.\n",
    "\n",
    "    # Placeholder for additional steps if needed\n",
    "    # For example, further evaluation, saving ensemble model, etc.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c95e22-cbef-4caa-a551-bbf9799f3ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
